{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":112509,"databundleVersionId":14254895,"sourceType":"competition"},{"sourceId":13900620,"sourceType":"datasetVersion","datasetId":8856212,"isSourceIdPinned":false},{"sourceId":14018229,"sourceType":"datasetVersion","datasetId":8929818,"isSourceIdPinned":false},{"sourceId":288467413,"sourceType":"kernelVersion"},{"sourceId":288761108,"sourceType":"kernelVersion"},{"sourceId":4537,"sourceType":"modelInstanceVersion","modelInstanceId":3329,"modelId":986},{"sourceId":268942,"sourceType":"modelInstanceVersion","modelInstanceId":230141,"modelId":251887},{"sourceId":663314,"sourceType":"modelInstanceVersion","modelInstanceId":471723,"modelId":487624}],"dockerImageVersionId":31234,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"c0ddd839-4e54-4dc6-823b-d7db8db1f672","_cell_guid":"f53f0eae-4d26-4bb8-89a6-f7450ede2dea","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2026-01-10T13:57:20.255706Z","iopub.execute_input":"2026-01-10T13:57:20.256110Z","iopub.status.idle":"2026-01-10T13:57:20.488206Z","shell.execute_reply.started":"2026-01-10T13:57:20.256079Z","shell.execute_reply":"2026-01-10T13:57:20.487373Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/csiro-biomass-competition-comprehensive-eda/__results__.html\n/kaggle/input/csiro-biomass-competition-comprehensive-eda/__notebook__.ipynb\n/kaggle/input/csiro-biomass-competition-comprehensive-eda/__output__.json\n/kaggle/input/csiro-biomass-competition-comprehensive-eda/custom.css\n/kaggle/input/csiro-biomass-competition-comprehensive-eda/__results___files/__results___15_0.png\n/kaggle/input/csiro-biomass-competition-comprehensive-eda/__results___files/__results___25_0.png\n/kaggle/input/csiro-biomass-competition-comprehensive-eda/__results___files/__results___19_0.png\n/kaggle/input/csiro-biomass-competition-comprehensive-eda/__results___files/__results___31_0.png\n/kaggle/input/csiro-biomass-competition-comprehensive-eda/__results___files/__results___22_0.png\n/kaggle/input/csiro-biomass-competition-comprehensive-eda/__results___files/__results___28_0.png\n/kaggle/input/dinov2/pytorch/giant/1/config.json\n/kaggle/input/dinov2/pytorch/giant/1/preprocessor_config.json\n/kaggle/input/dinov2/pytorch/giant/1/README.md\n/kaggle/input/dinov2/pytorch/giant/1/pytorch_model.bin\n/kaggle/input/dinov2/pytorch/giant/1/.gitattributes\n/kaggle/input/csiro-image-embeddings/train_siglip_embeddings.csv\n/kaggle/input/csiro-image-embeddings/train_dino_embeddings.csv\n/kaggle/input/csiro-datasplit/__results__.html\n/kaggle/input/csiro-datasplit/csiro_data_split.csv\n/kaggle/input/csiro-datasplit/__notebook__.ipynb\n/kaggle/input/csiro-datasplit/__output__.json\n/kaggle/input/csiro-datasplit/custom.css\n/kaggle/input/csiro-mvp-models/model1.pth\n/kaggle/input/csiro-mvp-models/model2.pth\n/kaggle/input/csiro-mvp-models/model10.pth\n/kaggle/input/csiro-mvp-models/model7.pth\n/kaggle/input/csiro-mvp-models/model6.pth\n/kaggle/input/csiro-mvp-models/model4.pth\n/kaggle/input/csiro-mvp-models/model5.pth\n/kaggle/input/csiro-mvp-models/model9.pth\n/kaggle/input/csiro-mvp-models/model3.pth\n/kaggle/input/csiro-mvp-models/model8.pth\n/kaggle/input/csiro-biomass/sample_submission.csv\n/kaggle/input/csiro-biomass/train.csv\n/kaggle/input/csiro-biomass/test.csv\n/kaggle/input/csiro-biomass/test/ID1001187975.jpg\n/kaggle/input/csiro-biomass/train/ID2099464826.jpg\n/kaggle/input/csiro-biomass/train/ID2037861084.jpg\n/kaggle/input/csiro-biomass/train/ID1211362607.jpg\n/kaggle/input/csiro-biomass/train/ID1853508321.jpg\n/kaggle/input/csiro-biomass/train/ID193102215.jpg\n/kaggle/input/csiro-biomass/train/ID698608346.jpg\n/kaggle/input/csiro-biomass/train/ID1859251563.jpg\n/kaggle/input/csiro-biomass/train/ID1880764911.jpg\n/kaggle/input/csiro-biomass/train/ID853954911.jpg\n/kaggle/input/csiro-biomass/train/ID1403107574.jpg\n/kaggle/input/csiro-biomass/train/ID1781353117.jpg\n/kaggle/input/csiro-biomass/train/ID384648061.jpg\n/kaggle/input/csiro-biomass/train/ID1563418511.jpg\n/kaggle/input/csiro-biomass/train/ID2125100696.jpg\n/kaggle/input/csiro-biomass/train/ID482555369.jpg\n/kaggle/input/csiro-biomass/train/ID638711343.jpg\n/kaggle/input/csiro-biomass/train/ID779628955.jpg\n/kaggle/input/csiro-biomass/train/ID1876271942.jpg\n/kaggle/input/csiro-biomass/train/ID1692894460.jpg\n/kaggle/input/csiro-biomass/train/ID746335827.jpg\n/kaggle/input/csiro-biomass/train/ID1136169672.jpg\n/kaggle/input/csiro-biomass/train/ID1471216911.jpg\n/kaggle/input/csiro-biomass/train/ID846154859.jpg\n/kaggle/input/csiro-biomass/train/ID1294770420.jpg\n/kaggle/input/csiro-biomass/train/ID1183807388.jpg\n/kaggle/input/csiro-biomass/train/ID423506847.jpg\n/kaggle/input/csiro-biomass/train/ID1889150649.jpg\n/kaggle/input/csiro-biomass/train/ID1140993511.jpg\n/kaggle/input/csiro-biomass/train/ID1413758094.jpg\n/kaggle/input/csiro-biomass/train/ID1545077474.jpg\n/kaggle/input/csiro-biomass/train/ID95050718.jpg\n/kaggle/input/csiro-biomass/train/ID528010569.jpg\n/kaggle/input/csiro-biomass/train/ID1645161155.jpg\n/kaggle/input/csiro-biomass/train/ID786365141.jpg\n/kaggle/input/csiro-biomass/train/ID896386823.jpg\n/kaggle/input/csiro-biomass/train/ID1025234388.jpg\n/kaggle/input/csiro-biomass/train/ID663006174.jpg\n/kaggle/input/csiro-biomass/train/ID1509266870.jpg\n/kaggle/input/csiro-biomass/train/ID1496750796.jpg\n/kaggle/input/csiro-biomass/train/ID471758347.jpg\n/kaggle/input/csiro-biomass/train/ID740402124.jpg\n/kaggle/input/csiro-biomass/train/ID1624268863.jpg\n/kaggle/input/csiro-biomass/train/ID1098771283.jpg\n/kaggle/input/csiro-biomass/train/ID710341728.jpg\n/kaggle/input/csiro-biomass/train/ID2086966681.jpg\n/kaggle/input/csiro-biomass/train/ID1573329652.jpg\n/kaggle/input/csiro-biomass/train/ID54128926.jpg\n/kaggle/input/csiro-biomass/train/ID50027657.jpg\n/kaggle/input/csiro-biomass/train/ID1559189397.jpg\n/kaggle/input/csiro-biomass/train/ID290369222.jpg\n/kaggle/input/csiro-biomass/train/ID1590632667.jpg\n/kaggle/input/csiro-biomass/train/ID552040066.jpg\n/kaggle/input/csiro-biomass/train/ID488873801.jpg\n/kaggle/input/csiro-biomass/train/ID363069566.jpg\n/kaggle/input/csiro-biomass/train/ID1839139621.jpg\n/kaggle/input/csiro-biomass/train/ID1131079710.jpg\n/kaggle/input/csiro-biomass/train/ID2010625680.jpg\n/kaggle/input/csiro-biomass/train/ID152157478.jpg\n/kaggle/input/csiro-biomass/train/ID1357758282.jpg\n/kaggle/input/csiro-biomass/train/ID1498398599.jpg\n/kaggle/input/csiro-biomass/train/ID679913293.jpg\n/kaggle/input/csiro-biomass/train/ID697718693.jpg\n/kaggle/input/csiro-biomass/train/ID4464212.jpg\n/kaggle/input/csiro-biomass/train/ID1275072698.jpg\n/kaggle/input/csiro-biomass/train/ID1579942839.jpg\n/kaggle/input/csiro-biomass/train/ID799079114.jpg\n/kaggle/input/csiro-biomass/train/ID1415329644.jpg\n/kaggle/input/csiro-biomass/train/ID1510574031.jpg\n/kaggle/input/csiro-biomass/train/ID1078930021.jpg\n/kaggle/input/csiro-biomass/train/ID1456861072.jpg\n/kaggle/input/csiro-biomass/train/ID930534670.jpg\n/kaggle/input/csiro-biomass/train/ID13162390.jpg\n/kaggle/input/csiro-biomass/train/ID567744300.jpg\n/kaggle/input/csiro-biomass/train/ID344618040.jpg\n/kaggle/input/csiro-biomass/train/ID566966892.jpg\n/kaggle/input/csiro-biomass/train/ID1437386574.jpg\n/kaggle/input/csiro-biomass/train/ID667059550.jpg\n/kaggle/input/csiro-biomass/train/ID72895391.jpg\n/kaggle/input/csiro-biomass/train/ID1193692654.jpg\n/kaggle/input/csiro-biomass/train/ID1386202352.jpg\n/kaggle/input/csiro-biomass/train/ID871463897.jpg\n/kaggle/input/csiro-biomass/train/ID2096636211.jpg\n/kaggle/input/csiro-biomass/train/ID2003438517.jpg\n/kaggle/input/csiro-biomass/train/ID21377800.jpg\n/kaggle/input/csiro-biomass/train/ID230058600.jpg\n/kaggle/input/csiro-biomass/train/ID1753847361.jpg\n/kaggle/input/csiro-biomass/train/ID1512751450.jpg\n/kaggle/input/csiro-biomass/train/ID12390962.jpg\n/kaggle/input/csiro-biomass/train/ID1746343319.jpg\n/kaggle/input/csiro-biomass/train/ID978026131.jpg\n/kaggle/input/csiro-biomass/train/ID383231615.jpg\n/kaggle/input/csiro-biomass/train/ID146920896.jpg\n/kaggle/input/csiro-biomass/train/ID1036339023.jpg\n/kaggle/input/csiro-biomass/train/ID1168534540.jpg\n/kaggle/input/csiro-biomass/train/ID1859792585.jpg\n/kaggle/input/csiro-biomass/train/ID1251029854.jpg\n/kaggle/input/csiro-biomass/train/ID1113329413.jpg\n/kaggle/input/csiro-biomass/train/ID1874904894.jpg\n/kaggle/input/csiro-biomass/train/ID1671844336.jpg\n/kaggle/input/csiro-biomass/train/ID1831254380.jpg\n/kaggle/input/csiro-biomass/train/ID1103883611.jpg\n/kaggle/input/csiro-biomass/train/ID797502182.jpg\n/kaggle/input/csiro-biomass/train/ID1784585001.jpg\n/kaggle/input/csiro-biomass/train/ID1058383417.jpg\n/kaggle/input/csiro-biomass/train/ID1488408526.jpg\n/kaggle/input/csiro-biomass/train/ID429799190.jpg\n/kaggle/input/csiro-biomass/train/ID1291116815.jpg\n/kaggle/input/csiro-biomass/train/ID1516374298.jpg\n/kaggle/input/csiro-biomass/train/ID1618597318.jpg\n/kaggle/input/csiro-biomass/train/ID1345375788.jpg\n/kaggle/input/csiro-biomass/train/ID686797154.jpg\n/kaggle/input/csiro-biomass/train/ID1139866256.jpg\n/kaggle/input/csiro-biomass/train/ID1149598723.jpg\n/kaggle/input/csiro-biomass/train/ID212206250.jpg\n/kaggle/input/csiro-biomass/train/ID112966473.jpg\n/kaggle/input/csiro-biomass/train/ID1540480250.jpg\n/kaggle/input/csiro-biomass/train/ID544444725.jpg\n/kaggle/input/csiro-biomass/train/ID1513184765.jpg\n/kaggle/input/csiro-biomass/train/ID668330410.jpg\n/kaggle/input/csiro-biomass/train/ID1444674500.jpg\n/kaggle/input/csiro-biomass/train/ID1962379474.jpg\n/kaggle/input/csiro-biomass/train/ID605134229.jpg\n/kaggle/input/csiro-biomass/train/ID914754166.jpg\n/kaggle/input/csiro-biomass/train/ID354528442.jpg\n/kaggle/input/csiro-biomass/train/ID950496197.jpg\n/kaggle/input/csiro-biomass/train/ID1395011773.jpg\n/kaggle/input/csiro-biomass/train/ID1357768767.jpg\n/kaggle/input/csiro-biomass/train/ID210865340.jpg\n/kaggle/input/csiro-biomass/train/ID936984905.jpg\n/kaggle/input/csiro-biomass/train/ID1976436386.jpg\n/kaggle/input/csiro-biomass/train/ID1215977190.jpg\n/kaggle/input/csiro-biomass/train/ID803479541.jpg\n/kaggle/input/csiro-biomass/train/ID1244346858.jpg\n/kaggle/input/csiro-biomass/train/ID158170916.jpg\n/kaggle/input/csiro-biomass/train/ID1208644039.jpg\n/kaggle/input/csiro-biomass/train/ID1314135397.jpg\n/kaggle/input/csiro-biomass/train/ID1012260530.jpg\n/kaggle/input/csiro-biomass/train/ID1053972079.jpg\n/kaggle/input/csiro-biomass/train/ID656251220.jpg\n/kaggle/input/csiro-biomass/train/ID1084819986.jpg\n/kaggle/input/csiro-biomass/train/ID1337107565.jpg\n/kaggle/input/csiro-biomass/train/ID1268934251.jpg\n/kaggle/input/csiro-biomass/train/ID617132135.jpg\n/kaggle/input/csiro-biomass/train/ID1472525822.jpg\n/kaggle/input/csiro-biomass/train/ID668475812.jpg\n/kaggle/input/csiro-biomass/train/ID681680726.jpg\n/kaggle/input/csiro-biomass/train/ID1476045099.jpg\n/kaggle/input/csiro-biomass/train/ID1570190541.jpg\n/kaggle/input/csiro-biomass/train/ID1403078396.jpg\n/kaggle/input/csiro-biomass/train/ID2030696575.jpg\n/kaggle/input/csiro-biomass/train/ID1782608354.jpg\n/kaggle/input/csiro-biomass/train/ID194823383.jpg\n/kaggle/input/csiro-biomass/train/ID196516535.jpg\n/kaggle/input/csiro-biomass/train/ID212206832.jpg\n/kaggle/input/csiro-biomass/train/ID1638922597.jpg\n/kaggle/input/csiro-biomass/train/ID1457700382.jpg\n/kaggle/input/csiro-biomass/train/ID1989506559.jpg\n/kaggle/input/csiro-biomass/train/ID789169173.jpg\n/kaggle/input/csiro-biomass/train/ID1634731537.jpg\n/kaggle/input/csiro-biomass/train/ID1428837636.jpg\n/kaggle/input/csiro-biomass/train/ID2006686196.jpg\n/kaggle/input/csiro-biomass/train/ID885388135.jpg\n/kaggle/input/csiro-biomass/train/ID1789853061.jpg\n/kaggle/input/csiro-biomass/train/ID1655778545.jpg\n/kaggle/input/csiro-biomass/train/ID697059386.jpg\n/kaggle/input/csiro-biomass/train/ID121331988.jpg\n/kaggle/input/csiro-biomass/train/ID2099742797.jpg\n/kaggle/input/csiro-biomass/train/ID342818398.jpg\n/kaggle/input/csiro-biomass/train/ID317990700.jpg\n/kaggle/input/csiro-biomass/train/ID706288721.jpg\n/kaggle/input/csiro-biomass/train/ID1159071020.jpg\n/kaggle/input/csiro-biomass/train/ID755710743.jpg\n/kaggle/input/csiro-biomass/train/ID1254829053.jpg\n/kaggle/input/csiro-biomass/train/ID475010202.jpg\n/kaggle/input/csiro-biomass/train/ID1693880739.jpg\n/kaggle/input/csiro-biomass/train/ID1894998379.jpg\n/kaggle/input/csiro-biomass/train/ID48303557.jpg\n/kaggle/input/csiro-biomass/train/ID1385921939.jpg\n/kaggle/input/csiro-biomass/train/ID147528735.jpg\n/kaggle/input/csiro-biomass/train/ID407646960.jpg\n/kaggle/input/csiro-biomass/train/ID1035947949.jpg\n/kaggle/input/csiro-biomass/train/ID1119761112.jpg\n/kaggle/input/csiro-biomass/train/ID1988033238.jpg\n/kaggle/input/csiro-biomass/train/ID1857489997.jpg\n/kaggle/input/csiro-biomass/train/ID742198710.jpg\n/kaggle/input/csiro-biomass/train/ID588120964.jpg\n/kaggle/input/csiro-biomass/train/ID431471530.jpg\n/kaggle/input/csiro-biomass/train/ID353424190.jpg\n/kaggle/input/csiro-biomass/train/ID380752847.jpg\n/kaggle/input/csiro-biomass/train/ID2069766023.jpg\n/kaggle/input/csiro-biomass/train/ID600602588.jpg\n/kaggle/input/csiro-biomass/train/ID560946727.jpg\n/kaggle/input/csiro-biomass/train/ID1011485656.jpg\n/kaggle/input/csiro-biomass/train/ID808079729.jpg\n/kaggle/input/csiro-biomass/train/ID1217108125.jpg\n/kaggle/input/csiro-biomass/train/ID1623964968.jpg\n/kaggle/input/csiro-biomass/train/ID980878870.jpg\n/kaggle/input/csiro-biomass/train/ID793526563.jpg\n/kaggle/input/csiro-biomass/train/ID397994621.jpg\n/kaggle/input/csiro-biomass/train/ID975115267.jpg\n/kaggle/input/csiro-biomass/train/ID1237349078.jpg\n/kaggle/input/csiro-biomass/train/ID684383343.jpg\n/kaggle/input/csiro-biomass/train/ID866684633.jpg\n/kaggle/input/csiro-biomass/train/ID1665142816.jpg\n/kaggle/input/csiro-biomass/train/ID2048645043.jpg\n/kaggle/input/csiro-biomass/train/ID1953171547.jpg\n/kaggle/input/csiro-biomass/train/ID1451025862.jpg\n/kaggle/input/csiro-biomass/train/ID71885430.jpg\n/kaggle/input/csiro-biomass/train/ID307060225.jpg\n/kaggle/input/csiro-biomass/train/ID969218269.jpg\n/kaggle/input/csiro-biomass/train/ID980538882.jpg\n/kaggle/input/csiro-biomass/train/ID1028611175.jpg\n/kaggle/input/csiro-biomass/train/ID670276799.jpg\n/kaggle/input/csiro-biomass/train/ID2002797732.jpg\n/kaggle/input/csiro-biomass/train/ID1374789439.jpg\n/kaggle/input/csiro-biomass/train/ID473494649.jpg\n/kaggle/input/csiro-biomass/train/ID1993907137.jpg\n/kaggle/input/csiro-biomass/train/ID1962197151.jpg\n/kaggle/input/csiro-biomass/train/ID828217731.jpg\n/kaggle/input/csiro-biomass/train/ID972274220.jpg\n/kaggle/input/csiro-biomass/train/ID1954669045.jpg\n/kaggle/input/csiro-biomass/train/ID1354190372.jpg\n/kaggle/input/csiro-biomass/train/ID1458758610.jpg\n/kaggle/input/csiro-biomass/train/ID40849327.jpg\n/kaggle/input/csiro-biomass/train/ID1952813879.jpg\n/kaggle/input/csiro-biomass/train/ID572336285.jpg\n/kaggle/input/csiro-biomass/train/ID1473228876.jpg\n/kaggle/input/csiro-biomass/train/ID1963715583.jpg\n/kaggle/input/csiro-biomass/train/ID1463690813.jpg\n/kaggle/input/csiro-biomass/train/ID1899025384.jpg\n/kaggle/input/csiro-biomass/train/ID386216505.jpg\n/kaggle/input/csiro-biomass/train/ID1789265307.jpg\n/kaggle/input/csiro-biomass/train/ID315357834.jpg\n/kaggle/input/csiro-biomass/train/ID2089023774.jpg\n/kaggle/input/csiro-biomass/train/ID520514019.jpg\n/kaggle/input/csiro-biomass/train/ID1970522802.jpg\n/kaggle/input/csiro-biomass/train/ID1139918758.jpg\n/kaggle/input/csiro-biomass/train/ID1051144034.jpg\n/kaggle/input/csiro-biomass/train/ID1370004842.jpg\n/kaggle/input/csiro-biomass/train/ID761508093.jpg\n/kaggle/input/csiro-biomass/train/ID2052993274.jpg\n/kaggle/input/csiro-biomass/train/ID1277756619.jpg\n/kaggle/input/csiro-biomass/train/ID6269659.jpg\n/kaggle/input/csiro-biomass/train/ID1574125908.jpg\n/kaggle/input/csiro-biomass/train/ID135365668.jpg\n/kaggle/input/csiro-biomass/train/ID1182523622.jpg\n/kaggle/input/csiro-biomass/train/ID554314721.jpg\n/kaggle/input/csiro-biomass/train/ID1049634115.jpg\n/kaggle/input/csiro-biomass/train/ID1127246618.jpg\n/kaggle/input/csiro-biomass/train/ID900012207.jpg\n/kaggle/input/csiro-biomass/train/ID574213894.jpg\n/kaggle/input/csiro-biomass/train/ID415656958.jpg\n/kaggle/input/csiro-biomass/train/ID61833032.jpg\n/kaggle/input/csiro-biomass/train/ID2053315094.jpg\n/kaggle/input/csiro-biomass/train/ID550623196.jpg\n/kaggle/input/csiro-biomass/train/ID657448172.jpg\n/kaggle/input/csiro-biomass/train/ID1675365449.jpg\n/kaggle/input/csiro-biomass/train/ID2014192906.jpg\n/kaggle/input/csiro-biomass/train/ID162394992.jpg\n/kaggle/input/csiro-biomass/train/ID968643034.jpg\n/kaggle/input/csiro-biomass/train/ID684062938.jpg\n/kaggle/input/csiro-biomass/train/ID802547515.jpg\n/kaggle/input/csiro-biomass/train/ID294150104.jpg\n/kaggle/input/csiro-biomass/train/ID1618145129.jpg\n/kaggle/input/csiro-biomass/train/ID956512130.jpg\n/kaggle/input/csiro-biomass/train/ID142751858.jpg\n/kaggle/input/csiro-biomass/train/ID325799913.jpg\n/kaggle/input/csiro-biomass/train/ID443091455.jpg\n/kaggle/input/csiro-biomass/train/ID661372352.jpg\n/kaggle/input/csiro-biomass/train/ID1062837331.jpg\n/kaggle/input/csiro-biomass/train/ID498304885.jpg\n/kaggle/input/csiro-biomass/train/ID187238869.jpg\n/kaggle/input/csiro-biomass/train/ID1450399782.jpg\n/kaggle/input/csiro-biomass/train/ID2056023629.jpg\n/kaggle/input/csiro-biomass/train/ID576621307.jpg\n/kaggle/input/csiro-biomass/train/ID1199150612.jpg\n/kaggle/input/csiro-biomass/train/ID1411613934.jpg\n/kaggle/input/csiro-biomass/train/ID105271783.jpg\n/kaggle/input/csiro-biomass/train/ID1703304524.jpg\n/kaggle/input/csiro-biomass/train/ID875119737.jpg\n/kaggle/input/csiro-biomass/train/ID1176292407.jpg\n/kaggle/input/csiro-biomass/train/ID1729002155.jpg\n/kaggle/input/csiro-biomass/train/ID2091439402.jpg\n/kaggle/input/csiro-biomass/train/ID576137678.jpg\n/kaggle/input/csiro-biomass/train/ID1946311744.jpg\n/kaggle/input/csiro-biomass/train/ID1982662138.jpg\n/kaggle/input/csiro-biomass/train/ID983582017.jpg\n/kaggle/input/csiro-biomass/train/ID661817669.jpg\n/kaggle/input/csiro-biomass/train/ID753699705.jpg\n/kaggle/input/csiro-biomass/train/ID1789834546.jpg\n/kaggle/input/csiro-biomass/train/ID529933668.jpg\n/kaggle/input/csiro-biomass/train/ID490139972.jpg\n/kaggle/input/csiro-biomass/train/ID743847993.jpg\n/kaggle/input/csiro-biomass/train/ID7850481.jpg\n/kaggle/input/csiro-biomass/train/ID1088965591.jpg\n/kaggle/input/csiro-biomass/train/ID629980789.jpg\n/kaggle/input/csiro-biomass/train/ID1119739385.jpg\n/kaggle/input/csiro-biomass/train/ID1477176296.jpg\n/kaggle/input/csiro-biomass/train/ID1113121340.jpg\n/kaggle/input/csiro-biomass/train/ID2131261930.jpg\n/kaggle/input/csiro-biomass/train/ID2145635095.jpg\n/kaggle/input/csiro-biomass/train/ID1414371018.jpg\n/kaggle/input/csiro-biomass/train/ID1148666289.jpg\n/kaggle/input/csiro-biomass/train/ID839432753.jpg\n/kaggle/input/csiro-biomass/train/ID157479394.jpg\n/kaggle/input/csiro-biomass/train/ID1761544403.jpg\n/kaggle/input/csiro-biomass/train/ID846984946.jpg\n/kaggle/input/csiro-biomass/train/ID751517087.jpg\n/kaggle/input/csiro-biomass/train/ID577112774.jpg\n/kaggle/input/csiro-biomass/train/ID353997899.jpg\n/kaggle/input/csiro-biomass/train/ID748979397.jpg\n/kaggle/input/csiro-biomass/train/ID1070112260.jpg\n/kaggle/input/csiro-biomass/train/ID1108283583.jpg\n/kaggle/input/csiro-biomass/train/ID1868719645.jpg\n/kaggle/input/csiro-biomass/train/ID1980675327.jpg\n/kaggle/input/csiro-biomass/train/ID1163061745.jpg\n/kaggle/input/csiro-biomass/train/ID1148528732.jpg\n/kaggle/input/csiro-biomass/train/ID534966093.jpg\n/kaggle/input/csiro-biomass/train/ID1717006117.jpg\n/kaggle/input/csiro-biomass/train/ID1953218650.jpg\n/kaggle/input/csiro-biomass/train/ID633775166.jpg\n/kaggle/input/csiro-biomass/train/ID808093827.jpg\n/kaggle/input/csiro-biomass/train/ID1997244125.jpg\n/kaggle/input/csiro-biomass/train/ID1920959057.jpg\n/kaggle/input/csiro-biomass/train/ID1948354837.jpg\n/kaggle/input/csiro-biomass/train/ID364856705.jpg\n/kaggle/input/csiro-biomass/train/ID249042826.jpg\n/kaggle/input/csiro-biomass/train/ID332742639.jpg\n/kaggle/input/csiro-biomass/train/ID1680597197.jpg\n/kaggle/input/csiro-biomass/train/ID1421714468.jpg\n/kaggle/input/csiro-biomass/train/ID905397692.jpg\n/kaggle/input/csiro-biomass/train/ID1782509721.jpg\n/kaggle/input/csiro-biomass/train/ID141370843.jpg\n/kaggle/input/csiro-biomass/train/ID2056982009.jpg\n/kaggle/input/csiro-biomass/train/ID94564238.jpg\n/kaggle/input/csiro-biomass/train/ID8209776.jpg\n/kaggle/input/csiro-biomass/train/ID908524512.jpg\n/kaggle/input/csiro-biomass/train/ID610397481.jpg\n/kaggle/input/csiro-biomass/train/ID750820644.jpg\n/kaggle/input/csiro-biomass/train/ID1515990019.jpg\n/kaggle/input/csiro-biomass/train/ID1547945326.jpg\n/kaggle/input/csiro-biomass/train/ID587125778.jpg\n/kaggle/input/csiro-biomass/train/ID1620371305.jpg\n/kaggle/input/csiro-biomass/train/ID1474775613.jpg\n/kaggle/input/csiro-biomass/train/ID545360459.jpg\n/kaggle/input/csiro-biomass/train/ID1783499590.jpg\n/kaggle/input/csiro-biomass/train/ID1249094008.jpg\n/kaggle/input/csiro-biomass/train/ID1525817840.jpg\n/kaggle/input/csiro-biomass/train/ID227847873.jpg\n/kaggle/input/csiro-biomass/train/ID1052620238.jpg\n/kaggle/input/csiro-biomass/train/ID1888700589.jpg\n/kaggle/input/csiro-biomass/train/ID2052442675.jpg\n/kaggle/input/csiro-biomass/train/ID963903358.jpg\n/kaggle/input/csiro-biomass/train/ID1121692672.jpg\n/kaggle/input/csiro-biomass/train/ID1343327476.jpg\n/kaggle/input/csiro-biomass/train/ID1667778338.jpg\n/kaggle/input/csiro-biomass/train/ID257822026.jpg\n/kaggle/input/csiro/pytorch/default/12/fold_0/metrics.csv\n/kaggle/input/csiro/pytorch/default/12/fold_0/swanlab_info.json\n/kaggle/input/csiro/pytorch/default/12/fold_0/checkpoints/best_loss.pt\n/kaggle/input/csiro/pytorch/default/12/fold_0/checkpoints/best_wr2.pt\n/kaggle/input/csiro/pytorch/default/12/fold_0/checkpoints/last.pt\n/kaggle/input/csiro/pytorch/default/12/fold_4/metrics.csv\n/kaggle/input/csiro/pytorch/default/12/fold_4/swanlab_info.json\n/kaggle/input/csiro/pytorch/default/12/fold_4/checkpoints/best_loss.pt\n/kaggle/input/csiro/pytorch/default/12/fold_4/checkpoints/best_wr2.pt\n/kaggle/input/csiro/pytorch/default/12/fold_4/checkpoints/last.pt\n/kaggle/input/csiro/pytorch/default/12/fold_1/metrics.csv\n/kaggle/input/csiro/pytorch/default/12/fold_1/swanlab_info.json\n/kaggle/input/csiro/pytorch/default/12/fold_1/checkpoints/best_loss.pt\n/kaggle/input/csiro/pytorch/default/12/fold_1/checkpoints/best_wr2.pt\n/kaggle/input/csiro/pytorch/default/12/fold_1/checkpoints/last.pt\n/kaggle/input/csiro/pytorch/default/12/fold_3/metrics.csv\n/kaggle/input/csiro/pytorch/default/12/fold_3/swanlab_info.json\n/kaggle/input/csiro/pytorch/default/12/fold_3/checkpoints/best_loss.pt\n/kaggle/input/csiro/pytorch/default/12/fold_3/checkpoints/best_wr2.pt\n/kaggle/input/csiro/pytorch/default/12/fold_3/checkpoints/last.pt\n/kaggle/input/csiro/pytorch/default/12/fold_2/metrics.csv\n/kaggle/input/csiro/pytorch/default/12/fold_2/swanlab_info.json\n/kaggle/input/csiro/pytorch/default/12/fold_2/checkpoints/best_loss.pt\n/kaggle/input/csiro/pytorch/default/12/fold_2/checkpoints/best_wr2.pt\n/kaggle/input/csiro/pytorch/default/12/fold_2/checkpoints/last.pt\n/kaggle/input/google-siglip-so400m-patch14-384/transformers/default/1/config.json\n/kaggle/input/google-siglip-so400m-patch14-384/transformers/default/1/preprocessor_config.json\n/kaggle/input/google-siglip-so400m-patch14-384/transformers/default/1/spiece.model\n/kaggle/input/google-siglip-so400m-patch14-384/transformers/default/1/README.md\n/kaggle/input/google-siglip-so400m-patch14-384/transformers/default/1/tokenizer.json\n/kaggle/input/google-siglip-so400m-patch14-384/transformers/default/1/tokenizer_config.json\n/kaggle/input/google-siglip-so400m-patch14-384/transformers/default/1/gitattributes\n/kaggle/input/google-siglip-so400m-patch14-384/transformers/default/1/model.safetensors\n/kaggle/input/google-siglip-so400m-patch14-384/transformers/default/1/special_tokens_map.json\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# ================================================================\n# CSIRO BIOMASS COMPETITION - PROFESSIONAL EDA\n# ================================================================\n# Professional visualizations with consistent color scheme\n# Complete statistical analysis with bell curves and annotations\n# ================================================================\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport cv2\nfrom PIL import Image\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom scipy import stats\nfrom scipy.stats import skew, kurtosis, normaltest, pearsonr, spearmanr, norm\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\n# ================================================================\n# PROFESSIONAL STYLING CONFIGURATION\n# ================================================================\n\n# Professional color palette (consistent across all plots)\nCOLORS = {\n    'primary': '#2E4057',      # Dark blue-gray (main)\n    'secondary': '#048A81',     # Teal (accent)\n    'tertiary': '#54C6EB',      # Light blue\n    'highlight': '#D95D39',     # Coral red (emphasis)\n    'neutral': '#8B8B8B',       # Gray\n    'background': '#F5F5F5',    # Light gray background\n    'grid': '#E0E0E0',          # Grid color\n    'text': '#2C3E50'           # Dark text\n}\n\n# Set professional matplotlib style\nplt.style.use('seaborn-v0_8-whitegrid')\nplt.rcParams.update({\n    'figure.figsize': (12, 7),\n    'font.size': 11,\n    'axes.labelsize': 12,\n    'axes.titlesize': 14,\n    'xtick.labelsize': 10,\n    'ytick.labelsize': 10,\n    'legend.fontsize': 10,\n    'figure.titlesize': 16,\n    'axes.facecolor': COLORS['background'],\n    'figure.facecolor': 'white',\n    'axes.edgecolor': COLORS['neutral'],\n    'grid.color': COLORS['grid'],\n    'grid.alpha': 0.3,\n    'text.color': COLORS['text'],\n    'axes.labelcolor': COLORS['text'],\n    'xtick.color': COLORS['text'],\n    'ytick.color': COLORS['text']\n})\n\n# ================================================================\n# CONFIGURATION\n# ================================================================\n\nclass Config:\n    BASE_PATH = Path(\"/kaggle/input/csiro-biomass/\")\n    TRAIN_PATH = BASE_PATH / \"train\"\n    TEST_PATH = BASE_PATH / \"test\"\n    OUTPUT_PATH = Path(\"/kaggle/working/\")\n    \n    TARGET_COLS = ['Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g', 'GDM_g', 'Dry_Total_g']\n    TARGET_NAMES = {\n        'Dry_Green_g': 'Green Biomass',\n        'Dry_Dead_g': 'Dead Biomass',\n        'Dry_Clover_g': 'Clover Biomass',\n        'GDM_g': 'Green Dry Matter',\n        'Dry_Total_g': 'Total Biomass'\n    }\n    \n    TARGET_WEIGHTS = {\n        'Dry_Green_g': 0.1,\n        'Dry_Dead_g': 0.1,\n        'Dry_Clover_g': 0.1,\n        'GDM_g': 0.2,\n        'Dry_Total_g': 0.5,\n    }\n\ncfg = Config()\n\nprint(\"=\"*80)\nprint(\"CSIRO BIOMASS COMPETITION - PROFESSIONAL EDA\")\nprint(\"=\"*80)\n\n# ================================================================\n# LOAD DATA\n# ================================================================\n\nprint(\"\\nLoading data...\")\ntrain_df = pd.read_csv(cfg.BASE_PATH / \"train.csv\")\ntest_df = pd.read_csv(cfg.BASE_PATH / \"test.csv\")\n\nprint(f\"Train shape: {train_df.shape}\")\nprint(f\"Test shape: {test_df.shape}\")\n\n# Pivot to get one row per image\ntrain_pivot = train_df.pivot_table(\n    values='target',\n    index='image_path',\n    columns='target_name',\n    aggfunc='mean'\n).reset_index()\n\nprint(f\"Pivoted train shape: {train_pivot.shape}\")\n\n# ================================================================\n# 1. COMPREHENSIVE TARGET DISTRIBUTION WITH BELL CURVES\n# ================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"Creating Visualization 1: Target Distributions with Normal Curves\")\nprint(\"=\"*80)\n\nfig = plt.figure(figsize=(20, 12))\ngs = fig.add_gridspec(3, 2, hspace=0.3, wspace=0.25)\n\nfor idx, col in enumerate(cfg.TARGET_COLS):\n    row = idx // 2\n    col_idx = idx % 2\n    ax = fig.add_subplot(gs[row, col_idx])\n    \n    values = train_pivot[col].dropna().values\n    \n    # Histogram\n    n, bins, patches = ax.hist(values, bins=40, alpha=0.6, color=COLORS['primary'], \n                                edgecolor='white', linewidth=1.5, density=True,\n                                label='Observed Distribution')\n    \n    # Fit normal distribution\n    mu, sigma = values.mean(), values.std()\n    x = np.linspace(values.min(), values.max(), 100)\n    normal_curve = norm.pdf(x, mu, sigma)\n    \n    # Plot normal curve\n    ax.plot(x, normal_curve, color=COLORS['highlight'], linewidth=3, \n            label=f'Normal Fit (μ={mu:.1f}, σ={sigma:.1f})', linestyle='--')\n    \n    # Statistics lines\n    ax.axvline(mu, color=COLORS['secondary'], linestyle='-', linewidth=2.5, \n               label=f'Mean: {mu:.2f}g', alpha=0.8)\n    ax.axvline(np.median(values), color=COLORS['tertiary'], linestyle='-', linewidth=2.5,\n               label=f'Median: {np.median(values):.2f}g', alpha=0.8)\n    \n    # Annotations\n    ax.text(0.98, 0.97, f'n = {len(values):,}', transform=ax.transAxes,\n            verticalalignment='top', horizontalalignment='right',\n            bbox=dict(boxstyle='round', facecolor='white', alpha=0.9, edgecolor=COLORS['neutral']),\n            fontsize=10, fontweight='bold')\n    \n    # Statistical properties\n    skewness = skew(values)\n    kurt = kurtosis(values)\n    stats_text = f'Skewness: {skewness:.2f}\\nKurtosis: {kurt:.2f}\\nMin: {values.min():.2f}g\\nMax: {values.max():.2f}g'\n    ax.text(0.02, 0.97, stats_text, transform=ax.transAxes,\n            verticalalignment='top', horizontalalignment='left',\n            bbox=dict(boxstyle='round', facecolor='white', alpha=0.9, edgecolor=COLORS['neutral']),\n            fontsize=9)\n    \n    # Labels and title\n    ax.set_xlabel(f'{cfg.TARGET_NAMES[col]} (grams)', fontsize=12, fontweight='bold')\n    ax.set_ylabel('Probability Density', fontsize=12, fontweight='bold')\n    ax.set_title(f'{cfg.TARGET_NAMES[col]} Distribution', \n                 fontsize=14, fontweight='bold', pad=15, color=COLORS['text'])\n    ax.legend(loc='upper right', framealpha=0.95, edgecolor=COLORS['neutral'])\n    ax.grid(True, alpha=0.3, linestyle='--')\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n\n# Add overall title\nfig.suptitle('Target Variable Distributions with Statistical Analysis', \n             fontsize=18, fontweight='bold', y=0.995, color=COLORS['text'])\n\nplt.savefig(cfg.OUTPUT_PATH / 'professional_1_distributions.png', dpi=300, bbox_inches='tight', \n            facecolor='white', edgecolor='none')\nprint(\"✓ Saved: professional_1_distributions.png\")\nplt.close()\n\n# ================================================================\n# 2. COMPREHENSIVE CORRELATION MATRIX  \n# ================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"Creating Visualization 2: Complete Correlation Matrix\")\nprint(\"=\"*80)\n\n# Calculate correlations\ncorrelation_matrix = train_pivot[cfg.TARGET_COLS].corr()\n\n# Create simple figure\nfig, ax = plt.subplots(figsize=(12, 10))\n\n# Professional RED-WHITE-GREEN diverging colormap\n# Red (negative) → White/Yellow (zero) → Green (positive)\nfrom matplotlib.colors import LinearSegmentedColormap\ncolors_cmap = [\n    '#D73027',  # Dark red (strong negative)\n    '#FC8D59',  # Medium red\n    '#FEE090',  # Light red/orange\n    '#FFFFBF',  # White/Yellow (zero)\n    '#E0F3DB',  # Very light green\n    '#A8DDB5',  # Light green\n    '#66C2A4',  # Medium light green\n    '#2CA25F',  # Medium green\n    '#006D2C'   # Dark green (strong positive)\n]\nn_bins = 100\ncmap_diverging = LinearSegmentedColormap.from_list('red_white_green', colors_cmap, N=n_bins)\n\n# Create heatmap with full range -1 to 1\nim = ax.imshow(correlation_matrix, cmap=cmap_diverging, aspect='auto', vmin=-1, vmax=1)\n\n# Set ticks\nax.set_xticks(np.arange(len(cfg.TARGET_COLS)))\nax.set_yticks(np.arange(len(cfg.TARGET_COLS)))\nax.set_xticklabels([cfg.TARGET_NAMES[col] for col in cfg.TARGET_COLS], \n                    rotation=45, ha='right', fontsize=11, fontweight='bold')\nax.set_yticklabels([cfg.TARGET_NAMES[col] for col in cfg.TARGET_COLS], \n                    fontsize=11, fontweight='bold')\n\n# Add correlation values and significance\nfor i in range(len(cfg.TARGET_COLS)):\n    for j in range(len(cfg.TARGET_COLS)):\n        corr_val = correlation_matrix.iloc[i, j]\n        \n        # Calculate p-value\n        if i != j:\n            _, p_val = pearsonr(train_pivot[cfg.TARGET_COLS[i]], \n                               train_pivot[cfg.TARGET_COLS[j]])\n            sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else ''\n        else:\n            sig = ''\n        \n        # Choose text color based on correlation value\n        # Dark text for light colors (near zero), white text for dark colors (strong correlations)\n        if abs(corr_val) > 0.6:\n            text_color = 'white'\n        else:\n            text_color = 'black'\n        \n        # Main correlation value\n        ax.text(j, i, f'{corr_val:.3f}', ha='center', va='center',\n                color=text_color, fontsize=12, fontweight='bold')\n        \n        # Significance stars\n        if sig:\n            ax.text(j, i + 0.35, sig, ha='center', va='center',\n                    color=text_color, fontsize=10, fontweight='bold')\n\n# Colorbar with professional labels\ncbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\ncbar.set_label('Pearson Correlation Coefficient (r)', rotation=270, labelpad=25, \n               fontsize=12, fontweight='bold')\ncbar.ax.tick_params(labelsize=10)\n\n# Add interpretive labels on colorbar\ncbar.ax.text(3.5, -0.9, 'Strong Negative', transform=cbar.ax.transData,\n            fontsize=9, fontweight='bold', color='#D73027', rotation=0)\ncbar.ax.text(3.5, 0.0, 'No Correlation', transform=cbar.ax.transData,\n            fontsize=9, fontweight='bold', color='#666666', rotation=0)\ncbar.ax.text(3.5, 0.9, 'Strong Positive', transform=cbar.ax.transData,\n            fontsize=9, fontweight='bold', color='#006D2C', rotation=0)\n\n# Title with significance legend\ntitle_text = 'Complete Correlation Matrix: All Target Variables\\n'\ntitle_text += 'Red = Negative | Yellow = Zero | Green = Positive  |  '\ntitle_text += '*** p<0.001, ** p<0.01, * p<0.05'\nax.set_title(title_text, fontsize=13, fontweight='bold', pad=20, color=COLORS['text'])\n\n# Grid\nax.set_xticks(np.arange(len(cfg.TARGET_COLS)) - 0.5, minor=True)\nax.set_yticks(np.arange(len(cfg.TARGET_COLS)) - 0.5, minor=True)\nax.grid(which='minor', color='white', linestyle='-', linewidth=2)\n\nplt.tight_layout()\nplt.savefig(cfg.OUTPUT_PATH / 'professional_2_correlation_matrix.png', dpi=300, bbox_inches='tight',\n            facecolor='white', edgecolor='none')\nprint(\"✓ Saved: professional_2_correlation_matrix.png\")\nplt.close()\n\n# ================================================================\n# 3. DETAILED BOX PLOTS WITH STATISTICS\n# ================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"Creating Visualization 3: Detailed Box Plots\")\nprint(\"=\"*80)\n\nfig, ax = plt.subplots(figsize=(16, 8))\n\n# Prepare data\ndata_for_box = [train_pivot[col].dropna().values for col in cfg.TARGET_COLS]\nlabels = [cfg.TARGET_NAMES[col] for col in cfg.TARGET_COLS]\n\n# Create box plot\nbp = ax.boxplot(data_for_box, labels=labels, patch_artist=True,\n                widths=0.6, showmeans=True,\n                meanprops=dict(marker='D', markerfacecolor=COLORS['highlight'], \n                              markeredgecolor='white', markersize=10),\n                medianprops=dict(color=COLORS['secondary'], linewidth=2.5),\n                boxprops=dict(facecolor=COLORS['primary'], alpha=0.7, \n                             edgecolor=COLORS['text'], linewidth=1.5),\n                whiskerprops=dict(color=COLORS['text'], linewidth=1.5),\n                capprops=dict(color=COLORS['text'], linewidth=1.5),\n                flierprops=dict(marker='o', markerfacecolor=COLORS['highlight'], \n                               markersize=5, alpha=0.5, markeredgecolor='none'))\n\n# Add detailed statistics\nfor idx, (col, data) in enumerate(zip(cfg.TARGET_COLS, data_for_box)):\n    # Calculate statistics\n    q1 = np.percentile(data, 25)\n    median = np.median(data)\n    q3 = np.percentile(data, 75)\n    mean = np.mean(data)\n    iqr = q3 - q1\n    lower_whisker = q1 - 1.5 * iqr\n    upper_whisker = q3 + 1.5 * iqr\n    outliers = len(data[(data < lower_whisker) | (data > upper_whisker)])\n    \n    # Add text annotation\n    stats_text = f'Mean: {mean:.1f}g\\nMedian: {median:.1f}g\\nIQR: {iqr:.1f}g\\nOutliers: {outliers}'\n    ax.text(idx + 1, ax.get_ylim()[1] * 0.95, stats_text,\n            ha='center', va='top', fontsize=9,\n            bbox=dict(boxstyle='round', facecolor='white', alpha=0.9, \n                     edgecolor=COLORS['neutral']))\n\n# Labels and formatting\nax.set_ylabel('Biomass (grams)', fontsize=13, fontweight='bold')\nax.set_xlabel('Target Variables', fontsize=13, fontweight='bold')\nax.set_title('Box Plot Analysis: Target Variables with Statistical Details', \n             fontsize=15, fontweight='bold', pad=20, color=COLORS['text'])\n\n# Legend\nlegend_elements = [\n    plt.Line2D([0], [0], marker='D', color='w', label='Mean',\n              markerfacecolor=COLORS['highlight'], markersize=10),\n    plt.Line2D([0], [0], color=COLORS['secondary'], linewidth=2.5, label='Median'),\n    plt.Line2D([0], [0], marker='o', color='w', label='Outliers',\n              markerfacecolor=COLORS['highlight'], markersize=7, alpha=0.5)\n]\nax.legend(handles=legend_elements, loc='upper right', framealpha=0.95, \n         edgecolor=COLORS['neutral'], fontsize=11)\n\nax.grid(True, alpha=0.3, axis='y', linestyle='--')\nax.set_axisbelow(True)\nplt.xticks(rotation=15, ha='right')\n\nplt.tight_layout()\nplt.savefig(cfg.OUTPUT_PATH / 'professional_3_boxplots.png', dpi=300, bbox_inches='tight',\n            facecolor='white', edgecolor='none')\nprint(\"✓ Saved: professional_3_boxplots.png\")\nplt.close()\n\n# ================================================================\n# 4. SCATTER PLOT MATRIX WITH REGRESSION LINES\n# ================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"Creating Visualization 4: Scatter Plot Matrix\")\nprint(\"=\"*80)\n\n# Select subset of targets for clarity\nmain_targets = ['Dry_Green_g', 'Dry_Dead_g', 'GDM_g', 'Dry_Total_g']\nn_targets = len(main_targets)\n\nfig, axes = plt.subplots(n_targets, n_targets, figsize=(18, 18))\n\nfor i, target1 in enumerate(main_targets):\n    for j, target2 in enumerate(main_targets):\n        ax = axes[i, j]\n        \n        if i == j:\n            # Diagonal: histogram with KDE\n            data = train_pivot[target1].dropna().values\n            ax.hist(data, bins=30, alpha=0.6, color=COLORS['primary'], \n                   edgecolor='white', density=True)\n            \n            # KDE\n            from scipy.stats import gaussian_kde\n            kde = gaussian_kde(data)\n            x_range = np.linspace(data.min(), data.max(), 100)\n            ax.plot(x_range, kde(x_range), color=COLORS['highlight'], \n                   linewidth=2.5, label='KDE')\n            \n            ax.set_ylabel('Density', fontsize=9)\n            if i == n_targets - 1:\n                ax.set_xlabel(cfg.TARGET_NAMES[target1], fontsize=10, fontweight='bold')\n        else:\n            # Off-diagonal: scatter plot\n            x_data = train_pivot[target2].dropna()\n            y_data = train_pivot[target1].dropna()\n            \n            # Align data\n            common_idx = x_data.index.intersection(y_data.index)\n            x_vals = x_data.loc[common_idx].values\n            y_vals = y_data.loc[common_idx].values\n            \n            # Scatter\n            ax.scatter(x_vals, y_vals, alpha=0.4, s=20, color=COLORS['primary'], \n                      edgecolors='none')\n            \n            # Regression line\n            if len(x_vals) > 1:\n                z = np.polyfit(x_vals, y_vals, 1)\n                p = np.poly1d(z)\n                x_line = np.linspace(x_vals.min(), x_vals.max(), 100)\n                ax.plot(x_line, p(x_line), color=COLORS['highlight'], \n                       linewidth=2.5, linestyle='--')\n                \n                # Correlation\n                r, p_val = pearsonr(x_vals, y_vals)\n                ax.text(0.05, 0.95, f'r={r:.3f}', transform=ax.transAxes,\n                       verticalalignment='top', fontsize=9, fontweight='bold',\n                       bbox=dict(boxstyle='round', facecolor='white', alpha=0.9))\n        \n        # Labels\n        if j == 0:\n            ax.set_ylabel(cfg.TARGET_NAMES[target1], fontsize=10, fontweight='bold')\n        if i == n_targets - 1:\n            ax.set_xlabel(cfg.TARGET_NAMES[target2], fontsize=10, fontweight='bold')\n        \n        ax.grid(True, alpha=0.2, linestyle='--')\n        ax.tick_params(labelsize=8)\n\nfig.suptitle('Scatter Plot Matrix: Pairwise Relationships Between Targets', \n             fontsize=16, fontweight='bold', y=0.995)\n\nplt.tight_layout()\nplt.savefig(cfg.OUTPUT_PATH / 'professional_4_scatter_matrix.png', dpi=300, bbox_inches='tight',\n            facecolor='white', edgecolor='none')\nprint(\"✓ Saved: professional_4_scatter_matrix.png\")\nplt.close()\n\n# ================================================================\n# 5. DETAILED SAMPLE IMAGE ANALYSIS\n# ================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"Creating Visualization 5: Detailed Sample Image Analysis\")\nprint(\"=\"*80)\n\n# Select 6 sample images\nsample_images = train_pivot['image_path'].sample(min(6, len(train_pivot)), random_state=42).tolist()\n\nfig = plt.figure(figsize=(20, 16))\ngs = fig.add_gridspec(6, 4, hspace=0.35, wspace=0.3)  # 6 rows, 4 columns\n\nfor idx, img_path in enumerate(sample_images):\n    img_name = os.path.basename(img_path)\n    full_path = cfg.TRAIN_PATH / img_name\n    \n    img = cv2.imread(str(full_path))\n    if img is None:\n        continue\n    \n    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img_hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n    \n    # Main image (left column)\n    ax_img = fig.add_subplot(gs[idx, 0])\n    ax_img.imshow(img_rgb)\n    ax_img.set_title(f'Sample {idx+1}', fontsize=12, fontweight='bold')\n    ax_img.axis('off')\n    \n    # Add image properties\n    h, w, _ = img_rgb.shape\n    green_ratio = img_rgb[:,:,1] / (img_rgb.sum(axis=2) + 1)\n    brightness = img_rgb.mean()\n    \n    props_text = f'Size: {w}×{h}\\nBrightness: {brightness:.1f}\\nGreen Ratio: {green_ratio.mean():.3f}'\n    ax_img.text(0.02, 0.98, props_text, transform=ax_img.transAxes,\n               verticalalignment='top', fontsize=9,\n               bbox=dict(boxstyle='round', facecolor='white', alpha=0.9))\n    \n    # RGB histogram\n    ax_hist = fig.add_subplot(gs[idx, 1])\n    colors_rgb = ['#E74C3C', '#27AE60', '#3498DB']  # Red, Green, Blue\n    labels_rgb = ['Red', 'Green', 'Blue']\n    \n    for c, color, label in zip(range(3), colors_rgb, labels_rgb):\n        hist = cv2.calcHist([img_rgb], [c], None, [256], [0, 256])\n        ax_hist.plot(hist, color=color, linewidth=2, label=label, alpha=0.7)\n    \n    ax_hist.set_xlabel('Pixel Value', fontsize=9)\n    ax_hist.set_ylabel('Frequency', fontsize=9)\n    ax_hist.set_title('RGB Distribution', fontsize=10, fontweight='bold')\n    ax_hist.legend(fontsize=8)\n    ax_hist.grid(True, alpha=0.3)\n    ax_hist.set_xlim([0, 256])\n    \n    # Green channel analysis\n    ax_green = fig.add_subplot(gs[idx, 2])\n    green_channel = img_rgb[:,:,1]\n    ax_green.imshow(green_channel, cmap='Greens')\n    ax_green.set_title('Green Channel', fontsize=10, fontweight='bold')\n    ax_green.axis('off')\n    \n    # Vegetation index\n    ax_veg = fig.add_subplot(gs[idx, 3])\n    ax_veg.imshow(green_ratio, cmap='RdYlGn', vmin=0, vmax=0.5)\n    ax_veg.set_title('Vegetation Index', fontsize=10, fontweight='bold')\n    ax_veg.axis('off')\n\nfig.suptitle('Detailed Sample Image Analysis: RGB, Green Channel, and Vegetation Index', \n             fontsize=16, fontweight='bold', y=0.995)\n\nplt.savefig(cfg.OUTPUT_PATH / 'professional_5_sample_images.png', dpi=300, bbox_inches='tight',\n            facecolor='white', edgecolor='none')\nprint(\"✓ Saved: professional_5_sample_images.png\")\nplt.close()\n\n# ================================================================\n# 6. HIERARCHICAL RELATIONSHIP VALIDATION\n# ================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"Creating Visualization 6: Hierarchical Relationships\")\nprint(\"=\"*80)\n\nfig, axes = plt.subplots(1, 2, figsize=(18, 7))\n\n# Green + Clover = GDM\nax = axes[0]\ncalculated = train_pivot['Dry_Green_g'] + train_pivot['Dry_Clover_g']\nactual = train_pivot['GDM_g']\n\nax.scatter(calculated, actual, alpha=0.5, s=40, color=COLORS['primary'], \n          edgecolors='white', linewidth=0.5)\n\n# Perfect match line\nmax_val = max(calculated.max(), actual.max())\nax.plot([0, max_val], [0, max_val], color=COLORS['highlight'], \n       linewidth=3, linestyle='--', label='Perfect Match (y=x)')\n\n# Regression line\nz = np.polyfit(calculated, actual, 1)\np = np.poly1d(z)\nx_line = np.linspace(calculated.min(), calculated.max(), 100)\nax.plot(x_line, p(x_line), color=COLORS['secondary'], \n       linewidth=2.5, label=f'Regression: y={z[0]:.3f}x+{z[1]:.2f}')\n\n# Statistics\nr, p_val = pearsonr(calculated, actual)\nrmse = np.sqrt(((calculated - actual) ** 2).mean())\n\nstats_text = f'Pearson r: {r:.4f}\\np-value: {p_val:.2e}\\nRMSE: {rmse:.2f}g\\nn: {len(calculated):,}'\nax.text(0.05, 0.95, stats_text, transform=ax.transAxes,\n       verticalalignment='top', fontsize=11, fontweight='bold',\n       bbox=dict(boxstyle='round', facecolor='white', alpha=0.95, \n                edgecolor=COLORS['neutral'], linewidth=2))\n\nax.set_xlabel('Green + Clover (calculated, grams)', fontsize=12, fontweight='bold')\nax.set_ylabel('GDM (actual, grams)', fontsize=12, fontweight='bold')\nax.set_title('Hierarchical Validation: Green + Clover = GDM', \n            fontsize=13, fontweight='bold', pad=15)\nax.legend(loc='lower right', fontsize=11, framealpha=0.95)\nax.grid(True, alpha=0.3, linestyle='--')\n\n# GDM + Dead = Total\nax = axes[1]\ncalculated = train_pivot['GDM_g'] + train_pivot['Dry_Dead_g']\nactual = train_pivot['Dry_Total_g']\n\nax.scatter(calculated, actual, alpha=0.5, s=40, color=COLORS['primary'], \n          edgecolors='white', linewidth=0.5)\n\nmax_val = max(calculated.max(), actual.max())\nax.plot([0, max_val], [0, max_val], color=COLORS['highlight'], \n       linewidth=3, linestyle='--', label='Perfect Match (y=x)')\n\nz = np.polyfit(calculated, actual, 1)\np = np.poly1d(z)\nx_line = np.linspace(calculated.min(), calculated.max(), 100)\nax.plot(x_line, p(x_line), color=COLORS['secondary'], \n       linewidth=2.5, label=f'Regression: y={z[0]:.3f}x+{z[1]:.2f}')\n\nr, p_val = pearsonr(calculated, actual)\nrmse = np.sqrt(((calculated - actual) ** 2).mean())\n\nstats_text = f'Pearson r: {r:.4f}\\np-value: {p_val:.2e}\\nRMSE: {rmse:.2f}g\\nn: {len(calculated):,}'\nax.text(0.05, 0.95, stats_text, transform=ax.transAxes,\n       verticalalignment='top', fontsize=11, fontweight='bold',\n       bbox=dict(boxstyle='round', facecolor='white', alpha=0.95, \n                edgecolor=COLORS['neutral'], linewidth=2))\n\nax.set_xlabel('GDM + Dead (calculated, grams)', fontsize=12, fontweight='bold')\nax.set_ylabel('Total Biomass (actual, grams)', fontsize=12, fontweight='bold')\nax.set_title('Hierarchical Validation: GDM + Dead = Total', \n            fontsize=13, fontweight='bold', pad=15)\nax.legend(loc='lower right', fontsize=11, framealpha=0.95)\nax.grid(True, alpha=0.3, linestyle='--')\n\nfig.suptitle('Hierarchical Relationship Validation with Statistical Analysis', \n             fontsize=15, fontweight='bold', y=1.00)\n\nplt.tight_layout()\nplt.savefig(cfg.OUTPUT_PATH / 'professional_6_hierarchical.png', dpi=300, bbox_inches='tight',\n            facecolor='white', edgecolor='none')\nprint(\"✓ Saved: professional_6_hierarchical.png\")\nplt.close()\n\n# ================================================================\n# SUMMARY\n# ================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"EDA COMPLETE - ALL VISUALIZATIONS SAVED\")\nprint(\"=\"*80)\nprint(\"\\nGenerated Professional Visualizations:\")\nprint(\"  1. professional_1_distributions.png - Target distributions with bell curves\")\nprint(\"  2. professional_2_correlation_matrix.png - Complete correlation analysis\")\nprint(\"  3. professional_3_boxplots.png - Detailed box plots with statistics\")\nprint(\"  4. professional_4_scatter_matrix.png - Scatter plot matrix with regression\")\nprint(\"  5. professional_5_sample_images.png - Detailed image analysis\")\nprint(\"  6. professional_6_hierarchical.png - Hierarchical relationship validation\")\nprint(\"\\nAll visualizations use consistent professional color scheme\")\nprint(\"Statistical details and annotations included on all plots\")\nprint(\"=\"*80)","metadata":{"_uuid":"900028d6-5e27-4e09-a86f-dcd3d7d5a2b5","_cell_guid":"4bbbc35a-1b46-41f0-ae38-a4cd5616aeaa","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2026-01-10T13:57:20.643577Z","iopub.execute_input":"2026-01-10T13:57:20.644227Z","iopub.status.idle":"2026-01-10T13:57:43.316668Z","shell.execute_reply.started":"2026-01-10T13:57:20.644198Z","shell.execute_reply":"2026-01-10T13:57:43.315963Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"================================================================================\nCSIRO BIOMASS COMPETITION - PROFESSIONAL EDA\n================================================================================\n\nLoading data...\nTrain shape: (1785, 9)\nTest shape: (5, 3)\nPivoted train shape: (357, 6)\n\n================================================================================\nCreating Visualization 1: Target Distributions with Normal Curves\n================================================================================\n✓ Saved: professional_1_distributions.png\n\n================================================================================\nCreating Visualization 2: Complete Correlation Matrix\n================================================================================\n✓ Saved: professional_2_correlation_matrix.png\n\n================================================================================\nCreating Visualization 3: Detailed Box Plots\n================================================================================\n✓ Saved: professional_3_boxplots.png\n\n================================================================================\nCreating Visualization 4: Scatter Plot Matrix\n================================================================================\n✓ Saved: professional_4_scatter_matrix.png\n\n================================================================================\nCreating Visualization 5: Detailed Sample Image Analysis\n================================================================================\n✓ Saved: professional_5_sample_images.png\n\n================================================================================\nCreating Visualization 6: Hierarchical Relationships\n================================================================================\n✓ Saved: professional_6_hierarchical.png\n\n================================================================================\nEDA COMPLETE - ALL VISUALIZATIONS SAVED\n================================================================================\n\nGenerated Professional Visualizations:\n  1. professional_1_distributions.png - Target distributions with bell curves\n  2. professional_2_correlation_matrix.png - Complete correlation analysis\n  3. professional_3_boxplots.png - Detailed box plots with statistics\n  4. professional_4_scatter_matrix.png - Scatter plot matrix with regression\n  5. professional_5_sample_images.png - Detailed image analysis\n  6. professional_6_hierarchical.png - Hierarchical relationship validation\n\nAll visualizations use consistent professional color scheme\nStatistical details and annotations included on all plots\n================================================================================\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import os\nimport gc\nimport math\nimport json\nimport random\nimport argparse\nimport shutil\nimport time\nfrom copy import deepcopy\nfrom dataclasses import dataclass\nfrom typing import Optional, List, Tuple, Dict\nfrom pathlib import Path\n\nimport cv2\nimport timm\nimport numpy as np\nimport pandas as pd\nimport albumentations as A\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom albumentations.pytorch import ToTensorV2\nfrom PIL import Image\nfrom tqdm.auto import tqdm\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, BayesianRidge\nfrom sklearn.svm import SVR, LinearSVR\nfrom sklearn.ensemble import GradientBoostingRegressor, HistGradientBoostingRegressor, ExtraTreesRegressor\nfrom sklearn.dummy import DummyRegressor\nfrom sklearn.decomposition import PCA\nfrom sklearn.cross_decomposition import PLSRegression\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn import preprocessing\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import StandardScaler\n\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\n\nfrom transformers import AutoProcessor, AutoImageProcessor, AutoModel, AutoTokenizer\n\n# ============================================================================\n# TIMEOUT MANAGER\n# ============================================================================\nclass TimeoutManager:\n    \"\"\"Manages execution time to prevent timeout\"\"\"\n    def __init__(self, max_time_seconds):\n        self.start_time = time.time()\n        self.max_time = max_time_seconds\n        \n    def time_elapsed(self):\n        return time.time() - self.start_time\n        \n    def time_remaining(self):\n        return self.max_time - self.time_elapsed()\n    \n    def should_continue(self, buffer_seconds=300):\n        \"\"\"Check if we have enough time to continue (with 5 min buffer)\"\"\"\n        return self.time_remaining() > buffer_seconds\n    \n    def log_time(self, message=\"\"):\n        elapsed = self.time_elapsed()\n        remaining = self.time_remaining()\n        print(f\"[TIME] {message} | Elapsed: {elapsed/3600:.2f}h | Remaining: {remaining/3600:.2f}h\")\n\n# ============================================================================\n# UTILITY FUNCTIONS\n# ============================================================================\ndef seeding(SEED):\n    np.random.seed(SEED)\n    random.seed(SEED)\n    os.environ['PYTHONHASHSEED'] = str(SEED)\n    torch.manual_seed(SEED)\n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(SEED)\n        torch.cuda.manual_seed_all(SEED)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n\ndef flush():\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        torch.cuda.reset_peak_memory_stats()\n\n@dataclass\nclass Config:\n    DATA_PATH: Path = Path(\"/kaggle/input/csiro-biomass/\")\n    TRAIN_DATA_PATH: Path = DATA_PATH/'train'\n    TEST_DATA_PATH: Path = DATA_PATH/'test'\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    seed = 42\n\ncfg = Config()\nseeding(cfg.seed)\n\nTARGET_NAMES = ['Dry_Clover_g', 'Dry_Dead_g', 'Dry_Green_g', 'Dry_Total_g', 'GDM_g']\nweights = {\n    'Dry_Green_g': 0.1,\n    'Dry_Dead_g': 0.1,\n    'Dry_Clover_g': 0.1,\n    'GDM_g': 0.2,\n    'Dry_Total_g': 0.5,\n}\nTARGET_MAX = {\n    \"Dry_Clover_g\": 71.7865,\n    \"Dry_Dead_g\": 83.8407,\n    \"Dry_Green_g\": 157.9836,\n    \"Dry_Total_g\": 185.70,\n    \"GDM_g\": 157.9836,\n}\n\ndef competition_metric(y_true, y_pred) -> float:\n    y_weighted = 0\n    for l, label in enumerate(TARGET_NAMES):\n        y_weighted = y_weighted + y_true[:, l].mean() * weights[label]\n    ss_res = 0\n    ss_tot = 0\n    for l, label in enumerate(TARGET_NAMES):\n        ss_res = ss_res + ((y_true[:, l] - y_pred[:, l])**2).mean() * weights[label]\n        ss_tot = ss_tot + ((y_true[:, l] - y_weighted)**2).mean() * weights[label]\n    return 1 - ss_res / ss_tot\n\ndef pivot_table(df: pd.DataFrame)->pd.DataFrame:\n    if 'target' in df.columns.tolist():\n        df_pt = pd.pivot_table(\n            df, \n            values='target', \n            index=['image_path', 'Sampling_Date', 'State', 'Species', 'Pre_GSHH_NDVI', 'Height_Ave_cm'], \n            columns='target_name', \n            aggfunc='mean'\n        ).reset_index()\n    else:\n        df['target'] = 0\n        df_pt = pd.pivot_table(\n            df, \n            values='target', \n            index='image_path', \n            columns='target_name', \n            aggfunc='mean'\n        ).reset_index()\n    return df_pt\n\ndef melt_table(df: pd.DataFrame) -> pd.DataFrame:\n    melted = df.melt(\n        id_vars='image_path',\n        value_vars=TARGET_NAMES,\n        var_name='target_name',\n        value_name='target'\n    )\n    melted['sample_id'] = (\n        melted['image_path']\n        .str.replace(r'^.*/', '', regex=True)\n        .str.replace('.jpg', '', regex=False)\n        + '__' + melted['target_name']\n    )\n    return melted[['sample_id', 'image_path', 'target_name', 'target']]\n\ndef post_process_biomass(df_preds):\n    ordered_cols = [\"Dry_Green_g\", \"Dry_Clover_g\", \"Dry_Dead_g\", \"GDM_g\", \"Dry_Total_g\"]\n    Y = df_preds[ordered_cols].values.T\n    C = np.array([[1, 1, 0, -1,  0], [0, 0, 1,  1, -1]])\n    C_T = C.T\n    inv_CCt = np.linalg.inv(C @ C_T)\n    P = np.eye(5) - C_T @ inv_CCt @ C\n    Y_reconciled = P @ Y\n    Y_reconciled = Y_reconciled.T.clip(min=0)\n    df_out = df_preds.copy()\n    df_out[ordered_cols] = Y_reconciled\n    return df_out\n\ndef split_image(image, patch_size=520, overlap=16):\n    h, w, c = image.shape\n    stride = patch_size - overlap\n    patches, coords = [], []\n    for y in range(0, h, stride):\n        for x in range(0, w, stride):\n            y1, x1, y2, x2 = y, x, y + patch_size, x + patch_size\n            patch = image[y1:y2, x1:x2, :]\n            if patch.shape[0] < patch_size or patch.shape[1] < patch_size:\n                pad_h = patch_size - patch.shape[0]\n                pad_w = patch_size - patch.shape[1]\n                patch = np.pad(patch, ((0,pad_h), (0,pad_w), (0,0)), mode='reflect')\n            patches.append(patch)\n            coords.append((y1, x1, y2, x2))\n    return patches, coords\n\ndef get_model(model_path: str, device: str = 'cpu'):\n    model = AutoModel.from_pretrained(model_path, local_files_only=True)\n    processor = AutoImageProcessor.from_pretrained(model_path)\n    return model.eval().to(device), processor\n\ndef compute_embeddings(model_path, df, patch_size=520):\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    model, processor = get_model(model_path=model_path, device=device)\n    IMAGE_PATHS, EMBEDDINGS = [], []\n    for i, row in tqdm(df.iterrows(), total=len(df), desc=\"Computing embeddings\"):\n        img_path = row['image_path']\n        img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n        patches, coords = split_image(img, patch_size=patch_size)\n        images = [Image.fromarray(p).convert(\"RGB\") for p in patches]\n        inputs = processor(images=images, return_tensors=\"pt\").to(model.device)\n        with torch.no_grad():\n            if 'siglip' in model_path:\n                features = model.get_image_features(**inputs)\n            elif 'dino' in model_path:\n                features = model(**inputs).pooler_output\n            else:\n                raise Exception(\"Model should be dino or siglip\")\n        embeds = features.mean(dim=0).detach().cpu().numpy()\n        EMBEDDINGS.append(embeds)\n        IMAGE_PATHS.append(img_path)\n    embeddings = np.stack(EMBEDDINGS, axis=0)\n    n_features = embeddings.shape[1]\n    emb_columns = [f\"emb{i+1}\" for i in range(n_features)]\n    emb_df = pd.DataFrame(embeddings, columns=emb_columns)\n    emb_df['image_path'] = IMAGE_PATHS\n    df_final = df.merge(emb_df, on='image_path', how='left')\n    flush()\n    return df_final\n\ndef generate_semantic_features(image_embeddings, model_path):\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    try:\n        model = AutoModel.from_pretrained(model_path).to(device)\n        tokenizer = AutoTokenizer.from_pretrained(model_path)\n    except Exception as e:\n        return None\n    concept_groups = {\n        \"bare\": [\"bare soil\", \"dirt ground\", \"sparse vegetation\", \"exposed earth\"],\n        \"sparse\": [\"low density pasture\", \"thin grass\", \"short clipped grass\"],\n        \"medium\": [\"average pasture cover\", \"medium height grass\", \"grazed pasture\"],\n        \"dense\": [\"dense tall pasture\", \"thick grassy volume\", \"high biomass\", \"overgrown vegetation\"],\n        \"green\": [\"lush green vibrant pasture\", \"photosynthesizing leaves\", \"fresh growth\"],\n        \"dead\": [\"dry brown dead grass\", \"yellow straw\", \"senesced material\", \"standing hay\"],\n        \"clover\": [\"white clover\", \"trifolium repens\", \"broadleaf legume\", \"clover flowers\"],\n        \"grass\": [\"ryegrass\", \"blade-like leaves\", \"fescue\", \"grassy sward\"],\n        \"weeds\": [\"broadleaf weeds\", \"thistles\", \"non-pasture vegetation\"]\n    }\n    concept_vectors = {}\n    with torch.no_grad():\n        for name, prompts in concept_groups.items():\n            inputs = tokenizer(prompts, padding=\"max_length\", return_tensors=\"pt\").to(device)\n            emb = model.get_text_features(**inputs)\n            emb = emb / emb.norm(p=2, dim=-1, keepdim=True)\n            concept_vectors[name] = emb.mean(dim=0, keepdim=True)\n    if isinstance(image_embeddings, np.ndarray):\n        img_tensor = torch.tensor(image_embeddings, dtype=torch.float32).to(device)\n    else:\n        img_tensor = image_embeddings.to(device)\n    img_tensor = img_tensor / img_tensor.norm(p=2, dim=-1, keepdim=True)\n    scores = {}\n    for name, vec in concept_vectors.items():\n        scores[name] = torch.matmul(img_tensor, vec.T).cpu().numpy().flatten()\n    df_scores = pd.DataFrame(scores)\n    df_scores['ratio_greenness'] = df_scores['green'] / (df_scores['green'] + df_scores['dead'] + 1e-6)\n    df_scores['ratio_clover'] = df_scores['clover'] / (df_scores['clover'] + df_scores['grass'] + 1e-6)\n    df_scores['ratio_cover'] = (df_scores['dense'] + df_scores['medium']) / (df_scores['bare'] + df_scores['sparse'] + 1e-6)\n    df_scores['max_density'] = df_scores[['bare', 'sparse', 'medium', 'dense']].max(axis=1)\n    return df_scores.values\n\nclass SupervisedEmbeddingEngine(BaseEstimator, TransformerMixin):\n    def __init__(self, n_pca=0.98, n_pls=8, n_gmm=5, random_state=42):\n        self.n_pca = n_pca\n        self.n_pls = n_pls\n        self.n_gmm = n_gmm\n        self.random_state = random_state\n        self.scaler = StandardScaler()\n        self.pca = PCA(n_components=n_pca, random_state=random_state)\n        self.pls = PLSRegression(n_components=n_pls, scale=False)\n        self.gmm = GaussianMixture(n_components=n_gmm, covariance_type='diag', random_state=random_state)\n        self.pls_fitted_ = False\n\n    def fit(self, X, y=None, X_semantic=None):\n        X_scaled = self.scaler.fit_transform(X)\n        self.pca.fit(X_scaled)\n        self.gmm.fit(X_scaled)\n        if y is not None:\n            y_clean = y.values if hasattr(y, 'values') else y\n            self.pls.fit(X_scaled, y_clean)\n            self.pls_fitted_ = True\n        return self\n\n    def transform(self, X, X_semantic=None):\n        X_scaled = self.scaler.transform(X)\n        return self._generate_features(X_scaled, X_semantic)\n\n    def _generate_features(self, X_scaled, X_semantic=None):\n        features = []\n        f_pca = self.pca.transform(X_scaled)\n        features.append(f_pca)\n        if self.pls_fitted_:\n            f_pls = self.pls.transform(X_scaled)\n            features.append(f_pls)\n        f_gmm = self.gmm.predict_proba(X_scaled)\n        features.append(f_gmm)\n        if X_semantic is not None:\n            sem_norm = (X_semantic - np.mean(X_semantic, axis=0)) / (np.std(X_semantic, axis=0) + 1e-6)\n            features.append(sem_norm)\n        return np.hstack(features)\n\ndef compare_results(oof, train_data):\n    y_oof_df = pd.DataFrame(oof, columns=TARGET_NAMES)\n    raw_score = competition_metric(train_data[TARGET_NAMES].values, y_oof_df.values)\n    print(f\"Raw CV Score: {raw_score:.6f}\")\n    y_oof_proc = post_process_biomass(y_oof_df)\n    proc_score = competition_metric(train_data[TARGET_NAMES].values, y_oof_proc.values)\n    print(f\"Processed CV Score: {proc_score:.6f}\")\n    print(f\"Improvement: {raw_score - proc_score:.6f}\")\n\ndef cross_validate(model, train_data, test_data, feature_engine, semantic_train=None, semantic_test=None, target_transform='max', seed=42):\n    n_splits = train_data['fold'].nunique()\n    target_max_arr = np.array([TARGET_MAX[t] for t in TARGET_NAMES], dtype=float)\n    y_true = train_data[TARGET_NAMES]\n    y_pred = pd.DataFrame(0.0, index=train_data.index, columns=TARGET_NAMES)\n    y_pred_test = np.zeros([len(test_data), len(TARGET_NAMES)], dtype=float)\n    \n    COLUMNS = [col for col in train_data.columns if col.startswith('emb')]\n    \n    for fold in range(n_splits):\n        seeding(seed*(seed//2 + fold))\n        train_mask = train_data['fold'] != fold\n        valid_mask = train_data['fold'] == fold\n        val_idx = train_data[valid_mask].index\n        X_train_raw = train_data[train_mask][COLUMNS].values\n        X_valid_raw = train_data[valid_mask][COLUMNS].values\n        X_test_raw = test_data[COLUMNS].values\n        sem_train_fold = semantic_train[train_mask] if semantic_train is not None else None\n        sem_valid_fold = semantic_train[valid_mask] if semantic_train is not None else None\n        y_train = train_data[train_mask][TARGET_NAMES].values\n        y_valid = train_data[valid_mask][TARGET_NAMES].values\n        if target_transform == 'log':\n            y_train_proc = np.log1p(y_train)\n        elif target_transform == 'max':\n            y_train_proc = y_train / target_max_arr\n        else:\n            y_train_proc = y_train\n        engine = deepcopy(feature_engine)\n        engine.fit(X_train_raw, y=y_train_proc, X_semantic=sem_train_fold)\n        x_train_eng = engine.transform(X_train_raw, X_semantic=sem_train_fold)\n        x_valid_eng = engine.transform(X_valid_raw, X_semantic=sem_valid_fold)\n        x_test_eng = engine.transform(X_test_raw, X_semantic=semantic_test)\n        fold_valid_pred = np.zeros_like(y_valid)\n        fold_test_pred = np.zeros([len(test_data), len(TARGET_NAMES)])\n        for k in range(len(TARGET_NAMES)):\n            regr = deepcopy(model)\n            regr.fit(x_train_eng, y_train_proc[:, k])\n            pred_valid_raw = regr.predict(x_valid_eng)\n            pred_test_raw = regr.predict(x_test_eng)\n            if target_transform == 'log':\n                pred_valid_inv = np.expm1(pred_valid_raw)\n                pred_test_inv = np.expm1(pred_test_raw)\n            elif target_transform == 'max':\n                pred_valid_inv = (pred_valid_raw * target_max_arr[k])\n                pred_test_inv = (pred_test_raw * target_max_arr[k])\n            else:\n                pred_valid_inv = pred_valid_raw\n                pred_test_inv = pred_test_raw\n            fold_valid_pred[:, k] = pred_valid_inv\n            fold_test_pred[:, k] = pred_test_inv\n        y_pred.loc[val_idx] = fold_valid_pred\n        y_pred_test += fold_test_pred / n_splits\n    full_cv = competition_metric(y_true.values, y_pred.values)\n    print(f\"Full CV Score: {full_cv:.6f}\")\n    return y_pred.values, y_pred_test\n\n# ============================================================================\n# DINO MODEL ARCHITECTURES\n# ============================================================================\nclass FeedForward(nn.Module):\n    def __init__(self, dim, mlp_ratio=4.0, dropout=0.0):\n        super().__init__()\n        hid = int(dim * mlp_ratio)\n        self.net = nn.Sequential(\n            nn.Linear(dim, hid), nn.GELU(), nn.Dropout(dropout),\n            nn.Linear(hid, dim), nn.Dropout(dropout))\n\n    def forward(self, x): return self.net(x)\n\nclass AttentionBlock(nn.Module):\n    def __init__(self, dim, heads=8, dropout=0.0, mlp_ratio=4.0):\n        super().__init__()\n        self.norm1 = nn.LayerNorm(dim)\n        self.attn = nn.MultiheadAttention(dim, heads, dropout=dropout, batch_first=True)\n        self.norm2 = nn.LayerNorm(dim)\n        self.ff = FeedForward(dim, mlp_ratio=mlp_ratio, dropout=dropout)\n\n    def forward(self, x):\n        h = self.norm1(x)\n        attn_out, _ = self.attn(h, h, h, need_weights=False)\n        x = x + attn_out\n        x = x + self.ff(self.norm2(x))\n        return x\n\nclass MobileViTBlock(nn.Module):\n    def __init__(self, dim, heads=4, depth=2, patch=(2, 2), dropout=0.0):\n        super().__init__()\n        self.local = nn.Sequential(\n            nn.Conv2d(dim, dim, 3, padding=1, groups=dim),\n            nn.Conv2d(dim, dim, 1), nn.GELU())\n        self.patch = patch\n        self.transformer = nn.ModuleList([AttentionBlock(dim, heads=heads, dropout=dropout, mlp_ratio=2.0) for _ in range(depth)])\n        self.fuse = nn.Conv2d(dim * 2, dim, kernel_size=1)\n\n    def forward(self, x: torch.Tensor):\n        local_feat = self.local(x)\n        B, C, H, W = local_feat.shape\n        ph, pw = self.patch\n        new_h = math.ceil(H / ph) * ph\n        new_w = math.ceil(W / pw) * pw\n        if new_h != H or new_w != W:\n            local_feat = F.interpolate(local_feat, size=(new_h, new_w), mode=\"bilinear\", align_corners=False)\n            H, W = new_h, new_w\n        tokens = local_feat.unfold(2, ph, ph).unfold(3, pw, pw)\n        tokens = tokens.contiguous().view(B, C, -1, ph, pw)\n        tokens = tokens.permute(0, 2, 3, 4, 1).reshape(B, -1, C)\n        for blk in self.transformer: tokens = blk(tokens)\n        feat = tokens.view(B, -1, ph * pw, C).permute(0, 3, 1, 2)\n        nh = H // ph\n        nw = W // pw\n        feat = feat.view(B, C, nh, nw, ph, pw).permute(0, 1, 2, 4, 3, 5)\n        feat = feat.reshape(B, C, H, W)\n        if feat.shape[-2:] != x.shape[-2:]:\n            feat = F.interpolate(feat, size=x.shape[-2:], mode=\"bilinear\", align_corners=False)\n        return self.fuse(torch.cat([x, feat], dim=1))\n\nclass SpatialReductionAttention(nn.Module):\n    def __init__(self, dim, heads=8, sr_ratio=2, dropout=0.0):\n        super().__init__()\n        self.heads = heads\n        self.scale = (dim // heads) ** -0.5\n        self.q = nn.Linear(dim, dim)\n        self.kv = nn.Linear(dim, dim * 2)\n        self.sr_ratio = sr_ratio\n        if sr_ratio > 1:\n            self.sr = nn.Conv2d(dim, dim, kernel_size=sr_ratio, stride=sr_ratio)\n            self.norm = nn.LayerNorm(dim)\n        else: self.sr = None\n        self.proj = nn.Linear(dim, dim)\n        self.drop = nn.Dropout(dropout)\n\n    def forward(self, x, hw: Tuple[int, int]):\n        B, N, C = x.shape\n        q = self.q(x).reshape(B, N, self.heads, C // self.heads).permute(0, 2, 1, 3)\n        if self.sr is not None:\n            H, W = hw\n            feat = x.transpose(1, 2).reshape(B, C, H, W)\n            feat = self.sr(feat)\n            feat = feat.reshape(B, C, -1).transpose(1, 2)\n            feat = self.norm(feat)\n        else: feat = x\n        kv = self.kv(feat)\n        k, v = kv.chunk(2, dim=-1)\n        k = k.reshape(B, -1, self.heads, C // self.heads).permute(0, 2, 3, 1)\n        v = v.reshape(B, -1, self.heads, C // self.heads).permute(0, 2, 1, 3)\n        attn = torch.matmul(q, k) * self.scale\n        attn = attn.softmax(dim=-1)\n        attn = self.drop(attn)\n        out = torch.matmul(attn, v).permute(0, 2, 1, 3).reshape(B, N, C)\n        return self.proj(out)\n\nclass PVTBlock(nn.Module):\n    def __init__(self, dim, heads=8, sr_ratio=2, dropout=0.0, mlp_ratio=4.0):\n        super().__init__()\n        self.norm1 = nn.LayerNorm(dim)\n        self.sra = SpatialReductionAttention(dim, heads=heads, sr_ratio=sr_ratio, dropout=dropout)\n        self.norm2 = nn.LayerNorm(dim)\n        self.ff = FeedForward(dim, mlp_ratio=mlp_ratio, dropout=dropout)\n\n    def forward(self, x, hw: Tuple[int, int]):\n        x = x + self.sra(self.norm1(x), hw)\n        x = x + self.ff(self.norm2(x))\n        return x\n\nclass LocalMambaBlock(nn.Module):\n    def __init__(self, dim, kernel_size=5, dropout=0.0):\n        super().__init__()\n        self.norm = nn.LayerNorm(dim)\n        self.dwconv = nn.Conv1d(dim, dim, kernel_size=kernel_size, padding=kernel_size//2, groups=dim)\n        self.gate = nn.Linear(dim, dim)\n        self.proj = nn.Linear(dim, dim)\n        self.drop = nn.Dropout(dropout)\n\n    def forward(self, x):\n        shortcut = x\n        x = self.norm(x)\n        g = torch.sigmoid(self.gate(x))\n        x = (x * g).transpose(1, 2)\n        x = self.dwconv(x).transpose(1, 2)\n        x = self.proj(x)\n        x = self.drop(x)\n        return shortcut + x\n\nclass T2TRetokenizer(nn.Module):\n    def __init__(self, dim, depth=2, heads=4, dropout=0.0):\n        super().__init__()\n        self.blocks = nn.ModuleList([AttentionBlock(dim, heads=heads, dropout=dropout, mlp_ratio=2.0) for _ in range(depth)])\n\n    def forward(self, tokens: torch.Tensor, grid_hw: Tuple[int, int]):\n        B, T, C = tokens.shape\n        H, W = grid_hw\n        feat_map = tokens.transpose(1, 2).reshape(B, C, H, W)\n        seq = feat_map.flatten(2).transpose(1, 2)\n        for blk in self.blocks: seq = blk(seq)\n        seq_map = seq.transpose(1, 2).reshape(B, C, H, W)\n        pooled = F.adaptive_avg_pool2d(seq_map, (2, 2))\n        retokens = pooled.flatten(2).transpose(1, 2)\n        return retokens, seq_map\n\nclass CrossScaleFusion(nn.Module):\n    def __init__(self, dim, heads=6, dropout=0.0, layers=2):\n        super().__init__()\n        self.layers_s = nn.ModuleList([AttentionBlock(dim, heads=heads, dropout=dropout, mlp_ratio=2.0) for _ in range(layers)])\n        self.layers_b = nn.ModuleList([AttentionBlock(dim, heads=heads, dropout=dropout, mlp_ratio=2.0) for _ in range(layers)])\n        self.cross_s = nn.ModuleList([nn.MultiheadAttention(dim, heads, dropout=dropout, batch_first=True, kdim=dim, vdim=dim) for _ in range(layers)])\n        self.cross_b = nn.ModuleList([nn.MultiheadAttention(dim, heads, dropout=dropout, batch_first=True, kdim=dim, vdim=dim) for _ in range(layers)])\n        self.norm_s = nn.LayerNorm(dim)\n        self.norm_b = nn.LayerNorm(dim)\n\n    def forward(self, tok_s: torch.Tensor, tok_b: torch.Tensor):\n        B, Ts, C = tok_s.shape\n        Tb = tok_b.shape[1]\n        cls_s = tok_s.new_zeros(B, 1, C)\n        cls_b = tok_b.new_zeros(B, 1, C)\n        tok_s = torch.cat([cls_s, tok_s], dim=1)\n        tok_b = torch.cat([cls_b, tok_b], dim=1)\n        for ls, lb, cs, cb in zip(self.layers_s, self.layers_b, self.cross_s, self.cross_b):\n            tok_s = ls(tok_s)\n            tok_b = lb(tok_b)\n            q_s = self.norm_s(tok_s[:, :1])\n            q_b = self.norm_b(tok_b[:, :1])\n            cls_s_upd, _ = cs(q_s, torch.cat([tok_b, q_b], dim=1), torch.cat([tok_b, q_b], dim=1), need_weights=False)\n            cls_b_upd, _ = cb(q_b, torch.cat([tok_s, q_s], dim=1), torch.cat([tok_s, q_s], dim=1), need_weights=False)\n            tok_s = torch.cat([tok_s[:, :1] + cls_s_upd, tok_s[:, 1:]], dim=1)\n            tok_b = torch.cat([tok_b[:, :1] + cls_b_upd, tok_b[:, 1:]], dim=1)\n        tokens = torch.cat([tok_s[:, :1], tok_b[:, :1], tok_s[:, 1:], tok_b[:, 1:]], dim=1)\n        return tokens\n\nclass TileEncoder(nn.Module):\n    def __init__(self, backbone: nn.Module, input_res: int):\n        super().__init__()\n        self.backbone = backbone\n        self.input_res = input_res\n\n    def forward(self, x: torch.Tensor, grid: Tuple[int, int]):\n        B, C, H, W = x.shape\n        r, c = grid\n        hs = torch.linspace(0, H, steps=r + 1, device=x.device).round().long()\n        ws = torch.linspace(0, W, steps=c + 1, device=x.device).round().long()\n        tiles = []\n        for i in range(r):\n            for j in range(c):\n                rs, re = hs[i].item(), hs[i + 1].item()\n                cs, ce = ws[j].item(), ws[j + 1].item()\n                xt = x[:, :, rs:re, cs:ce]\n                if xt.shape[-2:] != (self.input_res, self.input_res):\n                    xt = F.interpolate(xt, size=(self.input_res, self.input_res), mode=\"bilinear\", align_corners=False)\n                tiles.append(xt)\n        tiles = torch.stack(tiles, dim=1)\n        flat = tiles.view(-1, C, self.input_res, self.input_res)\n        feats = self.backbone(flat)\n        return feats.view(B, -1, feats.shape[-1])\n\nclass PyramidMixer(nn.Module):\n    def __init__(self, dim_in: int, dims: Tuple[int, int, int], mobilevit_heads=4, mobilevit_depth=2, sra_heads=6, sra_ratio=2, mamba_depth=3, mamba_kernel=5, dropout=0.0):\n        super().__init__()\n        c1, c2, c3 = dims\n        self.proj1 = nn.Linear(dim_in, c1)\n        self.mobilevit = MobileViTBlock(c1, heads=mobilevit_heads, depth=mobilevit_depth, dropout=dropout)\n        self.proj2 = nn.Linear(c1, c2)\n        self.pvt = PVTBlock(c2, heads=sra_heads, sr_ratio=sra_ratio, dropout=dropout, mlp_ratio=3.0)\n        self.mamba_local = LocalMambaBlock(c2, kernel_size=mamba_kernel, dropout=dropout)\n        self.proj3 = nn.Linear(c2, c3)\n        self.mamba_global = nn.ModuleList([LocalMambaBlock(c3, kernel_size=mamba_kernel, dropout=dropout) for _ in range(mamba_depth)])\n        self.final_attn = AttentionBlock(c3, heads=min(8, c3//64+1), dropout=dropout, mlp_ratio=2.0)\n\n    def _tokens_to_map(self, tokens: torch.Tensor, target_hw: Tuple[int, int]):\n        B, N, C = tokens.shape\n        H, W = target_hw\n        need = H * W\n        if N < need:\n            pad = tokens.new_zeros(B, need-N, C)\n            tokens = torch.cat([tokens, pad], dim=1)\n        tokens = tokens[:, :need, :]\n        return tokens.transpose(1, 2).reshape(B, C, H, W)\n\n    @staticmethod\n    def _fit_hw(n_tokens: int) -> Tuple[int, int]:\n        h = int(math.sqrt(n_tokens))\n        w = h\n        while h * w < n_tokens:\n            w += 1\n            if h * w < n_tokens: h += 1\n        return h, w\n\n    def forward(self, tokens: torch.Tensor):\n        B, N, C = tokens.shape\n        map_hw = (3, 4)\n        feat_map = self._tokens_to_map(tokens, map_hw)\n        t1 = self.proj1(tokens)\n        m1 = self._tokens_to_map(t1, map_hw)\n        m1 = self.mobilevit(m1)\n        t1_out = m1.flatten(2).transpose(1, 2)[:, :N]\n        t2 = self.proj2(t1_out)\n        new_len = max(4, N//2)\n        t2 = t2[:, :new_len] + F.adaptive_avg_pool1d(t2.transpose(1, 2), new_len).transpose(1, 2)\n        hw2 = self._fit_hw(t2.size(1))\n        if t2.size(1) < hw2[0] * hw2[1]:\n            pad = t2.new_zeros(B, hw2[0]*hw2[1]-t2.size(1), t2.size(2))\n            t2 = torch.cat([t2, pad], dim=1)\n        t2 = self.pvt(t2, hw2)\n        t2 = self.mamba_local(t2)\n        t3 = self.proj3(t2)\n        pooled = torch.stack([t3.mean(dim=1), t3.max(dim=1).values], dim=1)\n        t3 = pooled\n        for blk in self.mamba_global: t3 = blk(t3)\n        t3 = self.final_attn(t3)\n        return t3.mean(dim=1), {\"stage1_map\": m1.detach(), \"stage2_tokens\": t2.detach(), \"stage3_tokens\": t3.detach()}\n\n@dataclass\nclass TrainCFG:\n    dropout: float = 0.1\n    hidden_ratio: float = 0.35\n    dino_candidates: Tuple[str, ...] = (\"vit_base_patch14_dinov2\", \"vit_base_patch14_reg4_dinov2\", \"vit_small_patch14_dinov2\")\n    small_grid: Tuple[int, int] = (4, 4)\n    big_grid: Tuple[int, int] = (2, 2)\n    t2t_depth: int = 2\n    cross_layers: int = 2\n    cross_heads: int = 6\n    pyramid_dims: Tuple[int, int, int] = (384, 512, 640)\n    mobilevit_heads: int = 4\n    mobilevit_depth: int = 2\n    sra_heads: int = 8\n    sra_ratio: int = 2\n    mamba_depth: int = 3\n    mamba_kernel: int = 5\n    aux_head: bool = True\n    aux_loss_weight: float = 0.4\n    ALL_TARGET_COLS: Tuple[str, ...] = (\"Dry_Green_g\", \"Dry_Dead_g\", \"Dry_Clover_g\", \"GDM_g\", \"Dry_Total_g\")\n\nCFG = TrainCFG()\n\ndef update_cfg_from_checkpoint(cfg_dict: dict):\n    global CFG\n    if not cfg_dict: return\n    for k, v in cfg_dict.items():\n        if hasattr(CFG, k): setattr(CFG, k, v)\n\nclass CrossPVT_T2T_MambaDINO(nn.Module):\n    def __init__(self, dropout: float = 0.1, hidden_ratio: float = 0.35):\n        super().__init__()\n        self.backbone, self.feat_dim, self.backbone_name, self.input_res = self._build_dino_backbone()\n        self.tile_encoder = TileEncoder(self.backbone, self.input_res)\n        self.t2t = T2TRetokenizer(self.feat_dim, depth=CFG.t2t_depth, heads=CFG.cross_heads, dropout=dropout)\n        self.cross = CrossScaleFusion(self.feat_dim, heads=CFG.cross_heads, dropout=dropout, layers=CFG.cross_layers)\n        self.pyramid = PyramidMixer(dim_in=self.feat_dim, dims=CFG.pyramid_dims, mobilevit_heads=CFG.mobilevit_heads, mobilevit_depth=CFG.mobilevit_depth, sra_heads=CFG.sra_heads, sra_ratio=CFG.sra_ratio, mamba_depth=CFG.mamba_depth, mamba_kernel=CFG.mamba_kernel, dropout=dropout)\n        self.combined_dim = CFG.pyramid_dims[-1] * 2\n        hidden = max(32, int(self.combined_dim * hidden_ratio))\n        def head(): return nn.Sequential(nn.Linear(self.combined_dim, hidden), nn.GELU(), nn.Dropout(dropout), nn.Linear(hidden, 1))\n        self.head_green = head()\n        self.head_clover = head()\n        self.head_dead = head()\n        self.score_head = nn.Sequential(nn.LayerNorm(self.combined_dim), nn.Linear(self.combined_dim, 1))\n        self.aux_head = nn.Sequential(nn.LayerNorm(CFG.pyramid_dims[1]), nn.Linear(CFG.pyramid_dims[1], 5)) if CFG.aux_head else None\n        self.softplus = nn.Softplus(beta=1.0)\n        self.cross_gate_left = nn.Linear(CFG.pyramid_dims[-1], CFG.pyramid_dims[-1])\n        self.cross_gate_right = nn.Linear(CFG.pyramid_dims[-1], CFG.pyramid_dims[-1])\n\n    def _build_dino_backbone(self):\n        last_err = None\n        for name in CFG.dino_candidates:\n            for gp in [\"token\", \"avg\", \"__default__\"]:\n                try:\n                    if gp == \"__default__\":\n                        m = timm.create_model(name, pretrained=False, num_classes=0)\n                        gp_str = \"default\"\n                    else:\n                        m = timm.create_model(name, pretrained=False, num_classes=0, global_pool=gp)\n                        gp_str = gp\n                    feat = m.num_features\n                    input_res = self._infer_input_res(m)\n                    if hasattr(m, \"set_grad_checkpointing\"):\n                        m.set_grad_checkpointing(True)\n                    return m, feat, name, int(input_res)\n                except Exception as e: last_err = e; continue\n        raise RuntimeError(f\"Cannot create any DINO backbone. Last error: {last_err}\")\n\n    @staticmethod\n    def _infer_input_res(m) -> int:\n        if hasattr(m, \"patch_embed\") and hasattr(m.patch_embed, \"img_size\"):\n            isz = m.patch_embed.img_size\n            return int(isz if isinstance(isz, (int, float)) else isz[0])\n        if hasattr(m, \"img_size\"):\n            isz = m.img_size\n            return int(isz if isinstance(isz, (int, float)) else isz[0])\n        dc = getattr(m, \"default_cfg\", {}) or {}\n        ins = dc.get(\"input_size\", None)\n        if ins:\n            if isinstance(ins, (tuple, list)) and len(ins) >= 2:\n                return int(ins[1])\n            return int(ins if isinstance(ins, (int, float)) else 224)\n        return 518\n\n    def _half_forward(self, x_half: torch.Tensor):\n        tiles_small = self.tile_encoder(x_half, CFG.small_grid)\n        tiles_big = self.tile_encoder(x_half, CFG.big_grid)\n        t2, stage1_map = self.t2t(tiles_small, CFG.small_grid)\n        fused = self.cross(t2, tiles_big)\n        feat, feat_maps = self.pyramid(fused)\n        feat_maps[\"stage1_map\"] = stage1_map\n        return feat, feat_maps\n\n    def _merge_heads(self, f_l: torch.Tensor, f_r: torch.Tensor):\n        g_l = torch.sigmoid(self.cross_gate_left(f_r))\n        g_r = torch.sigmoid(self.cross_gate_right(f_l))\n        f_l = f_l * g_l\n        f_r = f_r * g_r\n        f = torch.cat([f_l, f_r], dim=1)\n        green_pos = self.softplus(self.head_green(f))\n        clover_pos = self.softplus(self.head_clover(f))\n        dead_pos = self.softplus(self.head_dead(f))\n        gdm = green_pos + clover_pos\n        total = gdm + dead_pos\n        return total, gdm, green_pos, f\n\n    def forward(self, *inputs, x_left=None, x_right=None, return_features: bool = False):\n        if inputs:\n            if len(inputs) == 1:\n                first = inputs[0]\n                if isinstance(first, (tuple, list)):\n                    if len(first) >= 1: x_left = first[0]\n                    if len(first) >= 2: x_right = first[1]\n                else: x_left = first\n            else: x_left = inputs[0]; x_right = inputs[1]\n        if x_left is None or (isinstance(x_left, torch.Tensor) and x_left.shape[0] == 0):\n            device = next(self.parameters()).device\n            dtype = next(self.parameters()).dtype\n            zero = torch.zeros(0, 1, device=device, dtype=dtype)\n            out = {\"total\": zero, \"gdm\": zero, \"green\": zero, \"score_feat\": torch.zeros(0, self.combined_dim, device=device, dtype=dtype)}\n            if self.aux_head is not None:\n                out[\"aux\"] = torch.zeros(0, len(CFG.ALL_TARGET_COLS), device=device, dtype=dtype)\n            if return_features: out[\"feature_maps\"] = {}\n            return out\n        if x_right is None:\n            if isinstance(x_left, torch.Tensor) and x_left.shape[1] % 2 == 0:\n                x_left, x_right = torch.chunk(x_left, 2, dim=1)\n            else: raise ValueError(\"Missing x_right input.\")\n        feat_l, feats_l = self._half_forward(x_left)\n        feat_r, feats_r = self._half_forward(x_right)\n        total, gdm, green, f_concat = self._merge_heads(feat_l, feat_r)\n        out = {\"total\": total, \"gdm\": gdm, \"green\": green, \"score_feat\": f_concat}\n        if self.aux_head is not None:\n            aux_tokens = torch.cat([feats_l[\"stage2_tokens\"], feats_r[\"stage2_tokens\"]], dim=1)\n            aux_pred = self.softplus(self.aux_head(aux_tokens.mean(dim=1)))\n            out[\"aux\"] = aux_pred\n        if return_features:\n            out[\"feature_maps\"] = {\"stage1_left\": feats_l.get(\"stage1_map\"), \"stage1_right\": feats_r.get(\"stage1_map\"), \"stage3_left\": feats_l.get(\"stage3_tokens\"), \"stage3_right\": feats_r.get(\"stage3_tokens\")}\n        return out\n\nclass INF_CFG:\n    BASE_PATH = \"/kaggle/input/csiro-biomass\"\n    TEST_CSV = os.path.join(BASE_PATH, \"test.csv\")\n    TEST_IMAGE_DIR = os.path.join(BASE_PATH, \"test\")\n    EXPERIMENT_DIR = \"/kaggle/input/csiro/pytorch/default/12\"\n    CKPT_PATTERN_FOLD_X = os.path.join(EXPERIMENT_DIR, \"fold_{fold}\", \"checkpoints\", \"best_wr2.pt\")\n    CKPT_PATTERN_FOLDX = os.path.join(EXPERIMENT_DIR, \"fold{fold}\", \"checkpoints\", \"best_wr2.pt\")\n    N_FOLDS = 3  # Reduced from 5 to 3 for speed\n    SUBMISSION_FILE = \"submission3.csv\"\n    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    BATCH_SIZE = 4  # Increased from 1 to 4\n    NUM_WORKERS = 0\n    MIXED_PRECISION = True\n    USE_TTA = True\n    TTA_TRANSFORMS = [\"original\", \"hflip\"]  # Reduced from 3 to 2\n    ALL_TARGET_COLS = [\"Dry_Green_g\", \"Dry_Dead_g\", \"Dry_Clover_g\", \"GDM_g\", \"Dry_Total_g\"]\n\nclass TestBiomassDataset(Dataset):\n    def __init__(self, df: pd.DataFrame, transform, image_dir: str):\n        self.df = df.reset_index(drop=True)\n        self.transform = transform\n        self.image_dir = image_dir\n        self.paths = self.df[\"image_path\"].values\n\n    def __len__(self): return len(self.df)\n\n    def __getitem__(self, idx):\n        filename = os.path.basename(self.paths[idx])\n        full_path = os.path.join(self.image_dir, filename)\n        img = cv2.imread(full_path)\n        if img is None: img = np.zeros((1000, 2000, 3), dtype=np.uint8)\n        else: img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        h, w, _ = img.shape\n        mid = w // 2\n        left = img[:, :mid]\n        right = img[:, mid:]\n        left_t = self.transform(image=left)[\"image\"]\n        right_t = self.transform(image=right)[\"image\"]\n        return left_t, right_t\n\ndef get_tta_transforms(img_size: int) -> List[A.Compose]:\n    base = [A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), ToTensorV2()]\n    transforms = []\n    transforms.append(A.Compose([A.Resize(img_size, img_size, interpolation=cv2.INTER_AREA), *base]))\n    transforms.append(A.Compose([A.HorizontalFlip(p=1.0), A.Resize(img_size, img_size, interpolation=cv2.INTER_AREA), *base]))\n    # Removed vertical flip for speed\n    return transforms\n\ndef strip_module_prefix(state_dict: dict) -> dict:\n    if not state_dict: return state_dict\n    keys = list(state_dict.keys())\n    if all(k.startswith(\"module.\") for k in keys):\n        return {k[len(\"module.\"):]: v for k, v in state_dict.items()}\n    return state_dict\n\ndef load_checkpoint(path: str) -> dict:\n    if not os.path.exists(path): raise FileNotFoundError(f\"Checkpoint not found: {path}\")\n    try: state = torch.load(path, map_location=\"cpu\", weights_only=False)\n    except TypeError: state = torch.load(path, map_location=\"cpu\")\n    return state\n\ndef load_model_from_checkpoint(ckpt_path: str) -> nn.Module:\n    state = load_checkpoint(ckpt_path)\n    cfg_dict = state.get(\"cfg\", {})\n    update_cfg_from_checkpoint(cfg_dict)\n    dropout = cfg_dict.get(\"dropout\", CFG.dropout)\n    hidden_ratio = cfg_dict.get(\"hidden_ratio\", CFG.hidden_ratio)\n    model = CrossPVT_T2T_MambaDINO(dropout=dropout, hidden_ratio=hidden_ratio)\n    model_state = state.get(\"model_state\", state)\n    model_state = strip_module_prefix(model_state)\n    missing_keys, unexpected_keys = model.load_state_dict(model_state, strict=False)\n    model.to(INF_CFG.DEVICE)\n    model.eval()\n    input_res = getattr(model, \"input_res\", 518)\n    return model\n\ndef pack5_targets(total: torch.Tensor, gdm: torch.Tensor, green: torch.Tensor) -> torch.Tensor:\n    clover = gdm - green\n    dead = total - gdm\n    return torch.cat([green, dead, clover, gdm, total], dim=1)\n\n@torch.no_grad()\ndef predict_one_view(models: List[nn.Module], loader: DataLoader) -> np.ndarray:\n    preds_list = []\n    amp_dtype = \"cuda\" if INF_CFG.DEVICE.type == \"cuda\" else \"cpu\"\n    for xl, xr in tqdm(loader, desc=\"  Predicting\", leave=False):\n        xl = xl.to(INF_CFG.DEVICE, non_blocking=True)\n        xr = xr.to(INF_CFG.DEVICE, non_blocking=True)\n        x_cat = torch.cat([xl, xr], dim=1)\n        per_model_preds = []\n        with torch.amp.autocast(amp_dtype, enabled=INF_CFG.MIXED_PRECISION):\n            for model in models:\n                out = model(x_cat, return_features=False)\n                total = out[\"total\"]\n                gdm = out[\"gdm\"]\n                green = out[\"green\"]\n                five = pack5_targets(total, gdm, green)\n                five = torch.clamp(five, min=0.0)\n                per_model_preds.append(five.float().cpu())\n        stacked = torch.mean(torch.stack(per_model_preds, dim=0), dim=0)\n        preds_list.append(stacked.numpy())\n    return np.concatenate(preds_list, axis=0)\n\ndef run_inference(test_df: pd.DataFrame, image_dir: str) -> np.ndarray:\n    models = []\n    input_res = None\n    for fold in range(INF_CFG.N_FOLDS):\n        ckpt_path = INF_CFG.CKPT_PATTERN_FOLD_X.format(fold=fold)\n        if not os.path.exists(ckpt_path):\n            ckpt_path = INF_CFG.CKPT_PATTERN_FOLDX.format(fold=fold)\n        if not os.path.exists(ckpt_path): continue\n        model = load_model_from_checkpoint(ckpt_path)\n        models.append(model)\n        if input_res is None: input_res = getattr(model, \"input_res\", 518)\n    if len(models) == 0:\n        raise RuntimeError(\"No checkpoints found!\")\n    if INF_CFG.USE_TTA:\n        tta_transforms = get_tta_transforms(input_res)\n        per_view_preds = []\n        for transform in tta_transforms:\n            ds = TestBiomassDataset(test_df, transform, image_dir)\n            dl = DataLoader(ds, batch_size=INF_CFG.BATCH_SIZE, shuffle=False, num_workers=INF_CFG.NUM_WORKERS, pin_memory=True)\n            view_pred = predict_one_view(models, dl)\n            per_view_preds.append(view_pred)\n        final_pred = np.mean(per_view_preds, axis=0)\n    else:\n        transform = get_tta_transforms(input_res)[0]\n        ds = TestBiomassDataset(test_df, transform, image_dir)\n        dl = DataLoader(ds, batch_size=INF_CFG.BATCH_SIZE, shuffle=False, num_workers=INF_CFG.NUM_WORKERS, pin_memory=True)\n        final_pred = predict_one_view(models, dl)\n    return final_pred\n\ndef create_submission(final_pred: np.ndarray, test_long: pd.DataFrame, test_unique: pd.DataFrame) -> pd.DataFrame:\n    def clean(x):\n        x = np.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0)\n        return np.maximum(0, x)\n    green, dead, clover, gdm, total = map(clean, [final_pred[:,0], final_pred[:,1], final_pred[:,2], final_pred[:,3], final_pred[:,4]])\n    wide = pd.DataFrame({\"image_path\": test_unique[\"image_path\"], \"Dry_Green_g\": green, \"Dry_Dead_g\": dead, \"Dry_Clover_g\": clover, \"GDM_g\": gdm, \"Dry_Total_g\": total})\n    long_preds = wide.melt(id_vars=[\"image_path\"], value_vars=INF_CFG.ALL_TARGET_COLS, var_name=\"target_name\", value_name=\"target\")\n    sub = pd.merge(test_long[[\"sample_id\", \"image_path\", \"target_name\"]], long_preds, on=[\"image_path\", \"target_name\"], how=\"left\")[[\"sample_id\", \"target\"]]\n    sub[\"target\"] = np.nan_to_num(sub[\"target\"], nan=0.0, posinf=0.0, neginf=0.0)\n    sub.to_csv(INF_CFG.SUBMISSION_FILE, index=False)\n    return sub\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"CSIRO v4 CrossPVT T2T Mamba Inference\")\n    parser.add_argument(\"--test-csv\", type=str, default=None)\n    parser.add_argument(\"--test-image-dir\", type=str, default=None)\n    parser.add_argument(\"--experiment-dir\", type=str, default=None)\n    parser.add_argument(\"--output\", type=str, default=None)\n    parser.add_argument(\"--batch-size\", type=int, default=None)\n    parser.add_argument(\"--no-tta\", action=\"store_true\")\n    args, _ = parser.parse_known_args()\n    return args\n\ndef run_dino_inference():\n    args = parse_args()\n    if args.test_csv: INF_CFG.TEST_CSV = args.test_csv\n    if args.test_image_dir: INF_CFG.TEST_IMAGE_DIR = args.test_image_dir\n    if args.experiment_dir:\n        INF_CFG.EXPERIMENT_DIR = args.experiment_dir\n        INF_CFG.CKPT_PATTERN_FOLD_X = os.path.join(INF_CFG.EXPERIMENT_DIR, \"fold_{fold}\", \"checkpoints\", \"best_wr2.pt\")\n        INF_CFG.CKPT_PATTERN_FOLDX = os.path.join(INF_CFG.EXPERIMENT_DIR, \"fold{fold}\", \"checkpoints\", \"best_wr2.pt\")\n    if args.output: INF_CFG.SUBMISSION_FILE = args.output\n    if args.batch_size: INF_CFG.BATCH_SIZE = args.batch_size\n    if args.no_tta: INF_CFG.USE_TTA = False\n    test_long = pd.read_csv(INF_CFG.TEST_CSV)\n    test_unique = test_long.drop_duplicates(subset=[\"image_path\"]).reset_index(drop=True)\n    final_pred = run_inference(test_unique, INF_CFG.TEST_IMAGE_DIR)\n    submission = create_submission(final_pred, test_long, test_unique)\n    gc.collect()\n    if torch.cuda.is_available(): torch.cuda.empty_cache()\n    return submission\n\n# ============================================================================\n# DINO MODEL 2 (MVP)\n# ============================================================================\ndef get_input_size(model):\n    if hasattr(model, \"patch_embed\") and hasattr(model.patch_embed, \"img_size\"):\n        size = model.patch_embed.img_size\n        return int(size if isinstance(size, (int, float)) else size[0])\n    if hasattr(model, \"img_size\"):\n        size = model.img_size\n        return int(size if isinstance(size, (int, float)) else size[0])\n    cfg = getattr(model, \"default_cfg\", {}) or {}\n    input_size = cfg.get(\"input_size\", None)\n    if input_size:\n        if isinstance(input_size, (tuple, list)) and len(input_size) >= 2:\n            return int(input_size[1])\n        return int(input_size if isinstance(input_size, (int, float)) else 224)\n    arch = cfg.get(\"architecture\", \"\") or str(type(model))\n    return 518 if \"dinov2\" in arch.lower() or \"dinov3\" in arch.lower() else 224\n\ndef build_backbone(name):\n    model = timm.create_model(name, pretrained=False, num_classes=0)\n    features = model.num_features\n    input_size = get_input_size(model)\n    return model, features, input_size\n\nclass BaseDINO(nn.Module):\n    def __init__(self, backbone_name):\n        super().__init__()\n        self.backbone, feat_dim, input_size = build_backbone(backbone_name)\n        self.input_size = int(input_size)\n        self.feat_dim = feat_dim\n        self.combined_dim = feat_dim * 2\n        hidden_size = max(8, int(self.combined_dim * 0.25))\n        def make_head():\n            return nn.Sequential(nn.Linear(self.combined_dim, hidden_size), nn.ReLU(inplace=True), nn.Dropout(0.30), nn.Linear(hidden_size, 1))\n        self.head_green = make_head()\n        self.head_clover = make_head()\n        self.head_dead = make_head()\n        self.softplus = nn.Softplus(beta=1.0)\n\n    def merge_features(self, left_feat, right_feat):\n        combined = torch.cat([left_feat, right_feat], dim=1)\n        green = self.softplus(self.head_green(combined))\n        clover = self.softplus(self.head_clover(combined))\n        dead = self.softplus(self.head_dead(combined))\n        gdm = green + clover\n        total = gdm + dead\n        return total, gdm, green\n\nclass TiledFiLMDINO(BaseDINO):\n    def __init__(self, backbone_name):\n        super().__init__(backbone_name)\n        self.grid = (2, 2)\n        class FiLM(nn.Module):\n            def __init__(self, feat_dim):\n                super().__init__()\n                hidden = max(32, feat_dim // 2)\n                self.mlp = nn.Sequential(nn.Linear(feat_dim, hidden), nn.ReLU(inplace=True), nn.Linear(hidden, feat_dim * 2))\n            def forward(self, context):\n                gamma_beta = self.mlp(context)\n                return torch.chunk(gamma_beta, 2, dim=1)\n        self.film_left = FiLM(self.feat_dim)\n        self.film_right = FiLM(self.feat_dim)\n\n    def extract_tile_features(self, x):\n        B, C, H, W = x.shape\n        rows, cols = self.grid\n        def split_dimension(length, parts):\n            step = length // parts\n            segments = []; start = 0\n            for _ in range(parts - 1):\n                segments.append((start, start + step))\n                start += step\n            segments.append((start, length))\n            return segments\n        row_segments = split_dimension(H, rows)\n        col_segments = split_dimension(W, cols)\n        features = []\n        for (rs, re) in row_segments:\n            for (cs, ce) in col_segments:\n                tile = x[:, :, rs:re, cs:ce]\n                if tile.shape[-2:] != (self.input_size, self.input_size):\n                    tile = F.interpolate(tile, size=(self.input_size, self.input_size), mode=\"bilinear\")\n                feat = self.backbone(tile)\n                features.append(feat)\n        return torch.stack(features, dim=0).permute(1, 0, 2)\n\n    def process_stream(self, x, film_layer):\n        tiles = self.extract_tile_features(x)\n        context = tiles.mean(dim=1)\n        gamma, beta = film_layer(context)\n        modulated = tiles * (1 + gamma.unsqueeze(1)) + beta.unsqueeze(1)\n        return modulated.mean(dim=1)\n\n    def forward(self, left_img, right_img):\n        left_feat = self.process_stream(left_img, self.film_left)\n        right_feat = self.process_stream(right_img, self.film_right)\n        return self.merge_features(left_feat, right_feat)\n\ndef clean_state_dict(state_dict):\n    if not state_dict: return state_dict\n    cleaned_dict = {}\n    for k, v in state_dict.items():\n        if k.startswith(\"module.\"): k = k[7:]\n        if k.startswith(\"student.\"): k = k[8:]\n        skip_prefixes = (\"txt_enc.\", \"img_proj.\", \"txt_film\", \"teacher.\", \"momentum_teacher.\")\n        if any(k.startswith(prefix) for prefix in skip_prefixes): continue\n        cleaned_dict[k] = v\n    return cleaned_dict\n\ndef load_model(checkpoint_path, device):\n    if not os.path.exists(checkpoint_path): raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\")\n    try: raw_state = torch.load(checkpoint_path, map_location=device, weights_only=False)\n    except Exception as e: return None\n    if isinstance(raw_state, dict):\n        if 'state_dict' in raw_state: state_dict = raw_state['state_dict']\n        elif 'model' in raw_state: state_dict = raw_state['model']\n        else: state_dict = raw_state\n    else: state_dict = raw_state\n    state_dict = clean_state_dict(state_dict)\n    if not state_dict: return None\n    backbones = [\"vit_base_patch14_reg4_dinov2\", \"vit_base_patch14_reg4_dinov3\", \"vit_base_patch14_dinov3\"]\n    for backbone in backbones:\n        try:\n            model = TiledFiLMDINO(backbone)\n            result = model.load_state_dict(state_dict, strict=False)\n            missing = [k for k in result.missing_keys if not k.startswith('backbone.pos_embed')]\n            if len(missing) == 0:\n                model.to(device); model.eval(); return model\n        except Exception as e: continue\n    return None\n\ndef get_tta_transforms_mvp(img_size):\n    norm = [A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), ToTensorV2()]\n    return [\n        A.Compose([A.Resize(img_size, img_size), *norm]),\n        A.Compose([A.HorizontalFlip(p=1.0), A.Resize(img_size, img_size), *norm]),\n        # Removed vertical flip and rotation for speed\n    ]\n\nclass BiomassDataset(Dataset):\n    def __init__(self, df, transform, img_dir):\n        self.df = df.reset_index(drop=True)\n        self.transform = transform\n        self.img_dir = img_dir\n        self.paths = self.df[\"image_path\"].values\n\n    def __len__(self): return len(self.df)\n\n    def __getitem__(self, idx):\n        filename = os.path.basename(self.paths[idx])\n        full_path = os.path.join(self.img_dir, filename)\n        img = cv2.imread(full_path)\n        if img is None: img = np.zeros((1000, 2000, 3), dtype=np.uint8)\n        else: img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        h, w, _ = img.shape\n        mid = w // 2\n        left_img = img[:, :mid]; right_img = img[:, mid:]\n        left_tensor = self.transform(image=left_img)[\"image\"]\n        right_tensor = self.transform(image=right_img)[\"image\"]\n        return left_tensor, right_tensor\n\n@torch.no_grad()\ndef predict_one_view_mvp(models, loader, device):\n    preds = []; use_amp = device.type == \"cuda\"\n    for left_imgs, right_imgs in tqdm(loader, desc=\"Infer MVP\", leave=False):\n        left_imgs = left_imgs.to(device, non_blocking=True)\n        right_imgs = right_imgs.to(device, non_blocking=True)\n        batch_preds = []\n        with torch.amp.autocast(\"cuda\", enabled=use_amp):\n            for model in models:\n                total, gdm, green = model(left_imgs, right_imgs)\n                dead = torch.clamp(total - gdm, min=0.0)\n                clover = torch.clamp(gdm - green, min=0.0)\n                pred = torch.cat([green, dead, clover, gdm, total], dim=1)\n                batch_preds.append(pred.clamp(0.05, 400.0).cpu())\n        preds.append(torch.stack(batch_preds).mean(dim=0).numpy())\n    return np.concatenate(preds)\n\ndef run_inference_for_ckpts(checkpoint_paths, df, img_dir, device):\n    models = []\n    for ckpt_path in checkpoint_paths:\n        model = load_model(ckpt_path, device)\n        if model is not None: models.append(model)\n    if not models: raise ValueError(f\"No models loaded from {checkpoint_paths}\")\n    input_size = models[0].input_size\n    tta_preds = []\n    for transform in get_tta_transforms_mvp(input_size):\n        ds = BiomassDataset(df, transform, img_dir)\n        dl = DataLoader(ds, batch_size=4, shuffle=False, num_workers=0, pin_memory=True)\n        tta_preds.append(predict_one_view_mvp(models, dl, device))\n    return np.mean(tta_preds, axis=0)\n\ndef create_submission_mvp(final_preds, test_df, unique_df):\n    cols = [\"Dry_Green_g\", \"Dry_Dead_g\", \"Dry_Clover_g\", \"GDM_g\", \"Dry_Total_g\"]\n    wide = pd.DataFrame({\"image_path\": unique_df[\"image_path\"]})\n    for i, col in enumerate(cols): wide[col] = np.clip(final_preds[:, i], 0.05, 400.0)\n    wide[\"GDM_g\"] = wide[\"Dry_Green_g\"] + wide[\"Dry_Clover_g\"]\n    wide[\"Dry_Total_g\"] = wide[\"GDM_g\"] + wide[\"Dry_Dead_g\"]\n    wide[cols] = wide[cols].clip(0.05, 400.0)\n    long_df = wide.melt(id_vars=\"image_path\", value_vars=cols, var_name=\"target_name\", value_name=\"target\")\n    sub = test_df[[\"sample_id\", \"image_path\", \"target_name\"]].merge(long_df, on=[\"image_path\", \"target_name\"], how=\"left\")[[\"sample_id\", \"target\"]]\n    sub.to_csv(\"submission2.csv\", index=False)\n    return sub\n\ndef run_mvp_inference():\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    test_csv = \"/kaggle/input/csiro-biomass/test.csv\"\n    test_img_dir = \"/kaggle/input/csiro-biomass/test\"\n    model_dir = \"/kaggle/input/csiro-mvp-models\"\n    model_paths = [os.path.join(model_dir, f\"model{i}.pth\") for i in range(1, 11)]\n    existing_models = [path for path in model_paths if os.path.exists(path)]\n    \n    # Reduced to only use first 5 models for speed\n    CKPTS_A = existing_models[:5]\n    CKPTS_B = existing_models[5:7] if len(existing_models) > 5 else []  # Use only 2 from second set\n    \n    W_A, W_B = 0.95, 0.075\n    test_df = pd.read_csv(test_csv)\n    unique_df = test_df.drop_duplicates(\"image_path\").reset_index(drop=True)\n    pred_a = run_inference_for_ckpts(CKPTS_A, unique_df, test_img_dir, device)\n    \n    if CKPTS_B:\n        pred_b = run_inference_for_ckpts(CKPTS_B, unique_df, test_img_dir, device)\n        final_preds = W_A * pred_a + W_B * pred_b\n    else:\n        final_preds = pred_a\n    \n    submission = create_submission_mvp(final_preds, test_df, unique_df)\n    return submission\n\n# ============================================================================\n# MAIN FUNCTION WITH TIMEOUT MANAGEMENT\n# ============================================================================\ndef main():\n    # Initialize timeout manager - adjust based on competition limits\n    # Typical Kaggle code competition: 9 hours\n    timeout_mgr = TimeoutManager(max_time_seconds=8.5 * 3600)  # 8.5 hours with 30min buffer\n    \n    seeding(42)\n    timeout_mgr.log_time(\"Started execution\")\n    \n    # Default to sample submission in case of errors\n    try:\n        sample_sub_path = cfg.DATA_PATH / 'sample_submission.csv'\n        if os.path.exists(sample_sub_path):\n            sample_sub = pd.read_csv(sample_sub_path)\n            sample_sub.to_csv('submission.csv', index=False)\n            print(\"Created fallback submission from sample\")\n    except Exception as e:\n        print(f\"Warning: Could not create fallback submission: {e}\")\n    \n    # ========================================================================\n    # PART 1: SigLIP/Ensemble Model (Fastest, most reliable)\n    # ========================================================================\n    if timeout_mgr.should_continue(buffer_seconds=6*3600):  # Need 6+ hours remaining\n        try:\n            print(\"\\n\" + \"=\"*80)\n            print(\"PART 1: SigLIP/Ensemble Model\")\n            print(\"=\"*80)\n            \n            test_df = pd.read_csv(cfg.DATA_PATH/'test.csv')\n            test_df = pivot_table(df=test_df)\n            test_df['image_path'] = test_df['image_path'].apply(lambda p: str(cfg.DATA_PATH / p))\n            \n            # Check if we have pre-computed embeddings to save time\n            train_embeddings_path = \"/kaggle/input/csiro-datasplit/csiro_data_split.csv\"\n            if os.path.exists(train_embeddings_path):\n                train_siglip_df = pd.read_csv(train_embeddings_path)\n                print(\"Loaded pre-computed training embeddings\")\n            else:\n                print(\"Warning: Pre-computed embeddings not found, this will be slow\")\n                train_df = pd.read_csv(cfg.DATA_PATH/'train.csv')\n                train_df = pivot_table(df=train_df)\n                train_df['image_path'] = train_df['image_path'].apply(lambda p: str(cfg.DATA_PATH / p))\n                train_siglip_df = train_df\n            \n            timeout_mgr.log_time(\"Loaded training data\")\n            \n            siglip_path = \"/kaggle/input/google-siglip-so400m-patch14-384/transformers/default/1\"\n            test_siglip_df = compute_embeddings(model_path=siglip_path, df=test_df, patch_size=520)\n            \n            flush()\n            timeout_mgr.log_time(\"Computed test embeddings\")\n            \n            X_all_emb = np.vstack([train_siglip_df.filter(like=\"emb\").values, test_siglip_df.filter(like=\"emb\").values])\n            try:\n                all_semantic_scores = generate_semantic_features(X_all_emb, model_path=siglip_path)\n                n_train = len(train_siglip_df)\n                sem_train_full = all_semantic_scores[:n_train]\n                sem_test_full = all_semantic_scores[n_train:]\n                timeout_mgr.log_time(\"Generated semantic features\")\n            except Exception as e:\n                print(f\"Semantic feature generation failed: {e}\")\n                sem_train_full = None\n                sem_test_full = None\n            \n            feat_engine = SupervisedEmbeddingEngine(n_pca=0.80, n_pls=8, n_gmm=6)\n            \n            # Only run the best 2 models instead of 4\n            print(\"\\nRunning LightGBM...\")\n            oof_lgbm, pred_test_lgbm = cross_validate(\n                LGBMRegressor(verbose=-1, n_estimators=100),  # Reduced estimators\n                train_siglip_df, test_siglip_df, \n                feature_engine=feat_engine, \n                semantic_train=sem_train_full, \n                semantic_test=sem_test_full, \n                target_transform='max'\n            )\n            compare_results(oof_lgbm, train_siglip_df)\n            timeout_mgr.log_time(\"Completed LightGBM\")\n            \n            print(\"\\nRunning CatBoost...\")\n            oof_cat, pred_test_cat = cross_validate(\n                CatBoostRegressor(verbose=0, iterations=100),  # Reduced iterations\n                train_siglip_df, test_siglip_df, \n                feature_engine=feat_engine, \n                semantic_train=sem_train_full, \n                semantic_test=sem_test_full\n            )\n            compare_results(oof_cat, train_siglip_df)\n            timeout_mgr.log_time(\"Completed CatBoost\")\n            \n            # Average only 2 models\n            pred_test = (pred_test_lgbm + pred_test_cat) / 2\n            test_df[TARGET_NAMES] = pred_test\n            test_df = post_process_biomass(test_df)\n            sub_df = melt_table(test_df)\n            sub_df[['sample_id', 'target']].to_csv(\"submission1.csv\", index=False)\n            \n            print(\"✓ SigLIP/Ensemble submission created successfully\")\n            timeout_mgr.log_time(\"Completed SigLIP ensemble\")\n            \n        except Exception as e:\n            print(f\"✗ SigLIP/Ensemble failed: {e}\")\n            # Use sample submission as fallback\n            try:\n                sample = pd.read_csv(cfg.DATA_PATH / 'sample_submission.csv')\n                sample.to_csv(\"submission1.csv\", index=False)\n            except:\n                pass\n    else:\n        print(\"\\n⚠ Skipping SigLIP/Ensemble - insufficient time\")\n        try:\n            sample = pd.read_csv(cfg.DATA_PATH / 'sample_submission.csv')\n            sample.to_csv(\"submission1.csv\", index=False)\n        except:\n            pass\n    \n    # ========================================================================\n    # PART 2: DINO Model 1 (CrossPVT)\n    # ========================================================================\n    if timeout_mgr.should_continue(buffer_seconds=3*3600):  # Need 3+ hours remaining\n        try:\n            print(\"\\n\" + \"=\"*80)\n            print(\"PART 2: DINO Model 1 (CrossPVT)\")\n            print(\"=\"*80)\n            \n            # Disable TTA if time is tight\n            if timeout_mgr.time_remaining() < 4 * 3600:\n                print(\"⚠ Limited time - disabling TTA\")\n                INF_CFG.USE_TTA = False\n            \n            run_dino_inference()\n            print(\"✓ DINO Model 1 submission created successfully\")\n            timeout_mgr.log_time(\"Completed DINO Model 1\")\n            \n        except Exception as e:\n            print(f\"✗ DINO Model 1 failed: {e}\")\n            # Copy submission1 as fallback\n            try:\n                if os.path.exists(\"submission1.csv\"):\n                    shutil.copy(\"submission1.csv\", \"submission3.csv\")\n                else:\n                    sample = pd.read_csv(cfg.DATA_PATH / 'sample_submission.csv')\n                    sample.to_csv(\"submission3.csv\", index=False)\n            except:\n                pass\n    else:\n        print(\"\\n⚠ Skipping DINO Model 1 - insufficient time\")\n        try:\n            if os.path.exists(\"submission1.csv\"):\n                shutil.copy(\"submission1.csv\", \"submission3.csv\")\n            else:\n                sample = pd.read_csv(cfg.DATA_PATH / 'sample_submission.csv')\n                sample.to_csv(\"submission3.csv\", index=False)\n        except:\n            pass\n    \n    # ========================================================================\n    # PART 3: DINO Model 2 (MVP)\n    # ========================================================================\n    if timeout_mgr.should_continue(buffer_seconds=2*3600):  # Need 2+ hours remaining\n        try:\n            print(\"\\n\" + \"=\"*80)\n            print(\"PART 3: DINO Model 2 (MVP)\")\n            print(\"=\"*80)\n            \n            run_mvp_inference()\n            print(\"✓ DINO Model 2 submission created successfully\")\n            timeout_mgr.log_time(\"Completed DINO Model 2\")\n            \n        except Exception as e:\n            print(f\"✗ DINO Model 2 failed: {e}\")\n            # Copy submission1 as fallback\n            try:\n                if os.path.exists(\"submission1.csv\"):\n                    shutil.copy(\"submission1.csv\", \"submission2.csv\")\n                else:\n                    sample = pd.read_csv(cfg.DATA_PATH / 'sample_submission.csv')\n                    sample.to_csv(\"submission2.csv\", index=False)\n            except:\n                pass\n    else:\n        print(\"\\n⚠ Skipping DINO Model 2 - insufficient time\")\n        try:\n            if os.path.exists(\"submission1.csv\"):\n                shutil.copy(\"submission1.csv\", \"submission2.csv\")\n            else:\n                sample = pd.read_csv(cfg.DATA_PATH / 'sample_submission.csv')\n                sample.to_csv(\"submission2.csv\", index=False)\n        except:\n            pass\n    \n    # ========================================================================\n    # PART 4: Ensemble Final Submissions\n    # ========================================================================\n    try:\n        print(\"\\n\" + \"=\"*80)\n        print(\"PART 4: Creating Final Ensemble\")\n        print(\"=\"*80)\n        \n        submission1 = pd.read_csv('submission1.csv')\n        submission2 = pd.read_csv('submission2.csv')\n        submission3 = pd.read_csv('submission3.csv')\n        \n        merged = pd.merge(submission1, submission2, on='sample_id', suffixes=('_1', '_2'))\n        merged = pd.merge(merged, submission3, on='sample_id')\n        if 'target' in merged.columns and 'target_1' in merged.columns and 'target_2' in merged.columns:\n            merged = merged.rename(columns={'target': 'target_3'})\n        \n        # Weighted ensemble\n        weight1, weight2, weight3 = 0.5, 0.25, 0.25\n        merged['target'] = (\n            merged['target_1'] * weight1 + \n            merged['target_2'] * weight2 + \n            merged['target_3'] * weight3\n        )\n        \n        final_submission = merged[['sample_id', 'target']]\n        final_submission.to_csv('submission.csv', index=False)\n        \n        print(\"✓ Final ensemble submission created successfully\")\n        timeout_mgr.log_time(\"Completed final ensemble\")\n        \n    except Exception as e:\n        print(f\"✗ Ensemble failed: {e}\")\n        # Use best available submission as fallback\n        for fallback in ['submission1.csv', 'submission3.csv', 'submission2.csv']:\n            if os.path.exists(fallback):\n                shutil.copy(fallback, '')\n                print(f\"Using {fallback} as final submission\")\n                break\n    \n    timeout_mgr.log_time(\"Execution completed\")\n    print(\"\\n\" + \"=\"*80)\n    print(\"EXECUTION COMPLETE\")\n    print(\"=\"*80)\n\nif __name__ == \"__main__\":\n    try:\n        main()\n    except Exception as e:\n        print(f\"CRITICAL ERROR: {e}\")\n        import traceback\n        traceback.print_exc()\n        # Emergency fallback - create sample submission\n        try:\n            sample_path = \"/kaggle/input/csiro-biomass/sample_submission.csv\"\n            if os.path.exists(sample_path):\n                sample = pd.read_csv(sample_path)\n                sample.to_csv('submission.csv', index=False)\n                print(\"Created emergency fallback submission\")\n        except:\n            print(\"Could not create emergency fallback\")","metadata":{"_uuid":"ea88a7f0-413c-4af6-97d4-9c2453b145ab","_cell_guid":"a181f50e-d2db-476c-812c-be6a36866fdd","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2026-01-10T13:57:43.318407Z","iopub.execute_input":"2026-01-10T13:57:43.318743Z","iopub.status.idle":"2026-01-10T13:58:37.350522Z","shell.execute_reply.started":"2026-01-10T13:57:43.318716Z","shell.execute_reply":"2026-01-10T13:58:37.349936Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"[TIME] Started execution | Elapsed: 0.00h | Remaining: 8.50h\nCreated fallback submission from sample\n\n================================================================================\nPART 1: SigLIP/Ensemble Model\n================================================================================\nLoaded pre-computed training embeddings\n[TIME] Loaded training data | Elapsed: 0.00h | Remaining: 8.50h\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Computing embeddings:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b87923abab5741a398c475e7ccc5a103"}},"metadata":{}},{"name":"stdout","text":"[TIME] Computed test embeddings | Elapsed: 0.00h | Remaining: 8.50h\n[TIME] Generated semantic features | Elapsed: 0.00h | Remaining: 8.50h\n\nRunning LightGBM...\nFull CV Score: 0.669802\nRaw CV Score: 0.669802\nProcessed CV Score: 0.679134\nImprovement: -0.009332\n[TIME] Completed LightGBM | Elapsed: 0.00h | Remaining: 8.50h\n\nRunning CatBoost...\nFull CV Score: 0.654876\nRaw CV Score: 0.654876\nProcessed CV Score: 0.669611\nImprovement: -0.014735\n[TIME] Completed CatBoost | Elapsed: 0.01h | Remaining: 8.49h\n✓ SigLIP/Ensemble submission created successfully\n[TIME] Completed SigLIP ensemble | Elapsed: 0.01h | Remaining: 8.49h\n\n================================================================================\nPART 2: DINO Model 1 (CrossPVT)\n================================================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  Predicting:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  Predicting:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"✓ DINO Model 1 submission created successfully\n[TIME] Completed DINO Model 1 | Elapsed: 0.01h | Remaining: 8.49h\n\n================================================================================\nPART 3: DINO Model 2 (MVP)\n================================================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Infer MVP:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Infer MVP:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Infer MVP:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Infer MVP:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"✓ DINO Model 2 submission created successfully\n[TIME] Completed DINO Model 2 | Elapsed: 0.01h | Remaining: 8.49h\n\n================================================================================\nPART 4: Creating Final Ensemble\n================================================================================\n✓ Final ensemble submission created successfully\n[TIME] Completed final ensemble | Elapsed: 0.01h | Remaining: 8.49h\n[TIME] Execution completed | Elapsed: 0.01h | Remaining: 8.49h\n\n================================================================================\nEXECUTION COMPLETE\n================================================================================\n","output_type":"stream"}],"execution_count":6}]}