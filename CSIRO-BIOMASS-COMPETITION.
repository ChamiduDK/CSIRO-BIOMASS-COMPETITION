{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b8d1ceb",
   "metadata": {
    "_cell_guid": "f53f0eae-4d26-4bb8-89a6-f7450ede2dea",
    "_uuid": "c0ddd839-4e54-4dc6-823b-d7db8db1f672",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-01-11T17:14:37.607726Z",
     "iopub.status.busy": "2026-01-11T17:14:37.606999Z",
     "iopub.status.idle": "2026-01-11T17:14:38.720074Z",
     "shell.execute_reply": "2026-01-11T17:14:38.719357Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 1.122309,
     "end_time": "2026-01-11T17:14:38.724463",
     "exception": false,
     "start_time": "2026-01-11T17:14:37.602154",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/csiro-datasplit/__results__.html\n",
      "/kaggle/input/csiro-datasplit/csiro_data_split.csv\n",
      "/kaggle/input/csiro-datasplit/__notebook__.ipynb\n",
      "/kaggle/input/csiro-datasplit/__output__.json\n",
      "/kaggle/input/csiro-datasplit/custom.css\n",
      "/kaggle/input/csiro-mvp-models/model1.pth\n",
      "/kaggle/input/csiro-mvp-models/model2.pth\n",
      "/kaggle/input/csiro-mvp-models/model10.pth\n",
      "/kaggle/input/csiro-mvp-models/model7.pth\n",
      "/kaggle/input/csiro-mvp-models/model6.pth\n",
      "/kaggle/input/csiro-mvp-models/model4.pth\n",
      "/kaggle/input/csiro-mvp-models/model5.pth\n",
      "/kaggle/input/csiro-mvp-models/model9.pth\n",
      "/kaggle/input/csiro-mvp-models/model3.pth\n",
      "/kaggle/input/csiro-mvp-models/model8.pth\n",
      "/kaggle/input/csiro-biomass/sample_submission.csv\n",
      "/kaggle/input/csiro-biomass/train.csv\n",
      "/kaggle/input/csiro-biomass/test.csv\n",
      "/kaggle/input/csiro-biomass/test/ID1001187975.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2099464826.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2037861084.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1211362607.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1853508321.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID193102215.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID698608346.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1859251563.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1880764911.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID853954911.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1403107574.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1781353117.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID384648061.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1563418511.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2125100696.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID482555369.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID638711343.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID779628955.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1876271942.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1692894460.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID746335827.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1136169672.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1471216911.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID846154859.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1294770420.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1183807388.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID423506847.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1889150649.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1140993511.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1413758094.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1545077474.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID95050718.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID528010569.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1645161155.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID786365141.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID896386823.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1025234388.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID663006174.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1509266870.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1496750796.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID471758347.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID740402124.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1624268863.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1098771283.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID710341728.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2086966681.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1573329652.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID54128926.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID50027657.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1559189397.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID290369222.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1590632667.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID552040066.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID488873801.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID363069566.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1839139621.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1131079710.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2010625680.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID152157478.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1357758282.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1498398599.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID679913293.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID697718693.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID4464212.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1275072698.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1579942839.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID799079114.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1415329644.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1510574031.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1078930021.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1456861072.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID930534670.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID13162390.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID567744300.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID344618040.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID566966892.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1437386574.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID667059550.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID72895391.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1193692654.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1386202352.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID871463897.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2096636211.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2003438517.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID21377800.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID230058600.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1753847361.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1512751450.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID12390962.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1746343319.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID978026131.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID383231615.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID146920896.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1036339023.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1168534540.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1859792585.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1251029854.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1113329413.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1874904894.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1671844336.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1831254380.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1103883611.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID797502182.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1784585001.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1058383417.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1488408526.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID429799190.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1291116815.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1516374298.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1618597318.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1345375788.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID686797154.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1139866256.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1149598723.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID212206250.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID112966473.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1540480250.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID544444725.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1513184765.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID668330410.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1444674500.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1962379474.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID605134229.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID914754166.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID354528442.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID950496197.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1395011773.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1357768767.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID210865340.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID936984905.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1976436386.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1215977190.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID803479541.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1244346858.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID158170916.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1208644039.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1314135397.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1012260530.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1053972079.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID656251220.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1084819986.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1337107565.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1268934251.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID617132135.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1472525822.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID668475812.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID681680726.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1476045099.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1570190541.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1403078396.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2030696575.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1782608354.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID194823383.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID196516535.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID212206832.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1638922597.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1457700382.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1989506559.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID789169173.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1634731537.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1428837636.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2006686196.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID885388135.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1789853061.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1655778545.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID697059386.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID121331988.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2099742797.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID342818398.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID317990700.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID706288721.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1159071020.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID755710743.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1254829053.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID475010202.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1693880739.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1894998379.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID48303557.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1385921939.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID147528735.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID407646960.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1035947949.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1119761112.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1988033238.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1857489997.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID742198710.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID588120964.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID431471530.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID353424190.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID380752847.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2069766023.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID600602588.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID560946727.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1011485656.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID808079729.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1217108125.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1623964968.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID980878870.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID793526563.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID397994621.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID975115267.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1237349078.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID684383343.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID866684633.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1665142816.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2048645043.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1953171547.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1451025862.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID71885430.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID307060225.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID969218269.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID980538882.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1028611175.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID670276799.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2002797732.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1374789439.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID473494649.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1993907137.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1962197151.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID828217731.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID972274220.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1954669045.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1354190372.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1458758610.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID40849327.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1952813879.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID572336285.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1473228876.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1963715583.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1463690813.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1899025384.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID386216505.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1789265307.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID315357834.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2089023774.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID520514019.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1970522802.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1139918758.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1051144034.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1370004842.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID761508093.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2052993274.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1277756619.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID6269659.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1574125908.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID135365668.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1182523622.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID554314721.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1049634115.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1127246618.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID900012207.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID574213894.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID415656958.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID61833032.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2053315094.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID550623196.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID657448172.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1675365449.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2014192906.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID162394992.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID968643034.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID684062938.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID802547515.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID294150104.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1618145129.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID956512130.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID142751858.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID325799913.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID443091455.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID661372352.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1062837331.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID498304885.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID187238869.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1450399782.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2056023629.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID576621307.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1199150612.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1411613934.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID105271783.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1703304524.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID875119737.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1176292407.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1729002155.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2091439402.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID576137678.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1946311744.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1982662138.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID983582017.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID661817669.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID753699705.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1789834546.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID529933668.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID490139972.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID743847993.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID7850481.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1088965591.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID629980789.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1119739385.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1477176296.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1113121340.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2131261930.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2145635095.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1414371018.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1148666289.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID839432753.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID157479394.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1761544403.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID846984946.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID751517087.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID577112774.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID353997899.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID748979397.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1070112260.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1108283583.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1868719645.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1980675327.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1163061745.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1148528732.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID534966093.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1717006117.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1953218650.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID633775166.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID808093827.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1997244125.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1920959057.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1948354837.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID364856705.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID249042826.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID332742639.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1680597197.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1421714468.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID905397692.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1782509721.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID141370843.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2056982009.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID94564238.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID8209776.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID908524512.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID610397481.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID750820644.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1515990019.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1547945326.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID587125778.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1620371305.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1474775613.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID545360459.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1783499590.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1249094008.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1525817840.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID227847873.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1052620238.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1888700589.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2052442675.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID963903358.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1121692672.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1343327476.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1667778338.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID257822026.jpg\n",
      "/kaggle/input/dinov2/pytorch/giant/1/config.json\n",
      "/kaggle/input/dinov2/pytorch/giant/1/preprocessor_config.json\n",
      "/kaggle/input/dinov2/pytorch/giant/1/README.md\n",
      "/kaggle/input/dinov2/pytorch/giant/1/pytorch_model.bin\n",
      "/kaggle/input/dinov2/pytorch/giant/1/.gitattributes\n",
      "/kaggle/input/google-siglip-so400m-patch14-384/transformers/default/1/config.json\n",
      "/kaggle/input/google-siglip-so400m-patch14-384/transformers/default/1/preprocessor_config.json\n",
      "/kaggle/input/google-siglip-so400m-patch14-384/transformers/default/1/spiece.model\n",
      "/kaggle/input/google-siglip-so400m-patch14-384/transformers/default/1/README.md\n",
      "/kaggle/input/google-siglip-so400m-patch14-384/transformers/default/1/tokenizer.json\n",
      "/kaggle/input/google-siglip-so400m-patch14-384/transformers/default/1/tokenizer_config.json\n",
      "/kaggle/input/google-siglip-so400m-patch14-384/transformers/default/1/gitattributes\n",
      "/kaggle/input/google-siglip-so400m-patch14-384/transformers/default/1/model.safetensors\n",
      "/kaggle/input/google-siglip-so400m-patch14-384/transformers/default/1/special_tokens_map.json\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_0/metrics.csv\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_0/swanlab_info.json\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_0/checkpoints/best_loss.pt\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_0/checkpoints/best_wr2.pt\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_0/checkpoints/last.pt\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_4/metrics.csv\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_4/swanlab_info.json\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_4/checkpoints/best_loss.pt\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_4/checkpoints/best_wr2.pt\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_4/checkpoints/last.pt\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_1/metrics.csv\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_1/swanlab_info.json\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_1/checkpoints/best_loss.pt\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_1/checkpoints/best_wr2.pt\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_1/checkpoints/last.pt\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_3/metrics.csv\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_3/swanlab_info.json\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_3/checkpoints/best_loss.pt\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_3/checkpoints/best_wr2.pt\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_3/checkpoints/last.pt\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_2/metrics.csv\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_2/swanlab_info.json\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_2/checkpoints/best_loss.pt\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_2/checkpoints/best_wr2.pt\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_2/checkpoints/last.pt\n",
      "/kaggle/input/csiro-biomass-competition-comprehensive-eda/__results__.html\n",
      "/kaggle/input/csiro-biomass-competition-comprehensive-eda/__notebook__.ipynb\n",
      "/kaggle/input/csiro-biomass-competition-comprehensive-eda/__output__.json\n",
      "/kaggle/input/csiro-biomass-competition-comprehensive-eda/custom.css\n",
      "/kaggle/input/csiro-biomass-competition-comprehensive-eda/__results___files/__results___15_0.png\n",
      "/kaggle/input/csiro-biomass-competition-comprehensive-eda/__results___files/__results___25_0.png\n",
      "/kaggle/input/csiro-biomass-competition-comprehensive-eda/__results___files/__results___19_0.png\n",
      "/kaggle/input/csiro-biomass-competition-comprehensive-eda/__results___files/__results___31_0.png\n",
      "/kaggle/input/csiro-biomass-competition-comprehensive-eda/__results___files/__results___22_0.png\n",
      "/kaggle/input/csiro-biomass-competition-comprehensive-eda/__results___files/__results___28_0.png\n",
      "/kaggle/input/csiro-image-embeddings/train_siglip_embeddings.csv\n",
      "/kaggle/input/csiro-image-embeddings/train_dino_embeddings.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c337f853",
   "metadata": {
    "_cell_guid": "4bbbc35a-1b46-41f0-ae38-a4cd5616aeaa",
    "_uuid": "900028d6-5e27-4e09-a86f-dcd3d7d5a2b5",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-01-11T17:14:38.733400Z",
     "iopub.status.busy": "2026-01-11T17:14:38.733073Z",
     "iopub.status.idle": "2026-01-11T17:15:05.841127Z",
     "shell.execute_reply": "2026-01-11T17:15:05.840163Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 27.114482,
     "end_time": "2026-01-11T17:15:05.842860",
     "exception": false,
     "start_time": "2026-01-11T17:14:38.728378",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CSIRO BIOMASS COMPETITION - PROFESSIONAL EDA\n",
      "================================================================================\n",
      "\n",
      "Loading data...\n",
      "Train shape: (1785, 9)\n",
      "Test shape: (5, 3)\n",
      "Pivoted train shape: (357, 6)\n",
      "\n",
      "================================================================================\n",
      "Creating Visualization 1: Target Distributions with Normal Curves\n",
      "================================================================================\n",
      "✓ Saved: professional_1_distributions.png\n",
      "\n",
      "================================================================================\n",
      "Creating Visualization 2: Complete Correlation Matrix\n",
      "================================================================================\n",
      "✓ Saved: professional_2_correlation_matrix.png\n",
      "\n",
      "================================================================================\n",
      "Creating Visualization 3: Detailed Box Plots\n",
      "================================================================================\n",
      "✓ Saved: professional_3_boxplots.png\n",
      "\n",
      "================================================================================\n",
      "Creating Visualization 4: Scatter Plot Matrix\n",
      "================================================================================\n",
      "✓ Saved: professional_4_scatter_matrix.png\n",
      "\n",
      "================================================================================\n",
      "Creating Visualization 5: Detailed Sample Image Analysis\n",
      "================================================================================\n",
      "✓ Saved: professional_5_sample_images.png\n",
      "\n",
      "================================================================================\n",
      "Creating Visualization 6: Hierarchical Relationships\n",
      "================================================================================\n",
      "✓ Saved: professional_6_hierarchical.png\n",
      "\n",
      "================================================================================\n",
      "EDA COMPLETE - ALL VISUALIZATIONS SAVED\n",
      "================================================================================\n",
      "\n",
      "Generated Professional Visualizations:\n",
      "  1. professional_1_distributions.png - Target distributions with bell curves\n",
      "  2. professional_2_correlation_matrix.png - Complete correlation analysis\n",
      "  3. professional_3_boxplots.png - Detailed box plots with statistics\n",
      "  4. professional_4_scatter_matrix.png - Scatter plot matrix with regression\n",
      "  5. professional_5_sample_images.png - Detailed image analysis\n",
      "  6. professional_6_hierarchical.png - Hierarchical relationship validation\n",
      "\n",
      "All visualizations use consistent professional color scheme\n",
      "Statistical details and annotations included on all plots\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# CSIRO BIOMASS COMPETITION - PROFESSIONAL EDA\n",
    "# ================================================================\n",
    "# Professional visualizations with consistent color scheme\n",
    "# Complete statistical analysis with bell curves and annotations\n",
    "# ================================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import skew, kurtosis, normaltest, pearsonr, spearmanr, norm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# ================================================================\n",
    "# PROFESSIONAL STYLING CONFIGURATION\n",
    "# ================================================================\n",
    "\n",
    "# Professional color palette (consistent across all plots)\n",
    "COLORS = {\n",
    "    'primary': '#2E4057',      # Dark blue-gray (main)\n",
    "    'secondary': '#048A81',     # Teal (accent)\n",
    "    'tertiary': '#54C6EB',      # Light blue\n",
    "    'highlight': '#D95D39',     # Coral red (emphasis)\n",
    "    'neutral': '#8B8B8B',       # Gray\n",
    "    'background': '#F5F5F5',    # Light gray background\n",
    "    'grid': '#E0E0E0',          # Grid color\n",
    "    'text': '#2C3E50'           # Dark text\n",
    "}\n",
    "\n",
    "# Set professional matplotlib style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams.update({\n",
    "    'figure.figsize': (12, 7),\n",
    "    'font.size': 11,\n",
    "    'axes.labelsize': 12,\n",
    "    'axes.titlesize': 14,\n",
    "    'xtick.labelsize': 10,\n",
    "    'ytick.labelsize': 10,\n",
    "    'legend.fontsize': 10,\n",
    "    'figure.titlesize': 16,\n",
    "    'axes.facecolor': COLORS['background'],\n",
    "    'figure.facecolor': 'white',\n",
    "    'axes.edgecolor': COLORS['neutral'],\n",
    "    'grid.color': COLORS['grid'],\n",
    "    'grid.alpha': 0.3,\n",
    "    'text.color': COLORS['text'],\n",
    "    'axes.labelcolor': COLORS['text'],\n",
    "    'xtick.color': COLORS['text'],\n",
    "    'ytick.color': COLORS['text']\n",
    "})\n",
    "\n",
    "# ================================================================\n",
    "# CONFIGURATION\n",
    "# ================================================================\n",
    "\n",
    "class Config:\n",
    "    BASE_PATH = Path(\"/kaggle/input/csiro-biomass/\")\n",
    "    TRAIN_PATH = BASE_PATH / \"train\"\n",
    "    TEST_PATH = BASE_PATH / \"test\"\n",
    "    OUTPUT_PATH = Path(\"/kaggle/working/\")\n",
    "    \n",
    "    TARGET_COLS = ['Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g', 'GDM_g', 'Dry_Total_g']\n",
    "    TARGET_NAMES = {\n",
    "        'Dry_Green_g': 'Green Biomass',\n",
    "        'Dry_Dead_g': 'Dead Biomass',\n",
    "        'Dry_Clover_g': 'Clover Biomass',\n",
    "        'GDM_g': 'Green Dry Matter',\n",
    "        'Dry_Total_g': 'Total Biomass'\n",
    "    }\n",
    "    \n",
    "    TARGET_WEIGHTS = {\n",
    "        'Dry_Green_g': 0.1,\n",
    "        'Dry_Dead_g': 0.1,\n",
    "        'Dry_Clover_g': 0.1,\n",
    "        'GDM_g': 0.2,\n",
    "        'Dry_Total_g': 0.5,\n",
    "    }\n",
    "\n",
    "cfg = Config()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CSIRO BIOMASS COMPETITION - PROFESSIONAL EDA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ================================================================\n",
    "# LOAD DATA\n",
    "# ================================================================\n",
    "\n",
    "print(\"\\nLoading data...\")\n",
    "train_df = pd.read_csv(cfg.BASE_PATH / \"train.csv\")\n",
    "test_df = pd.read_csv(cfg.BASE_PATH / \"test.csv\")\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")\n",
    "\n",
    "# Pivot to get one row per image\n",
    "train_pivot = train_df.pivot_table(\n",
    "    values='target',\n",
    "    index='image_path',\n",
    "    columns='target_name',\n",
    "    aggfunc='mean'\n",
    ").reset_index()\n",
    "\n",
    "print(f\"Pivoted train shape: {train_pivot.shape}\")\n",
    "\n",
    "# ================================================================\n",
    "# 1. COMPREHENSIVE TARGET DISTRIBUTION WITH BELL CURVES\n",
    "# ================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Creating Visualization 1: Target Distributions with Normal Curves\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fig = plt.figure(figsize=(20, 12))\n",
    "gs = fig.add_gridspec(3, 2, hspace=0.3, wspace=0.25)\n",
    "\n",
    "for idx, col in enumerate(cfg.TARGET_COLS):\n",
    "    row = idx // 2\n",
    "    col_idx = idx % 2\n",
    "    ax = fig.add_subplot(gs[row, col_idx])\n",
    "    \n",
    "    values = train_pivot[col].dropna().values\n",
    "    \n",
    "    # Histogram\n",
    "    n, bins, patches = ax.hist(values, bins=40, alpha=0.6, color=COLORS['primary'], \n",
    "                                edgecolor='white', linewidth=1.5, density=True,\n",
    "                                label='Observed Distribution')\n",
    "    \n",
    "    # Fit normal distribution\n",
    "    mu, sigma = values.mean(), values.std()\n",
    "    x = np.linspace(values.min(), values.max(), 100)\n",
    "    normal_curve = norm.pdf(x, mu, sigma)\n",
    "    \n",
    "    # Plot normal curve\n",
    "    ax.plot(x, normal_curve, color=COLORS['highlight'], linewidth=3, \n",
    "            label=f'Normal Fit (μ={mu:.1f}, σ={sigma:.1f})', linestyle='--')\n",
    "    \n",
    "    # Statistics lines\n",
    "    ax.axvline(mu, color=COLORS['secondary'], linestyle='-', linewidth=2.5, \n",
    "               label=f'Mean: {mu:.2f}g', alpha=0.8)\n",
    "    ax.axvline(np.median(values), color=COLORS['tertiary'], linestyle='-', linewidth=2.5,\n",
    "               label=f'Median: {np.median(values):.2f}g', alpha=0.8)\n",
    "    \n",
    "    # Annotations\n",
    "    ax.text(0.98, 0.97, f'n = {len(values):,}', transform=ax.transAxes,\n",
    "            verticalalignment='top', horizontalalignment='right',\n",
    "            bbox=dict(boxstyle='round', facecolor='white', alpha=0.9, edgecolor=COLORS['neutral']),\n",
    "            fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # Statistical properties\n",
    "    skewness = skew(values)\n",
    "    kurt = kurtosis(values)\n",
    "    stats_text = f'Skewness: {skewness:.2f}\\nKurtosis: {kurt:.2f}\\nMin: {values.min():.2f}g\\nMax: {values.max():.2f}g'\n",
    "    ax.text(0.02, 0.97, stats_text, transform=ax.transAxes,\n",
    "            verticalalignment='top', horizontalalignment='left',\n",
    "            bbox=dict(boxstyle='round', facecolor='white', alpha=0.9, edgecolor=COLORS['neutral']),\n",
    "            fontsize=9)\n",
    "    \n",
    "    # Labels and title\n",
    "    ax.set_xlabel(f'{cfg.TARGET_NAMES[col]} (grams)', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Probability Density', fontsize=12, fontweight='bold')\n",
    "    ax.set_title(f'{cfg.TARGET_NAMES[col]} Distribution', \n",
    "                 fontsize=14, fontweight='bold', pad=15, color=COLORS['text'])\n",
    "    ax.legend(loc='upper right', framealpha=0.95, edgecolor=COLORS['neutral'])\n",
    "    ax.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "# Add overall title\n",
    "fig.suptitle('Target Variable Distributions with Statistical Analysis', \n",
    "             fontsize=18, fontweight='bold', y=0.995, color=COLORS['text'])\n",
    "\n",
    "plt.savefig(cfg.OUTPUT_PATH / 'professional_1_distributions.png', dpi=300, bbox_inches='tight', \n",
    "            facecolor='white', edgecolor='none')\n",
    "print(\"✓ Saved: professional_1_distributions.png\")\n",
    "plt.close()\n",
    "\n",
    "# ================================================================\n",
    "# 2. COMPREHENSIVE CORRELATION MATRIX  \n",
    "# ================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Creating Visualization 2: Complete Correlation Matrix\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate correlations\n",
    "correlation_matrix = train_pivot[cfg.TARGET_COLS].corr()\n",
    "\n",
    "# Create simple figure\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "# Professional RED-WHITE-GREEN diverging colormap\n",
    "# Red (negative) → White/Yellow (zero) → Green (positive)\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "colors_cmap = [\n",
    "    '#D73027',  # Dark red (strong negative)\n",
    "    '#FC8D59',  # Medium red\n",
    "    '#FEE090',  # Light red/orange\n",
    "    '#FFFFBF',  # White/Yellow (zero)\n",
    "    '#E0F3DB',  # Very light green\n",
    "    '#A8DDB5',  # Light green\n",
    "    '#66C2A4',  # Medium light green\n",
    "    '#2CA25F',  # Medium green\n",
    "    '#006D2C'   # Dark green (strong positive)\n",
    "]\n",
    "n_bins = 100\n",
    "cmap_diverging = LinearSegmentedColormap.from_list('red_white_green', colors_cmap, N=n_bins)\n",
    "\n",
    "# Create heatmap with full range -1 to 1\n",
    "im = ax.imshow(correlation_matrix, cmap=cmap_diverging, aspect='auto', vmin=-1, vmax=1)\n",
    "\n",
    "# Set ticks\n",
    "ax.set_xticks(np.arange(len(cfg.TARGET_COLS)))\n",
    "ax.set_yticks(np.arange(len(cfg.TARGET_COLS)))\n",
    "ax.set_xticklabels([cfg.TARGET_NAMES[col] for col in cfg.TARGET_COLS], \n",
    "                    rotation=45, ha='right', fontsize=11, fontweight='bold')\n",
    "ax.set_yticklabels([cfg.TARGET_NAMES[col] for col in cfg.TARGET_COLS], \n",
    "                    fontsize=11, fontweight='bold')\n",
    "\n",
    "# Add correlation values and significance\n",
    "for i in range(len(cfg.TARGET_COLS)):\n",
    "    for j in range(len(cfg.TARGET_COLS)):\n",
    "        corr_val = correlation_matrix.iloc[i, j]\n",
    "        \n",
    "        # Calculate p-value\n",
    "        if i != j:\n",
    "            _, p_val = pearsonr(train_pivot[cfg.TARGET_COLS[i]], \n",
    "                               train_pivot[cfg.TARGET_COLS[j]])\n",
    "            sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else ''\n",
    "        else:\n",
    "            sig = ''\n",
    "        \n",
    "        # Choose text color based on correlation value\n",
    "        # Dark text for light colors (near zero), white text for dark colors (strong correlations)\n",
    "        if abs(corr_val) > 0.6:\n",
    "            text_color = 'white'\n",
    "        else:\n",
    "            text_color = 'black'\n",
    "        \n",
    "        # Main correlation value\n",
    "        ax.text(j, i, f'{corr_val:.3f}', ha='center', va='center',\n",
    "                color=text_color, fontsize=12, fontweight='bold')\n",
    "        \n",
    "        # Significance stars\n",
    "        if sig:\n",
    "            ax.text(j, i + 0.35, sig, ha='center', va='center',\n",
    "                    color=text_color, fontsize=10, fontweight='bold')\n",
    "\n",
    "# Colorbar with professional labels\n",
    "cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "cbar.set_label('Pearson Correlation Coefficient (r)', rotation=270, labelpad=25, \n",
    "               fontsize=12, fontweight='bold')\n",
    "cbar.ax.tick_params(labelsize=10)\n",
    "\n",
    "# Add interpretive labels on colorbar\n",
    "cbar.ax.text(3.5, -0.9, 'Strong Negative', transform=cbar.ax.transData,\n",
    "            fontsize=9, fontweight='bold', color='#D73027', rotation=0)\n",
    "cbar.ax.text(3.5, 0.0, 'No Correlation', transform=cbar.ax.transData,\n",
    "            fontsize=9, fontweight='bold', color='#666666', rotation=0)\n",
    "cbar.ax.text(3.5, 0.9, 'Strong Positive', transform=cbar.ax.transData,\n",
    "            fontsize=9, fontweight='bold', color='#006D2C', rotation=0)\n",
    "\n",
    "# Title with significance legend\n",
    "title_text = 'Complete Correlation Matrix: All Target Variables\\n'\n",
    "title_text += 'Red = Negative | Yellow = Zero | Green = Positive  |  '\n",
    "title_text += '*** p<0.001, ** p<0.01, * p<0.05'\n",
    "ax.set_title(title_text, fontsize=13, fontweight='bold', pad=20, color=COLORS['text'])\n",
    "\n",
    "# Grid\n",
    "ax.set_xticks(np.arange(len(cfg.TARGET_COLS)) - 0.5, minor=True)\n",
    "ax.set_yticks(np.arange(len(cfg.TARGET_COLS)) - 0.5, minor=True)\n",
    "ax.grid(which='minor', color='white', linestyle='-', linewidth=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(cfg.OUTPUT_PATH / 'professional_2_correlation_matrix.png', dpi=300, bbox_inches='tight',\n",
    "            facecolor='white', edgecolor='none')\n",
    "print(\"✓ Saved: professional_2_correlation_matrix.png\")\n",
    "plt.close()\n",
    "\n",
    "# ================================================================\n",
    "# 3. DETAILED BOX PLOTS WITH STATISTICS\n",
    "# ================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Creating Visualization 3: Detailed Box Plots\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "# Prepare data\n",
    "data_for_box = [train_pivot[col].dropna().values for col in cfg.TARGET_COLS]\n",
    "labels = [cfg.TARGET_NAMES[col] for col in cfg.TARGET_COLS]\n",
    "\n",
    "# Create box plot\n",
    "bp = ax.boxplot(data_for_box, labels=labels, patch_artist=True,\n",
    "                widths=0.6, showmeans=True,\n",
    "                meanprops=dict(marker='D', markerfacecolor=COLORS['highlight'], \n",
    "                              markeredgecolor='white', markersize=10),\n",
    "                medianprops=dict(color=COLORS['secondary'], linewidth=2.5),\n",
    "                boxprops=dict(facecolor=COLORS['primary'], alpha=0.7, \n",
    "                             edgecolor=COLORS['text'], linewidth=1.5),\n",
    "                whiskerprops=dict(color=COLORS['text'], linewidth=1.5),\n",
    "                capprops=dict(color=COLORS['text'], linewidth=1.5),\n",
    "                flierprops=dict(marker='o', markerfacecolor=COLORS['highlight'], \n",
    "                               markersize=5, alpha=0.5, markeredgecolor='none'))\n",
    "\n",
    "# Add detailed statistics\n",
    "for idx, (col, data) in enumerate(zip(cfg.TARGET_COLS, data_for_box)):\n",
    "    # Calculate statistics\n",
    "    q1 = np.percentile(data, 25)\n",
    "    median = np.median(data)\n",
    "    q3 = np.percentile(data, 75)\n",
    "    mean = np.mean(data)\n",
    "    iqr = q3 - q1\n",
    "    lower_whisker = q1 - 1.5 * iqr\n",
    "    upper_whisker = q3 + 1.5 * iqr\n",
    "    outliers = len(data[(data < lower_whisker) | (data > upper_whisker)])\n",
    "    \n",
    "    # Add text annotation\n",
    "    stats_text = f'Mean: {mean:.1f}g\\nMedian: {median:.1f}g\\nIQR: {iqr:.1f}g\\nOutliers: {outliers}'\n",
    "    ax.text(idx + 1, ax.get_ylim()[1] * 0.95, stats_text,\n",
    "            ha='center', va='top', fontsize=9,\n",
    "            bbox=dict(boxstyle='round', facecolor='white', alpha=0.9, \n",
    "                     edgecolor=COLORS['neutral']))\n",
    "\n",
    "# Labels and formatting\n",
    "ax.set_ylabel('Biomass (grams)', fontsize=13, fontweight='bold')\n",
    "ax.set_xlabel('Target Variables', fontsize=13, fontweight='bold')\n",
    "ax.set_title('Box Plot Analysis: Target Variables with Statistical Details', \n",
    "             fontsize=15, fontweight='bold', pad=20, color=COLORS['text'])\n",
    "\n",
    "# Legend\n",
    "legend_elements = [\n",
    "    plt.Line2D([0], [0], marker='D', color='w', label='Mean',\n",
    "              markerfacecolor=COLORS['highlight'], markersize=10),\n",
    "    plt.Line2D([0], [0], color=COLORS['secondary'], linewidth=2.5, label='Median'),\n",
    "    plt.Line2D([0], [0], marker='o', color='w', label='Outliers',\n",
    "              markerfacecolor=COLORS['highlight'], markersize=7, alpha=0.5)\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc='upper right', framealpha=0.95, \n",
    "         edgecolor=COLORS['neutral'], fontsize=11)\n",
    "\n",
    "ax.grid(True, alpha=0.3, axis='y', linestyle='--')\n",
    "ax.set_axisbelow(True)\n",
    "plt.xticks(rotation=15, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(cfg.OUTPUT_PATH / 'professional_3_boxplots.png', dpi=300, bbox_inches='tight',\n",
    "            facecolor='white', edgecolor='none')\n",
    "print(\"✓ Saved: professional_3_boxplots.png\")\n",
    "plt.close()\n",
    "\n",
    "# ================================================================\n",
    "# 4. SCATTER PLOT MATRIX WITH REGRESSION LINES\n",
    "# ================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Creating Visualization 4: Scatter Plot Matrix\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Select subset of targets for clarity\n",
    "main_targets = ['Dry_Green_g', 'Dry_Dead_g', 'GDM_g', 'Dry_Total_g']\n",
    "n_targets = len(main_targets)\n",
    "\n",
    "fig, axes = plt.subplots(n_targets, n_targets, figsize=(18, 18))\n",
    "\n",
    "for i, target1 in enumerate(main_targets):\n",
    "    for j, target2 in enumerate(main_targets):\n",
    "        ax = axes[i, j]\n",
    "        \n",
    "        if i == j:\n",
    "            # Diagonal: histogram with KDE\n",
    "            data = train_pivot[target1].dropna().values\n",
    "            ax.hist(data, bins=30, alpha=0.6, color=COLORS['primary'], \n",
    "                   edgecolor='white', density=True)\n",
    "            \n",
    "            # KDE\n",
    "            from scipy.stats import gaussian_kde\n",
    "            kde = gaussian_kde(data)\n",
    "            x_range = np.linspace(data.min(), data.max(), 100)\n",
    "            ax.plot(x_range, kde(x_range), color=COLORS['highlight'], \n",
    "                   linewidth=2.5, label='KDE')\n",
    "            \n",
    "            ax.set_ylabel('Density', fontsize=9)\n",
    "            if i == n_targets - 1:\n",
    "                ax.set_xlabel(cfg.TARGET_NAMES[target1], fontsize=10, fontweight='bold')\n",
    "        else:\n",
    "            # Off-diagonal: scatter plot\n",
    "            x_data = train_pivot[target2].dropna()\n",
    "            y_data = train_pivot[target1].dropna()\n",
    "            \n",
    "            # Align data\n",
    "            common_idx = x_data.index.intersection(y_data.index)\n",
    "            x_vals = x_data.loc[common_idx].values\n",
    "            y_vals = y_data.loc[common_idx].values\n",
    "            \n",
    "            # Scatter\n",
    "            ax.scatter(x_vals, y_vals, alpha=0.4, s=20, color=COLORS['primary'], \n",
    "                      edgecolors='none')\n",
    "            \n",
    "            # Regression line\n",
    "            if len(x_vals) > 1:\n",
    "                z = np.polyfit(x_vals, y_vals, 1)\n",
    "                p = np.poly1d(z)\n",
    "                x_line = np.linspace(x_vals.min(), x_vals.max(), 100)\n",
    "                ax.plot(x_line, p(x_line), color=COLORS['highlight'], \n",
    "                       linewidth=2.5, linestyle='--')\n",
    "                \n",
    "                # Correlation\n",
    "                r, p_val = pearsonr(x_vals, y_vals)\n",
    "                ax.text(0.05, 0.95, f'r={r:.3f}', transform=ax.transAxes,\n",
    "                       verticalalignment='top', fontsize=9, fontweight='bold',\n",
    "                       bbox=dict(boxstyle='round', facecolor='white', alpha=0.9))\n",
    "        \n",
    "        # Labels\n",
    "        if j == 0:\n",
    "            ax.set_ylabel(cfg.TARGET_NAMES[target1], fontsize=10, fontweight='bold')\n",
    "        if i == n_targets - 1:\n",
    "            ax.set_xlabel(cfg.TARGET_NAMES[target2], fontsize=10, fontweight='bold')\n",
    "        \n",
    "        ax.grid(True, alpha=0.2, linestyle='--')\n",
    "        ax.tick_params(labelsize=8)\n",
    "\n",
    "fig.suptitle('Scatter Plot Matrix: Pairwise Relationships Between Targets', \n",
    "             fontsize=16, fontweight='bold', y=0.995)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(cfg.OUTPUT_PATH / 'professional_4_scatter_matrix.png', dpi=300, bbox_inches='tight',\n",
    "            facecolor='white', edgecolor='none')\n",
    "print(\"✓ Saved: professional_4_scatter_matrix.png\")\n",
    "plt.close()\n",
    "\n",
    "# ================================================================\n",
    "# 5. DETAILED SAMPLE IMAGE ANALYSIS\n",
    "# ================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Creating Visualization 5: Detailed Sample Image Analysis\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Select 6 sample images\n",
    "sample_images = train_pivot['image_path'].sample(min(6, len(train_pivot)), random_state=42).tolist()\n",
    "\n",
    "fig = plt.figure(figsize=(20, 16))\n",
    "gs = fig.add_gridspec(6, 4, hspace=0.35, wspace=0.3)  # 6 rows, 4 columns\n",
    "\n",
    "for idx, img_path in enumerate(sample_images):\n",
    "    img_name = os.path.basename(img_path)\n",
    "    full_path = cfg.TRAIN_PATH / img_name\n",
    "    \n",
    "    img = cv2.imread(str(full_path))\n",
    "    if img is None:\n",
    "        continue\n",
    "    \n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img_hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    # Main image (left column)\n",
    "    ax_img = fig.add_subplot(gs[idx, 0])\n",
    "    ax_img.imshow(img_rgb)\n",
    "    ax_img.set_title(f'Sample {idx+1}', fontsize=12, fontweight='bold')\n",
    "    ax_img.axis('off')\n",
    "    \n",
    "    # Add image properties\n",
    "    h, w, _ = img_rgb.shape\n",
    "    green_ratio = img_rgb[:,:,1] / (img_rgb.sum(axis=2) + 1)\n",
    "    brightness = img_rgb.mean()\n",
    "    \n",
    "    props_text = f'Size: {w}×{h}\\nBrightness: {brightness:.1f}\\nGreen Ratio: {green_ratio.mean():.3f}'\n",
    "    ax_img.text(0.02, 0.98, props_text, transform=ax_img.transAxes,\n",
    "               verticalalignment='top', fontsize=9,\n",
    "               bbox=dict(boxstyle='round', facecolor='white', alpha=0.9))\n",
    "    \n",
    "    # RGB histogram\n",
    "    ax_hist = fig.add_subplot(gs[idx, 1])\n",
    "    colors_rgb = ['#E74C3C', '#27AE60', '#3498DB']  # Red, Green, Blue\n",
    "    labels_rgb = ['Red', 'Green', 'Blue']\n",
    "    \n",
    "    for c, color, label in zip(range(3), colors_rgb, labels_rgb):\n",
    "        hist = cv2.calcHist([img_rgb], [c], None, [256], [0, 256])\n",
    "        ax_hist.plot(hist, color=color, linewidth=2, label=label, alpha=0.7)\n",
    "    \n",
    "    ax_hist.set_xlabel('Pixel Value', fontsize=9)\n",
    "    ax_hist.set_ylabel('Frequency', fontsize=9)\n",
    "    ax_hist.set_title('RGB Distribution', fontsize=10, fontweight='bold')\n",
    "    ax_hist.legend(fontsize=8)\n",
    "    ax_hist.grid(True, alpha=0.3)\n",
    "    ax_hist.set_xlim([0, 256])\n",
    "    \n",
    "    # Green channel analysis\n",
    "    ax_green = fig.add_subplot(gs[idx, 2])\n",
    "    green_channel = img_rgb[:,:,1]\n",
    "    ax_green.imshow(green_channel, cmap='Greens')\n",
    "    ax_green.set_title('Green Channel', fontsize=10, fontweight='bold')\n",
    "    ax_green.axis('off')\n",
    "    \n",
    "    # Vegetation index\n",
    "    ax_veg = fig.add_subplot(gs[idx, 3])\n",
    "    ax_veg.imshow(green_ratio, cmap='RdYlGn', vmin=0, vmax=0.5)\n",
    "    ax_veg.set_title('Vegetation Index', fontsize=10, fontweight='bold')\n",
    "    ax_veg.axis('off')\n",
    "\n",
    "fig.suptitle('Detailed Sample Image Analysis: RGB, Green Channel, and Vegetation Index', \n",
    "             fontsize=16, fontweight='bold', y=0.995)\n",
    "\n",
    "plt.savefig(cfg.OUTPUT_PATH / 'professional_5_sample_images.png', dpi=300, bbox_inches='tight',\n",
    "            facecolor='white', edgecolor='none')\n",
    "print(\"✓ Saved: professional_5_sample_images.png\")\n",
    "plt.close()\n",
    "\n",
    "# ================================================================\n",
    "# 6. HIERARCHICAL RELATIONSHIP VALIDATION\n",
    "# ================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Creating Visualization 6: Hierarchical Relationships\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
    "\n",
    "# Green + Clover = GDM\n",
    "ax = axes[0]\n",
    "calculated = train_pivot['Dry_Green_g'] + train_pivot['Dry_Clover_g']\n",
    "actual = train_pivot['GDM_g']\n",
    "\n",
    "ax.scatter(calculated, actual, alpha=0.5, s=40, color=COLORS['primary'], \n",
    "          edgecolors='white', linewidth=0.5)\n",
    "\n",
    "# Perfect match line\n",
    "max_val = max(calculated.max(), actual.max())\n",
    "ax.plot([0, max_val], [0, max_val], color=COLORS['highlight'], \n",
    "       linewidth=3, linestyle='--', label='Perfect Match (y=x)')\n",
    "\n",
    "# Regression line\n",
    "z = np.polyfit(calculated, actual, 1)\n",
    "p = np.poly1d(z)\n",
    "x_line = np.linspace(calculated.min(), calculated.max(), 100)\n",
    "ax.plot(x_line, p(x_line), color=COLORS['secondary'], \n",
    "       linewidth=2.5, label=f'Regression: y={z[0]:.3f}x+{z[1]:.2f}')\n",
    "\n",
    "# Statistics\n",
    "r, p_val = pearsonr(calculated, actual)\n",
    "rmse = np.sqrt(((calculated - actual) ** 2).mean())\n",
    "\n",
    "stats_text = f'Pearson r: {r:.4f}\\np-value: {p_val:.2e}\\nRMSE: {rmse:.2f}g\\nn: {len(calculated):,}'\n",
    "ax.text(0.05, 0.95, stats_text, transform=ax.transAxes,\n",
    "       verticalalignment='top', fontsize=11, fontweight='bold',\n",
    "       bbox=dict(boxstyle='round', facecolor='white', alpha=0.95, \n",
    "                edgecolor=COLORS['neutral'], linewidth=2))\n",
    "\n",
    "ax.set_xlabel('Green + Clover (calculated, grams)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('GDM (actual, grams)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Hierarchical Validation: Green + Clover = GDM', \n",
    "            fontsize=13, fontweight='bold', pad=15)\n",
    "ax.legend(loc='lower right', fontsize=11, framealpha=0.95)\n",
    "ax.grid(True, alpha=0.3, linestyle='--')\n",
    "\n",
    "# GDM + Dead = Total\n",
    "ax = axes[1]\n",
    "calculated = train_pivot['GDM_g'] + train_pivot['Dry_Dead_g']\n",
    "actual = train_pivot['Dry_Total_g']\n",
    "\n",
    "ax.scatter(calculated, actual, alpha=0.5, s=40, color=COLORS['primary'], \n",
    "          edgecolors='white', linewidth=0.5)\n",
    "\n",
    "max_val = max(calculated.max(), actual.max())\n",
    "ax.plot([0, max_val], [0, max_val], color=COLORS['highlight'], \n",
    "       linewidth=3, linestyle='--', label='Perfect Match (y=x)')\n",
    "\n",
    "z = np.polyfit(calculated, actual, 1)\n",
    "p = np.poly1d(z)\n",
    "x_line = np.linspace(calculated.min(), calculated.max(), 100)\n",
    "ax.plot(x_line, p(x_line), color=COLORS['secondary'], \n",
    "       linewidth=2.5, label=f'Regression: y={z[0]:.3f}x+{z[1]:.2f}')\n",
    "\n",
    "r, p_val = pearsonr(calculated, actual)\n",
    "rmse = np.sqrt(((calculated - actual) ** 2).mean())\n",
    "\n",
    "stats_text = f'Pearson r: {r:.4f}\\np-value: {p_val:.2e}\\nRMSE: {rmse:.2f}g\\nn: {len(calculated):,}'\n",
    "ax.text(0.05, 0.95, stats_text, transform=ax.transAxes,\n",
    "       verticalalignment='top', fontsize=11, fontweight='bold',\n",
    "       bbox=dict(boxstyle='round', facecolor='white', alpha=0.95, \n",
    "                edgecolor=COLORS['neutral'], linewidth=2))\n",
    "\n",
    "ax.set_xlabel('GDM + Dead (calculated, grams)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Total Biomass (actual, grams)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Hierarchical Validation: GDM + Dead = Total', \n",
    "            fontsize=13, fontweight='bold', pad=15)\n",
    "ax.legend(loc='lower right', fontsize=11, framealpha=0.95)\n",
    "ax.grid(True, alpha=0.3, linestyle='--')\n",
    "\n",
    "fig.suptitle('Hierarchical Relationship Validation with Statistical Analysis', \n",
    "             fontsize=15, fontweight='bold', y=1.00)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(cfg.OUTPUT_PATH / 'professional_6_hierarchical.png', dpi=300, bbox_inches='tight',\n",
    "            facecolor='white', edgecolor='none')\n",
    "print(\"✓ Saved: professional_6_hierarchical.png\")\n",
    "plt.close()\n",
    "\n",
    "# ================================================================\n",
    "# SUMMARY\n",
    "# ================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EDA COMPLETE - ALL VISUALIZATIONS SAVED\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nGenerated Professional Visualizations:\")\n",
    "print(\"  1. professional_1_distributions.png - Target distributions with bell curves\")\n",
    "print(\"  2. professional_2_correlation_matrix.png - Complete correlation analysis\")\n",
    "print(\"  3. professional_3_boxplots.png - Detailed box plots with statistics\")\n",
    "print(\"  4. professional_4_scatter_matrix.png - Scatter plot matrix with regression\")\n",
    "print(\"  5. professional_5_sample_images.png - Detailed image analysis\")\n",
    "print(\"  6. professional_6_hierarchical.png - Hierarchical relationship validation\")\n",
    "print(\"\\nAll visualizations use consistent professional color scheme\")\n",
    "print(\"Statistical details and annotations included on all plots\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "904607be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T17:15:05.851804Z",
     "iopub.status.busy": "2026-01-11T17:15:05.851367Z",
     "iopub.status.idle": "2026-01-11T17:15:05.876248Z",
     "shell.execute_reply": "2026-01-11T17:15:05.875521Z"
    },
    "papermill": {
     "duration": 0.031386,
     "end_time": "2026-01-11T17:15:05.878044",
     "exception": false,
     "start_time": "2026-01-11T17:15:05.846658",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "╔══════════════════════════════════════════════════════════════════════════════╗\n",
      "║                    LOW SCORE DIAGNOSTIC TOOL                                  ║\n",
      "║                    Public Score: 0.65 vs Expected: 0.74+                      ║\n",
      "╚══════════════════════════════════════════════════════════════════════════════╝\n",
      "\n",
      "================================================================================\n",
      "1. CHECKING SUBMISSION FORMAT\n",
      "================================================================================\n",
      "❌ ERROR loading submission: [Errno 2] No such file or directory: 'submission.csv'\n",
      "\n",
      "================================================================================\n",
      "2. ANALYZING CV vs PUBLIC LB GAP\n",
      "================================================================================\n",
      "CV Score:     0.7550\n",
      "Public Score: 0.6500\n",
      "Gap:          0.1050 (13.9%)\n",
      "\n",
      "❌ CRITICAL: Very large gap (>10%) indicates:\n",
      "   1. Target leakage in CV (most likely)\n",
      "   2. Different data distribution\n",
      "   3. Overfitting to train set\n",
      "\n",
      "🔍 Most Likely Cause: EMBEDDING LEAKAGE\n",
      "   Are you using pre-computed embeddings that include test data?\n",
      "\n",
      "================================================================================\n",
      "3. COMMON ISSUES CHECKLIST\n",
      "================================================================================\n",
      "\n",
      "❓ Are you using pre-computed embeddings?\n",
      "   Path: /kaggle/input/csiro-datasplit/csiro_data_split.csv\n",
      "   ⚠ If these embeddings were computed WITH test data visible,\n",
      "      this causes MASSIVE target leakage!\n",
      "   ✓ Solution: Compute embeddings fresh, test data separately\n",
      "\n",
      "❓ Is hierarchical reconciliation working correctly?\n",
      "   Expected improvement: ~0.005-0.010\n",
      "   If you see negative improvement, there's a bug!\n",
      "\n",
      "❓ Is target order correct?\n",
      "   Expected: [Dry_Clover_g, Dry_Dead_g, Dry_Green_g, Dry_Total_g, GDM_g]\n",
      "   Common mistake: Wrong order after reconciliation\n",
      "\n",
      "❓ Are image paths correct?\n",
      "   Test images should be in: /kaggle/input/csiro-biomass/test/\n",
      "   Common mistake: Looking in wrong directory\n",
      "\n",
      "❓ Are you using the right model?\n",
      "   Some DINO models in your original code may not work\n",
      "   Recommended: Use only SigLIP + gradient boosting ensemble\n",
      "\n",
      "================================================================================\n",
      "4. RECOMMENDED FIXES FOR 0.65 SCORE\n",
      "================================================================================\n",
      "\n",
      "🔧 FIX #1: Recompute Embeddings WITHOUT Test Data Leakage\n",
      "------------------------------------------------------------\n",
      "\n",
      "# CRITICAL: Compute train and test embeddings SEPARATELY\n",
      "    \n",
      "# For TRAINING data only:\n",
      "train_df = pd.read_csv('train.csv')\n",
      "train_df = pivot_table(train_df)\n",
      "train_df['image_path'] = train_df['image_path'].apply(lambda p: f'/kaggle/input/csiro-biomass/{p}')\n",
      "\n",
      "# Compute embeddings ONLY on training data\n",
      "train_embeddings = compute_embeddings(\n",
      "    model_path=siglip_path,\n",
      "    df=train_df,\n",
      "    patch_size=520,\n",
      "    overlap=16\n",
      ")\n",
      "\n",
      "# For TEST data - compute SEPARATELY, never seen during training\n",
      "test_df = pd.read_csv('test.csv')\n",
      "test_df = pivot_table(test_df)\n",
      "test_df['image_path'] = test_df['image_path'].apply(lambda p: f'/kaggle/input/csiro-biomass/{p}')\n",
      "\n",
      "test_embeddings = compute_embeddings(\n",
      "    model_path=siglip_path,\n",
      "    df=test_df,\n",
      "    patch_size=520,\n",
      "    overlap=16\n",
      ")\n",
      "\n",
      "# ❌ DON'T DO THIS (causes leakage):\n",
      "# all_data = pd.concat([train_df, test_df])\n",
      "# all_embeddings = compute_embeddings(all_data)  # Test data sees training!\n",
      "\n",
      "\n",
      "🔧 FIX #2: Verify Reconciliation Order\n",
      "------------------------------------------------------------\n",
      "\n",
      "def hierarchical_reconciliation(predictions):\n",
      "    # Input order: [Dry_Clover_g, Dry_Dead_g, Dry_Green_g, Dry_Total_g, GDM_g]\n",
      "    # Reorder to: [Green, Clover, Dead, GDM, Total]\n",
      "    ordered_cols_idx = [2, 0, 1, 4, 3]\n",
      "    Y = predictions[:, ordered_cols_idx].T\n",
      "    \n",
      "    # Apply reconciliation...\n",
      "    Y_reconciled = ...\n",
      "    \n",
      "    # CRITICAL: Reorder back to ORIGINAL order\n",
      "    # [Green, Clover, Dead, GDM, Total] -> [Clover, Dead, Green, Total, GDM]\n",
      "    reverse_idx = [1, 2, 0, 4, 3]\n",
      "    result = Y_reconciled[:, reverse_idx]\n",
      "    \n",
      "    return result  # Should match input order!\n",
      "\n",
      "\n",
      "🔧 FIX #3: Simplify Pipeline (Remove Complex DINO Models)\n",
      "------------------------------------------------------------\n",
      "\n",
      "# Your original code has 3 DINO models that may have issues\n",
      "# The paper shows SigLIP + gradient boosting alone achieves 0.748\n",
      "\n",
      "# SIMPLIFIED, MORE RELIABLE PIPELINE:\n",
      "1. Use ONLY SigLIP embeddings (most stable)\n",
      "2. Use LightGBM + CatBoost ensemble\n",
      "3. Apply hierarchical reconciliation\n",
      "4. This should give 0.74-0.75 reliably\n",
      "\n",
      "# Skip the complex CrossPVT and MVP DINO models for now\n",
      "\n",
      "\n",
      "🔧 FIX #4: Check Test Image Paths\n",
      "------------------------------------------------------------\n",
      "\n",
      "# Verify test images load correctly\n",
      "test_df = pd.read_csv('/kaggle/input/csiro-biomass/test.csv')\n",
      "test_df['image_path'] = test_df['image_path'].apply(\n",
      "    lambda p: f'/kaggle/input/csiro-biomass/{p}'\n",
      ")\n",
      "\n",
      "# Test loading first image\n",
      "import cv2\n",
      "img = cv2.imread(test_df['image_path'].iloc[0])\n",
      "print(f\"Image shape: {img.shape if img is not None else 'FAILED TO LOAD'}\")\n",
      "\n",
      "# If None, paths are wrong!\n",
      "\n",
      "\n",
      "================================================================================\n",
      "5. CREATING MINIMAL WORKING SCRIPT\n",
      "================================================================================\n",
      "✓ Created: /tmp/minimal_working_script.py\n",
      "\n",
      "This minimal script:\n",
      "  1. NO embedding leakage (train/test separate)\n",
      "  2. Proper GroupKFold CV\n",
      "  3. Simple LGB + CAT ensemble\n",
      "  4. Hierarchical reconciliation\n",
      "  5. Should achieve 0.74-0.75 score\n",
      "\n",
      "================================================================================\n",
      "DIAGNOSIS COMPLETE\n",
      "================================================================================\n",
      "\n",
      "📋 SUMMARY:\n",
      "  ❌ MOST LIKELY ISSUE: Embedding leakage from pre-computed data\n",
      "  🔧 PRIMARY FIX: Recompute embeddings separately for train/test\n",
      "\n",
      "📝 NEXT STEPS:\n",
      "  1. Examine your embedding computation code\n",
      "  2. Verify train/test are NEVER mixed before prediction\n",
      "  3. Use the minimal_working_script.py as reference\n",
      "  4. Re-run with fixes and check new public score\n",
      "\n",
      "💡 EXPECTED RESULTS AFTER FIXES:\n",
      "  • CV Score: 0.74-0.76\n",
      "  • Public Score: 0.73-0.75\n",
      "  • Gap: <0.02\n",
      "\n",
      "================================================================================\n",
      "Run this script with your actual values:\n",
      "  python diagnose_and_fix.py\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Diagnostic Script for Low Public Score (0.65 vs Expected 0.74+)\n",
    "\n",
    "This script helps identify why your submission scored 0.65 instead of the expected 0.74+\n",
    "and provides fixes for common issues.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# ============================================================================\n",
    "# DIAGNOSTIC CHECKS\n",
    "# ============================================================================\n",
    "\n",
    "def check_submission_format(submission_path='submission.csv'):\n",
    "    \"\"\"Check if submission file has correct format\"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"1. CHECKING SUBMISSION FORMAT\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    try:\n",
    "        sub = pd.read_csv(submission_path)\n",
    "        print(f\"✓ Submission loaded: {len(sub)} rows\")\n",
    "        print(f\"✓ Columns: {sub.columns.tolist()}\")\n",
    "        \n",
    "        # Check required columns\n",
    "        if 'sample_id' not in sub.columns or 'target' not in sub.columns:\n",
    "            print(\"❌ ERROR: Missing required columns 'sample_id' and/or 'target'\")\n",
    "            return False\n",
    "            \n",
    "        # Check for NaN values\n",
    "        nan_count = sub['target'].isna().sum()\n",
    "        if nan_count > 0:\n",
    "            print(f\"⚠ WARNING: {nan_count} NaN values in predictions!\")\n",
    "            print(\"  Filling NaNs with 0...\")\n",
    "            sub['target'] = sub['target'].fillna(0)\n",
    "            sub.to_csv(submission_path.replace('.csv', '_fixed.csv'), index=False)\n",
    "        \n",
    "        # Check for negative values\n",
    "        neg_count = (sub['target'] < 0).sum()\n",
    "        if neg_count > 0:\n",
    "            print(f\"⚠ WARNING: {neg_count} negative predictions!\")\n",
    "            print(\"  Clipping to 0...\")\n",
    "            sub['target'] = sub['target'].clip(lower=0)\n",
    "            sub.to_csv(submission_path.replace('.csv', '_fixed.csv'), index=False)\n",
    "        \n",
    "        # Check for extreme values\n",
    "        print(f\"\\nPrediction Statistics:\")\n",
    "        print(sub['target'].describe())\n",
    "        \n",
    "        extreme_high = (sub['target'] > 200).sum()\n",
    "        if extreme_high > 0:\n",
    "            print(f\"⚠ WARNING: {extreme_high} predictions > 200g (very high!)\")\n",
    "        \n",
    "        extreme_low = (sub['target'] < 0.01).sum()\n",
    "        if extreme_low > len(sub) * 0.5:\n",
    "            print(f\"⚠ WARNING: {extreme_low} predictions near zero (too many!)\")\n",
    "            \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ ERROR loading submission: {e}\")\n",
    "        return False\n",
    "\n",
    "def check_cv_vs_lb_gap(cv_score=0.755, public_score=0.65):\n",
    "    \"\"\"Analyze the CV vs LB gap\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"2. ANALYZING CV vs PUBLIC LB GAP\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    gap = cv_score - public_score\n",
    "    print(f\"CV Score:     {cv_score:.4f}\")\n",
    "    print(f\"Public Score: {public_score:.4f}\")\n",
    "    print(f\"Gap:          {gap:.4f} ({gap/cv_score*100:.1f}%)\")\n",
    "    \n",
    "    if gap > 0.10:\n",
    "        print(\"\\n❌ CRITICAL: Very large gap (>10%) indicates:\")\n",
    "        print(\"   1. Target leakage in CV (most likely)\")\n",
    "        print(\"   2. Different data distribution\")\n",
    "        print(\"   3. Overfitting to train set\")\n",
    "        print(\"\\n🔍 Most Likely Cause: EMBEDDING LEAKAGE\")\n",
    "        print(\"   Are you using pre-computed embeddings that include test data?\")\n",
    "        return \"LEAKAGE\"\n",
    "    elif gap > 0.05:\n",
    "        print(\"\\n⚠ WARNING: Moderate gap (5-10%) suggests:\")\n",
    "        print(\"   1. Some overfitting\")\n",
    "        print(\"   2. CV split not representative\")\n",
    "        return \"OVERFIT\"\n",
    "    else:\n",
    "        print(\"\\n✓ Gap is acceptable (<5%)\")\n",
    "        return \"OK\"\n",
    "\n",
    "def check_common_issues():\n",
    "    \"\"\"Check for common issues that cause low scores\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"3. COMMON ISSUES CHECKLIST\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    issues = []\n",
    "    \n",
    "    # Issue 1: Pre-computed embeddings include test data\n",
    "    print(\"\\n❓ Are you using pre-computed embeddings?\")\n",
    "    print(\"   Path: /kaggle/input/csiro-datasplit/csiro_data_split.csv\")\n",
    "    print(\"   ⚠ If these embeddings were computed WITH test data visible,\")\n",
    "    print(\"      this causes MASSIVE target leakage!\")\n",
    "    print(\"   ✓ Solution: Compute embeddings fresh, test data separately\")\n",
    "    issues.append(\"CHECK_EMBEDDINGS\")\n",
    "    \n",
    "    # Issue 2: Reconciliation bugs\n",
    "    print(\"\\n❓ Is hierarchical reconciliation working correctly?\")\n",
    "    print(\"   Expected improvement: ~0.005-0.010\")\n",
    "    print(\"   If you see negative improvement, there's a bug!\")\n",
    "    issues.append(\"CHECK_RECONCILIATION\")\n",
    "    \n",
    "    # Issue 3: Wrong target order\n",
    "    print(\"\\n❓ Is target order correct?\")\n",
    "    print(\"   Expected: [Dry_Clover_g, Dry_Dead_g, Dry_Green_g, Dry_Total_g, GDM_g]\")\n",
    "    print(\"   Common mistake: Wrong order after reconciliation\")\n",
    "    issues.append(\"CHECK_TARGET_ORDER\")\n",
    "    \n",
    "    # Issue 4: Wrong image paths\n",
    "    print(\"\\n❓ Are image paths correct?\")\n",
    "    print(\"   Test images should be in: /kaggle/input/csiro-biomass/test/\")\n",
    "    print(\"   Common mistake: Looking in wrong directory\")\n",
    "    issues.append(\"CHECK_PATHS\")\n",
    "    \n",
    "    # Issue 5: Model checkpoints issue\n",
    "    print(\"\\n❓ Are you using the right model?\")\n",
    "    print(\"   Some DINO models in your original code may not work\")\n",
    "    print(\"   Recommended: Use only SigLIP + gradient boosting ensemble\")\n",
    "    issues.append(\"CHECK_MODEL\")\n",
    "    \n",
    "    return issues\n",
    "\n",
    "def suggest_fixes():\n",
    "    \"\"\"Suggest specific fixes based on the score\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"4. RECOMMENDED FIXES FOR 0.65 SCORE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\n🔧 FIX #1: Recompute Embeddings WITHOUT Test Data Leakage\")\n",
    "    print(\"-\" * 60)\n",
    "    print(\"\"\"\n",
    "# CRITICAL: Compute train and test embeddings SEPARATELY\n",
    "    \n",
    "# For TRAINING data only:\n",
    "train_df = pd.read_csv('train.csv')\n",
    "train_df = pivot_table(train_df)\n",
    "train_df['image_path'] = train_df['image_path'].apply(lambda p: f'/kaggle/input/csiro-biomass/{p}')\n",
    "\n",
    "# Compute embeddings ONLY on training data\n",
    "train_embeddings = compute_embeddings(\n",
    "    model_path=siglip_path,\n",
    "    df=train_df,\n",
    "    patch_size=520,\n",
    "    overlap=16\n",
    ")\n",
    "\n",
    "# For TEST data - compute SEPARATELY, never seen during training\n",
    "test_df = pd.read_csv('test.csv')\n",
    "test_df = pivot_table(test_df)\n",
    "test_df['image_path'] = test_df['image_path'].apply(lambda p: f'/kaggle/input/csiro-biomass/{p}')\n",
    "\n",
    "test_embeddings = compute_embeddings(\n",
    "    model_path=siglip_path,\n",
    "    df=test_df,\n",
    "    patch_size=520,\n",
    "    overlap=16\n",
    ")\n",
    "\n",
    "# ❌ DON'T DO THIS (causes leakage):\n",
    "# all_data = pd.concat([train_df, test_df])\n",
    "# all_embeddings = compute_embeddings(all_data)  # Test data sees training!\n",
    "\"\"\")\n",
    "    \n",
    "    print(\"\\n🔧 FIX #2: Verify Reconciliation Order\")\n",
    "    print(\"-\" * 60)\n",
    "    print(\"\"\"\n",
    "def hierarchical_reconciliation(predictions):\n",
    "    # Input order: [Dry_Clover_g, Dry_Dead_g, Dry_Green_g, Dry_Total_g, GDM_g]\n",
    "    # Reorder to: [Green, Clover, Dead, GDM, Total]\n",
    "    ordered_cols_idx = [2, 0, 1, 4, 3]\n",
    "    Y = predictions[:, ordered_cols_idx].T\n",
    "    \n",
    "    # Apply reconciliation...\n",
    "    Y_reconciled = ...\n",
    "    \n",
    "    # CRITICAL: Reorder back to ORIGINAL order\n",
    "    # [Green, Clover, Dead, GDM, Total] -> [Clover, Dead, Green, Total, GDM]\n",
    "    reverse_idx = [1, 2, 0, 4, 3]\n",
    "    result = Y_reconciled[:, reverse_idx]\n",
    "    \n",
    "    return result  # Should match input order!\n",
    "\"\"\")\n",
    "    \n",
    "    print(\"\\n🔧 FIX #3: Simplify Pipeline (Remove Complex DINO Models)\")\n",
    "    print(\"-\" * 60)\n",
    "    print(\"\"\"\n",
    "# Your original code has 3 DINO models that may have issues\n",
    "# The paper shows SigLIP + gradient boosting alone achieves 0.748\n",
    "\n",
    "# SIMPLIFIED, MORE RELIABLE PIPELINE:\n",
    "1. Use ONLY SigLIP embeddings (most stable)\n",
    "2. Use LightGBM + CatBoost ensemble\n",
    "3. Apply hierarchical reconciliation\n",
    "4. This should give 0.74-0.75 reliably\n",
    "\n",
    "# Skip the complex CrossPVT and MVP DINO models for now\n",
    "\"\"\")\n",
    "    \n",
    "    print(\"\\n🔧 FIX #4: Check Test Image Paths\")\n",
    "    print(\"-\" * 60)\n",
    "    print(\"\"\"\n",
    "# Verify test images load correctly\n",
    "test_df = pd.read_csv('/kaggle/input/csiro-biomass/test.csv')\n",
    "test_df['image_path'] = test_df['image_path'].apply(\n",
    "    lambda p: f'/kaggle/input/csiro-biomass/{p}'\n",
    ")\n",
    "\n",
    "# Test loading first image\n",
    "import cv2\n",
    "img = cv2.imread(test_df['image_path'].iloc[0])\n",
    "print(f\"Image shape: {img.shape if img is not None else 'FAILED TO LOAD'}\")\n",
    "\n",
    "# If None, paths are wrong!\n",
    "\"\"\")\n",
    "\n",
    "def create_minimal_working_example():\n",
    "    \"\"\"Create a minimal script that should work\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"5. CREATING MINIMAL WORKING SCRIPT\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    script = \"\"\"\n",
    "# ============================================================================\n",
    "# MINIMAL WORKING SCRIPT - Should achieve 0.74-0.75\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from copy import deepcopy\n",
    "\n",
    "# Load data\n",
    "train_df = pd.read_csv('/kaggle/input/csiro-biomass/train.csv')\n",
    "test_df = pd.read_csv('/kaggle/input/csiro-biomass/test.csv')\n",
    "\n",
    "# Pivot to wide format\n",
    "train_wide = pivot_table(train_df)\n",
    "test_wide = pivot_table(test_df)\n",
    "\n",
    "# Add image paths\n",
    "train_wide['image_path'] = train_wide['image_path'].apply(lambda p: f'/kaggle/input/csiro-biomass/{p}')\n",
    "test_wide['image_path'] = test_wide['image_path'].apply(lambda p: f'/kaggle/input/csiro-biomass/{p}')\n",
    "\n",
    "# Compute embeddings (NO LEAKAGE - separate for train and test)\n",
    "print(\"Computing train embeddings...\")\n",
    "train_emb = compute_embeddings(siglip_path, train_wide, patch_size=520, overlap=16)\n",
    "\n",
    "print(\"Computing test embeddings...\")\n",
    "test_emb = compute_embeddings(siglip_path, test_wide, patch_size=520, overlap=16)\n",
    "\n",
    "# Extract features\n",
    "emb_cols = [c for c in train_emb.columns if c.startswith('emb')]\n",
    "X_train = train_emb[emb_cols].values\n",
    "X_test = test_emb[emb_cols].values\n",
    "y_train = train_emb[TARGET_NAMES].values\n",
    "\n",
    "# 5-Fold GroupKFold CV\n",
    "groups = train_emb['image_path'].map({img: i for i, img in enumerate(train_emb['image_path'].unique())}).values\n",
    "kfold = GroupKFold(n_splits=5)\n",
    "\n",
    "preds_test = np.zeros((len(test_emb), 5))\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(X_train, groups=groups)):\n",
    "    print(f\"\\\\nFold {fold+1}/5\")\n",
    "    \n",
    "    X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "    y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "    \n",
    "    fold_preds = np.zeros((len(X_test), 5))\n",
    "    \n",
    "    for i, target in enumerate(TARGET_NAMES):\n",
    "        # Train LightGBM\n",
    "        lgb_model = LGBMRegressor(\n",
    "            learning_rate=0.01, n_estimators=500, num_leaves=31,\n",
    "            max_depth=6, random_state=42, verbose=-1\n",
    "        )\n",
    "        lgb_model.fit(\n",
    "            X_tr, y_tr[:, i],\n",
    "            eval_set=[(X_val, y_val[:, i])],\n",
    "            callbacks=[lgb.early_stopping(50, verbose=False)]\n",
    "        )\n",
    "        \n",
    "        # Train CatBoost  \n",
    "        cat_model = CatBoostRegressor(\n",
    "            learning_rate=0.01, iterations=500, depth=6,\n",
    "            random_state=42, verbose=0\n",
    "        )\n",
    "        cat_model.fit(X_tr, y_tr[:, i], eval_set=(X_val, y_val[:, i]))\n",
    "        \n",
    "        # Ensemble predictions\n",
    "        fold_preds[:, i] = 0.6 * lgb_model.predict(X_test) + 0.4 * cat_model.predict(X_test)\n",
    "    \n",
    "    preds_test += fold_preds / 5\n",
    "\n",
    "# Apply reconciliation\n",
    "preds_reconciled = hierarchical_reconciliation(preds_test)\n",
    "\n",
    "# Create submission\n",
    "test_wide[TARGET_NAMES] = preds_reconciled\n",
    "sub = melt_table(test_wide)\n",
    "sub[['sample_id', 'target']].to_csv('submission_fixed.csv', index=False)\n",
    "\n",
    "print(\"\\\\n✓ Created submission_fixed.csv\")\n",
    "print(f\"Predictions range: {preds_reconciled.min():.2f} to {preds_reconciled.max():.2f}\")\n",
    "\"\"\"\n",
    "    \n",
    "    with open('/tmp/minimal_working_script.py', 'w') as f:\n",
    "        f.write(script)\n",
    "    \n",
    "    print(\"✓ Created: /tmp/minimal_working_script.py\")\n",
    "    print(\"\\nThis minimal script:\")\n",
    "    print(\"  1. NO embedding leakage (train/test separate)\")\n",
    "    print(\"  2. Proper GroupKFold CV\")\n",
    "    print(\"  3. Simple LGB + CAT ensemble\")\n",
    "    print(\"  4. Hierarchical reconciliation\")\n",
    "    print(\"  5. Should achieve 0.74-0.75 score\")\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN DIAGNOSTIC RUNNER\n",
    "# ============================================================================\n",
    "\n",
    "def run_full_diagnosis(submission_path='submission.csv', cv_score=0.755, public_score=0.65):\n",
    "    \"\"\"Run complete diagnostic analysis\"\"\"\n",
    "    print(\"\"\"\n",
    "╔══════════════════════════════════════════════════════════════════════════════╗\n",
    "║                    LOW SCORE DIAGNOSTIC TOOL                                  ║\n",
    "║                    Public Score: 0.65 vs Expected: 0.74+                      ║\n",
    "╚══════════════════════════════════════════════════════════════════════════════╝\n",
    "\"\"\")\n",
    "    \n",
    "    # Check 1: Submission format\n",
    "    format_ok = check_submission_format(submission_path)\n",
    "    \n",
    "    # Check 2: CV vs LB gap analysis\n",
    "    gap_type = check_cv_vs_lb_gap(cv_score, public_score)\n",
    "    \n",
    "    # Check 3: Common issues\n",
    "    issues = check_common_issues()\n",
    "    \n",
    "    # Check 4: Suggest fixes\n",
    "    suggest_fixes()\n",
    "    \n",
    "    # Check 5: Create minimal example\n",
    "    create_minimal_working_example()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DIAGNOSIS COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\n📋 SUMMARY:\")\n",
    "    if gap_type == \"LEAKAGE\":\n",
    "        print(\"  ❌ MOST LIKELY ISSUE: Embedding leakage from pre-computed data\")\n",
    "        print(\"  🔧 PRIMARY FIX: Recompute embeddings separately for train/test\")\n",
    "    elif gap_type == \"OVERFIT\":\n",
    "        print(\"  ⚠ LIKELY ISSUE: Overfitting or CV not representative\")\n",
    "        print(\"  🔧 PRIMARY FIX: Check GroupKFold implementation\")\n",
    "    \n",
    "    print(\"\\n📝 NEXT STEPS:\")\n",
    "    print(\"  1. Examine your embedding computation code\")\n",
    "    print(\"  2. Verify train/test are NEVER mixed before prediction\")\n",
    "    print(\"  3. Use the minimal_working_script.py as reference\")\n",
    "    print(\"  4. Re-run with fixes and check new public score\")\n",
    "    \n",
    "    print(\"\\n💡 EXPECTED RESULTS AFTER FIXES:\")\n",
    "    print(\"  • CV Score: 0.74-0.76\")\n",
    "    print(\"  • Public Score: 0.73-0.75\")\n",
    "    print(\"  • Gap: <0.02\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_full_diagnosis()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Run this script with your actual values:\")\n",
    "    print(\"  python diagnose_and_fix.py\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aea20bd6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T17:15:05.887592Z",
     "iopub.status.busy": "2026-01-11T17:15:05.887362Z",
     "iopub.status.idle": "2026-01-11T17:30:39.178409Z",
     "shell.execute_reply": "2026-01-11T17:30:39.177510Z"
    },
    "papermill": {
     "duration": 933.300203,
     "end_time": "2026-01-11T17:30:39.182528",
     "exception": false,
     "start_time": "2026-01-11T17:15:05.882325",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-11 17:15:25.248362: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1768151725.408756      24 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1768151725.457596      24 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1768151725.850793      24 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768151725.850838      24 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768151725.850841      24 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768151725.850843      24 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CORRECTED PIPELINE - NO EMBEDDING LEAKAGE\n",
      "================================================================================\n",
      "\n",
      "1. Loading data...\n",
      "✓ Train: 357 samples\n",
      "✓ Test:  1 samples\n",
      "\n",
      "2. Computing embeddings (NO LEAKAGE - separate train/test)...\n",
      "\n",
      "2a. Computing TRAINING embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88bf05a2629743f8a83cea53351a99f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing embeddings:   0%|          | 0/357 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2b. Computing TEST embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98308b6137544a8da6d204815afb6c68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing embeddings:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Embeddings computed separately (no leakage!)\n",
      "\n",
      "3. Preparing features...\n",
      "✓ Features: 1152 dimensions\n",
      "✓ Targets:  5 outputs\n",
      "\n",
      "4. Training with 5-fold GroupKFold CV...\n",
      "\n",
      "============================================================\n",
      "Fold 1/5\n",
      "============================================================\n",
      "  Training Dry_Clover_g... ✓\n",
      "  Training Dry_Dead_g... ✓\n",
      "  Training Dry_Green_g... ✓\n",
      "  Training Dry_Total_g... ✓\n",
      "  Training GDM_g... ✓\n",
      "\n",
      "Fold 1 Score: 0.854164\n",
      "\n",
      "============================================================\n",
      "Fold 2/5\n",
      "============================================================\n",
      "  Training Dry_Clover_g... ✓\n",
      "  Training Dry_Dead_g... ✓\n",
      "  Training Dry_Green_g... ✓\n",
      "  Training Dry_Total_g... ✓\n",
      "  Training GDM_g... ✓\n",
      "\n",
      "Fold 2 Score: 0.773343\n",
      "\n",
      "============================================================\n",
      "Fold 3/5\n",
      "============================================================\n",
      "  Training Dry_Clover_g... ✓\n",
      "  Training Dry_Dead_g... ✓\n",
      "  Training Dry_Green_g... ✓\n",
      "  Training Dry_Total_g... ✓\n",
      "  Training GDM_g... ✓\n",
      "\n",
      "Fold 3 Score: 0.697714\n",
      "\n",
      "============================================================\n",
      "Fold 4/5\n",
      "============================================================\n",
      "  Training Dry_Clover_g... ✓\n",
      "  Training Dry_Dead_g... ✓\n",
      "  Training Dry_Green_g... ✓\n",
      "  Training Dry_Total_g... ✓\n",
      "  Training GDM_g... ✓\n",
      "\n",
      "Fold 4 Score: 0.769228\n",
      "\n",
      "============================================================\n",
      "Fold 5/5\n",
      "============================================================\n",
      "  Training Dry_Clover_g... ✓\n",
      "  Training Dry_Dead_g... ✓\n",
      "  Training Dry_Green_g... ✓\n",
      "  Training Dry_Total_g... ✓\n",
      "  Training GDM_g... ✓\n",
      "\n",
      "Fold 5 Score: 0.780503\n",
      "\n",
      "============================================================\n",
      "OOF CV Score: 0.776224\n",
      "============================================================\n",
      "\n",
      "5. Applying hierarchical reconciliation...\n",
      "Before reconciliation:\n",
      "  Mean predictions: [ 1.81398807 28.24848458 33.43602018 65.12956762 33.87203265]\n",
      "After reconciliation:\n",
      "  Mean predictions: [ 1.67337852 29.20463153 33.29541063 64.17342067 34.96878914]\n",
      "\n",
      "6. Creating submission...\n",
      "\n",
      "✓ Submission shape: (5, 2)\n",
      "✓ NaN count: 0\n",
      "✓ Negative count: 0\n",
      "\n",
      "Prediction statistics:\n",
      "count     5.000000\n",
      "mean     32.663126\n",
      "std      22.197636\n",
      "min       1.673379\n",
      "25%      29.204632\n",
      "50%      33.295411\n",
      "75%      34.968789\n",
      "max      64.173421\n",
      "Name: target, dtype: float64\n",
      "\n",
      "✓ Saved: submission.csv\n",
      "\n",
      "================================================================================\n",
      "PIPELINE COMPLETE\n",
      "================================================================================\n",
      "\n",
      "Expected public LB score: 0.73-0.75\n",
      "(Based on OOF CV: 0.7762)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CORRECTED PIPELINE - Fixes for 0.65 Score Issue\n",
    "\n",
    "Key fixes:\n",
    "1. NO EMBEDDING LEAKAGE - Train and test processed completely separately\n",
    "2. Proper target order in reconciliation\n",
    "3. Simplified to most reliable components only\n",
    "4. Should achieve 0.73-0.75 public LB score\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "class Config:\n",
    "    DATA_PATH = Path(\"/kaggle/input/csiro-biomass/\")\n",
    "    SIGLIP_PATH = \"/kaggle/input/google-siglip-so400m-patch14-384/transformers/default/1\"\n",
    "    \n",
    "    patch_size = 520\n",
    "    patch_overlap = 16\n",
    "    n_folds = 5\n",
    "    seed = 42\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "cfg = Config()\n",
    "\n",
    "# Set seeds\n",
    "np.random.seed(cfg.seed)\n",
    "torch.manual_seed(cfg.seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(cfg.seed)\n",
    "\n",
    "TARGET_NAMES = ['Dry_Clover_g', 'Dry_Dead_g', 'Dry_Green_g', 'Dry_Total_g', 'GDM_g']\n",
    "\n",
    "weights = {\n",
    "    'Dry_Green_g': 0.1,\n",
    "    'Dry_Dead_g': 0.1,\n",
    "    'Dry_Clover_g': 0.1,\n",
    "    'GDM_g': 0.2,\n",
    "    'Dry_Total_g': 0.5,\n",
    "}\n",
    "\n",
    "TARGET_MAX = {\n",
    "    \"Dry_Clover_g\": 71.7865,\n",
    "    \"Dry_Dead_g\": 83.8407,\n",
    "    \"Dry_Green_g\": 157.9836,\n",
    "    \"Dry_Total_g\": 185.70,\n",
    "    \"GDM_g\": 157.9836,\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# UTILITY FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def competition_metric(y_true, y_pred):\n",
    "    \"\"\"Weighted R² metric\"\"\"\n",
    "    y_weighted = sum(y_true[:, l].mean() * weights[label] for l, label in enumerate(TARGET_NAMES))\n",
    "    ss_res = sum(((y_true[:, l] - y_pred[:, l])**2).mean() * weights[label] for l, label in enumerate(TARGET_NAMES))\n",
    "    ss_tot = sum(((y_true[:, l] - y_weighted)**2).mean() * weights[label] for l, label in enumerate(TARGET_NAMES))\n",
    "    return 1 - ss_res / ss_tot\n",
    "\n",
    "def pivot_table(df):\n",
    "    \"\"\"Convert long to wide format\"\"\"\n",
    "    if 'target' in df.columns:\n",
    "        df_pt = pd.pivot_table(\n",
    "            df, values='target',\n",
    "            index=['image_path', 'Sampling_Date', 'State', 'Species', 'Pre_GSHH_NDVI', 'Height_Ave_cm'],\n",
    "            columns='target_name', aggfunc='mean'\n",
    "        ).reset_index()\n",
    "    else:\n",
    "        df['target'] = 0\n",
    "        df_pt = pd.pivot_table(\n",
    "            df, values='target',\n",
    "            index='image_path', columns='target_name', aggfunc='mean'\n",
    "        ).reset_index()\n",
    "    return df_pt\n",
    "\n",
    "def melt_table(df):\n",
    "    \"\"\"Convert wide to long format for submission\"\"\"\n",
    "    melted = df.melt(\n",
    "        id_vars='image_path',\n",
    "        value_vars=TARGET_NAMES,\n",
    "        var_name='target_name',\n",
    "        value_name='target'\n",
    "    )\n",
    "    melted['sample_id'] = (\n",
    "        melted['image_path']\n",
    "        .str.replace(r'^.*/', '', regex=True)\n",
    "        .str.replace('.jpg', '', regex=False)\n",
    "        + '__' + melted['target_name']\n",
    "    )\n",
    "    return melted[['sample_id', 'image_path', 'target_name', 'target']]\n",
    "\n",
    "def hierarchical_reconciliation(predictions):\n",
    "    \"\"\"\n",
    "    FIXED: Proper order handling for reconciliation\n",
    "    \n",
    "    Input/Output order: [Dry_Clover_g, Dry_Dead_g, Dry_Green_g, Dry_Total_g, GDM_g]\n",
    "    \"\"\"\n",
    "    # Reorder to: [Green, Clover, Dead, GDM, Total] for constraint matrix\n",
    "    # From [Clover, Dead, Green, Total, GDM] to [Green, Clover, Dead, GDM, Total]\n",
    "    ordered_idx = [2, 0, 1, 4, 3]\n",
    "    Y = predictions[:, ordered_idx].T\n",
    "    \n",
    "    # Constraint matrix\n",
    "    C = np.array([\n",
    "        [1, 1, 0, -1,  0],  # Green + Clover - GDM = 0\n",
    "        [0, 0, 1,  1, -1]   # Dead + GDM - Total = 0\n",
    "    ])\n",
    "    \n",
    "    # Project\n",
    "    C_T = C.T\n",
    "    P = np.eye(5) - C_T @ np.linalg.inv(C @ C_T) @ C\n",
    "    Y_rec = (P @ Y).T\n",
    "    Y_rec = np.maximum(Y_rec, 0)\n",
    "    \n",
    "    # Reorder back to original: [Clover, Dead, Green, Total, GDM]\n",
    "    reverse_idx = [1, 2, 0, 4, 3]\n",
    "    return Y_rec[:, reverse_idx]\n",
    "\n",
    "def extract_overlapping_patches(image, patch_size=520, overlap=16):\n",
    "    \"\"\"Extract patches with overlap\"\"\"\n",
    "    h, w, c = image.shape\n",
    "    stride = patch_size - overlap\n",
    "    patches = []\n",
    "    \n",
    "    for y in range(0, h, stride):\n",
    "        for x in range(0, w, stride):\n",
    "            y2 = min(y + patch_size, h)\n",
    "            x2 = min(x + patch_size, w)\n",
    "            patch = image[y:y2, x:x2]\n",
    "            \n",
    "            if patch.shape[0] < patch_size or patch.shape[1] < patch_size:\n",
    "                pad_h = patch_size - patch.shape[0]\n",
    "                pad_w = patch.shape[1] - patch.shape[1]\n",
    "                patch = np.pad(patch, ((0, pad_h), (0, pad_w), (0, 0)), mode='reflect')\n",
    "            \n",
    "            patches.append(patch)\n",
    "    \n",
    "    return patches\n",
    "\n",
    "def compute_embeddings(model_path, df, patch_size=520, overlap=16):\n",
    "    \"\"\"\n",
    "    CRITICAL: This must be called SEPARATELY for train and test\n",
    "    NEVER concatenate train and test before computing embeddings!\n",
    "    \"\"\"\n",
    "    device = cfg.device\n",
    "    model = AutoModel.from_pretrained(model_path, local_files_only=True).eval().to(device)\n",
    "    processor = AutoImageProcessor.from_pretrained(model_path)\n",
    "    \n",
    "    embeddings_list = []\n",
    "    image_paths = []\n",
    "    \n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Computing embeddings\"):\n",
    "        img_path = row['image_path']\n",
    "        img = cv2.imread(img_path)\n",
    "        \n",
    "        if img is None:\n",
    "            print(f\"Warning: Could not load {img_path}\")\n",
    "            continue\n",
    "        \n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        patches = extract_overlapping_patches(img, patch_size, overlap)\n",
    "        \n",
    "        # Convert to PIL\n",
    "        pil_patches = [Image.fromarray(p) for p in patches]\n",
    "        \n",
    "        # Process\n",
    "        inputs = processor(images=pil_patches, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            features = model.get_image_features(**inputs)\n",
    "            emb = features.mean(dim=0).cpu().numpy()\n",
    "        \n",
    "        embeddings_list.append(emb)\n",
    "        image_paths.append(img_path)\n",
    "    \n",
    "    # Create embedding dataframe\n",
    "    embeddings = np.stack(embeddings_list)\n",
    "    emb_cols = [f\"emb{i+1}\" for i in range(embeddings.shape[1])]\n",
    "    emb_df = pd.DataFrame(embeddings, columns=emb_cols)\n",
    "    emb_df['image_path'] = image_paths\n",
    "    \n",
    "    # Merge\n",
    "    result = df.merge(emb_df, on='image_path', how='left')\n",
    "    \n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return result\n",
    "\n",
    "class FeatureEngine(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Feature engineering pipeline\"\"\"\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.pca = PCA(n_components=0.95, random_state=42)\n",
    "        self.pls = PLSRegression(n_components=8, scale=False)\n",
    "        self.gmm = GaussianMixture(n_components=5, random_state=42)\n",
    "        self.fitted = False\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        self.pca.fit(X_scaled)\n",
    "        self.gmm.fit(X_scaled)\n",
    "        if y is not None:\n",
    "            self.pls.fit(X_scaled, y)\n",
    "            self.fitted = True\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        features = [self.pca.transform(X_scaled)]\n",
    "        if self.fitted:\n",
    "            features.append(self.pls.transform(X_scaled))\n",
    "        features.append(self.gmm.predict_proba(X_scaled))\n",
    "        return np.hstack(features)\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN PIPELINE\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Fixed pipeline with NO EMBEDDING LEAKAGE\n",
    "    Should achieve 0.73-0.75 public LB\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"CORRECTED PIPELINE - NO EMBEDDING LEAKAGE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 1: Load and format data\n",
    "    # ========================================================================\n",
    "    print(\"\\n1. Loading data...\")\n",
    "    train_df = pd.read_csv(cfg.DATA_PATH / 'train.csv')\n",
    "    test_df = pd.read_csv(cfg.DATA_PATH / 'test.csv')\n",
    "    \n",
    "    train_wide = pivot_table(train_df)\n",
    "    test_wide = pivot_table(test_df)\n",
    "    \n",
    "    # Add full paths\n",
    "    train_wide['image_path'] = train_wide['image_path'].apply(\n",
    "        lambda p: str(cfg.DATA_PATH / p) if not p.startswith('/') else p\n",
    "    )\n",
    "    test_wide['image_path'] = test_wide['image_path'].apply(\n",
    "        lambda p: str(cfg.DATA_PATH / p) if not p.startswith('/') else p\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ Train: {len(train_wide)} samples\")\n",
    "    print(f\"✓ Test:  {len(test_wide)} samples\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 2: Compute embeddings SEPARATELY (NO LEAKAGE!)\n",
    "    # ========================================================================\n",
    "    print(\"\\n2. Computing embeddings (NO LEAKAGE - separate train/test)...\")\n",
    "    \n",
    "    print(\"\\n2a. Computing TRAINING embeddings...\")\n",
    "    train_emb = compute_embeddings(cfg.SIGLIP_PATH, train_wide, cfg.patch_size, cfg.patch_overlap)\n",
    "    \n",
    "    print(\"\\n2b. Computing TEST embeddings...\")\n",
    "    test_emb = compute_embeddings(cfg.SIGLIP_PATH, test_wide, cfg.patch_size, cfg.patch_overlap)\n",
    "    \n",
    "    print(\"\\n✓ Embeddings computed separately (no leakage!)\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 3: Prepare features and targets\n",
    "    # ========================================================================\n",
    "    print(\"\\n3. Preparing features...\")\n",
    "    emb_cols = [c for c in train_emb.columns if c.startswith('emb')]\n",
    "    \n",
    "    X_train = train_emb[emb_cols].values\n",
    "    y_train = train_emb[TARGET_NAMES].values\n",
    "    X_test = test_emb[emb_cols].values\n",
    "    \n",
    "    # Normalize targets\n",
    "    target_max_arr = np.array([TARGET_MAX[t] for t in TARGET_NAMES])\n",
    "    y_train_norm = y_train / target_max_arr\n",
    "    \n",
    "    print(f\"✓ Features: {X_train.shape[1]} dimensions\")\n",
    "    print(f\"✓ Targets:  {y_train.shape[1]} outputs\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 4: GroupKFold Cross-Validation\n",
    "    # ========================================================================\n",
    "    print(f\"\\n4. Training with {cfg.n_folds}-fold GroupKFold CV...\")\n",
    "    \n",
    "    # Create groups from image paths\n",
    "    unique_images = train_emb['image_path'].unique()\n",
    "    img_to_group = {img: i for i, img in enumerate(unique_images)}\n",
    "    groups = train_emb['image_path'].map(img_to_group).values\n",
    "    \n",
    "    kfold = GroupKFold(n_splits=cfg.n_folds)\n",
    "    \n",
    "    oof_preds = np.zeros_like(y_train)\n",
    "    test_preds = np.zeros((len(test_emb), len(TARGET_NAMES)))\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(X_train, groups=groups)):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Fold {fold+1}/{cfg.n_folds}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_tr, y_val = y_train_norm[train_idx], y_train_norm[val_idx]\n",
    "        \n",
    "        # Feature engineering\n",
    "        engine = FeatureEngine()\n",
    "        engine.fit(X_tr, y_tr)\n",
    "        \n",
    "        X_tr_eng = engine.transform(X_tr)\n",
    "        X_val_eng = engine.transform(X_val)\n",
    "        X_test_eng = engine.transform(X_test)\n",
    "        \n",
    "        fold_test_preds = np.zeros((len(X_test), len(TARGET_NAMES)))\n",
    "        fold_val_preds = np.zeros((len(X_val), len(TARGET_NAMES)))\n",
    "        \n",
    "        for i, target_name in enumerate(TARGET_NAMES):\n",
    "            print(f\"  Training {target_name}...\", end=\" \")\n",
    "            \n",
    "            # LightGBM\n",
    "            lgb_model = LGBMRegressor(\n",
    "                learning_rate=0.01,\n",
    "                n_estimators=500,\n",
    "                num_leaves=31,\n",
    "                max_depth=6,\n",
    "                min_child_samples=20,\n",
    "                feature_fraction=0.8,\n",
    "                bagging_fraction=0.8,\n",
    "                bagging_freq=5,\n",
    "                random_state=42,\n",
    "                verbose=-1\n",
    "            )\n",
    "            lgb_model.fit(\n",
    "                X_tr_eng, y_tr[:, i],\n",
    "                eval_set=[(X_val_eng, y_val[:, i])],\n",
    "                callbacks=[lgb.early_stopping(50, verbose=False)]\n",
    "            )\n",
    "            \n",
    "            # CatBoost\n",
    "            cat_model = CatBoostRegressor(\n",
    "                learning_rate=0.01,\n",
    "                iterations=500,\n",
    "                depth=6,\n",
    "                l2_leaf_reg=3,\n",
    "                random_state=42,\n",
    "                verbose=0\n",
    "            )\n",
    "            cat_model.fit(\n",
    "                X_tr_eng, y_tr[:, i],\n",
    "                eval_set=(X_val_eng, y_val[:, i])\n",
    "            )\n",
    "            \n",
    "            # Ensemble\n",
    "            pred_val = 0.6 * lgb_model.predict(X_val_eng) + 0.4 * cat_model.predict(X_val_eng)\n",
    "            pred_test = 0.6 * lgb_model.predict(X_test_eng) + 0.4 * cat_model.predict(X_test_eng)\n",
    "            \n",
    "            # Denormalize\n",
    "            fold_val_preds[:, i] = pred_val * target_max_arr[i]\n",
    "            fold_test_preds[:, i] = pred_test * target_max_arr[i]\n",
    "            \n",
    "            print(\"✓\")\n",
    "        \n",
    "        # Store OOF predictions\n",
    "        oof_preds[val_idx] = fold_val_preds\n",
    "        test_preds += fold_test_preds / cfg.n_folds\n",
    "        \n",
    "        # Fold score\n",
    "        fold_score = competition_metric(y_train[val_idx], fold_val_preds)\n",
    "        print(f\"\\nFold {fold+1} Score: {fold_score:.6f}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 5: Evaluate OOF Performance\n",
    "    # ========================================================================\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    oof_score = competition_metric(y_train, oof_preds)\n",
    "    print(f\"OOF CV Score: {oof_score:.6f}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 6: Apply Reconciliation\n",
    "    # ========================================================================\n",
    "    print(\"\\n5. Applying hierarchical reconciliation...\")\n",
    "    \n",
    "    # Before reconciliation\n",
    "    print(\"Before reconciliation:\")\n",
    "    print(f\"  Mean predictions: {test_preds.mean(axis=0)}\")\n",
    "    \n",
    "    test_preds_rec = hierarchical_reconciliation(test_preds)\n",
    "    \n",
    "    print(\"After reconciliation:\")\n",
    "    print(f\"  Mean predictions: {test_preds_rec.mean(axis=0)}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 7: Create Submission\n",
    "    # ========================================================================\n",
    "    print(\"\\n6. Creating submission...\")\n",
    "    \n",
    "    test_wide_final = test_wide.copy()\n",
    "    test_wide_final[TARGET_NAMES] = test_preds_rec\n",
    "    \n",
    "    submission = melt_table(test_wide_final)\n",
    "    submission = submission[['sample_id', 'target']]\n",
    "    \n",
    "    # Final checks\n",
    "    print(f\"\\n✓ Submission shape: {submission.shape}\")\n",
    "    print(f\"✓ NaN count: {submission['target'].isna().sum()}\")\n",
    "    print(f\"✓ Negative count: {(submission['target'] < 0).sum()}\")\n",
    "    print(f\"\\nPrediction statistics:\")\n",
    "    print(submission['target'].describe())\n",
    "    \n",
    "    # Save\n",
    "    submission.to_csv('submission.csv', index=False)\n",
    "    print(f\"\\n✓ Saved: submission.csv\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PIPELINE COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nExpected public LB score: 0.73-0.75\")\n",
    "    print(f\"(Based on OOF CV: {oof_score:.4f})\")\n",
    "    \n",
    "    return submission, oof_score\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    submission, oof_score = main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 14254895,
     "sourceId": 112509,
     "sourceType": "competition"
    },
    {
     "datasetId": 8856212,
     "isSourceIdPinned": false,
     "sourceId": 13900620,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8929818,
     "isSourceIdPinned": false,
     "sourceId": 14018229,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 288467413,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 288761108,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 986,
     "modelInstanceId": 3329,
     "sourceId": 4537,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 251887,
     "modelInstanceId": 230141,
     "sourceId": 268942,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 487624,
     "modelInstanceId": 471723,
     "sourceId": 663314,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 967.43175,
   "end_time": "2026-01-11T17:30:42.556342",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-11T17:14:35.124592",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "00ac9679b563431db836cce8209e4f6d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_19fa7de544ac49dd9f2d01344b8e8952",
       "placeholder": "​",
       "style": "IPY_MODEL_46fbf19aa61e4b3d959780caa79fbeef",
       "tabbable": null,
       "tooltip": null,
       "value": " 1/1 [00:02&lt;00:00,  2.04s/it]"
      }
     },
     "19fa7de544ac49dd9f2d01344b8e8952": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "38b4d0f8d2ab4060a06708ed58c542fd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4305720f82db4438b1458fed139afe1e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "46fbf19aa61e4b3d959780caa79fbeef": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "5ac3ddf2c8364e0f95571e3ec420d476": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5d3c30d4e12a4cc0bbd699bad2e503ea": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "66a7c86c27404236b0f777aba4316b3c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "74c3b08ec515477e98cbc91d63371754": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e72443ef0e7142c094fafd33c25e2c99",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_5d3c30d4e12a4cc0bbd699bad2e503ea",
       "tabbable": null,
       "tooltip": null,
       "value": 1.0
      }
     },
     "7637083d8ce14ac092e6eb6de290ee55": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "807b2acc437943098814419c2881d7d4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "88bf05a2629743f8a83cea53351a99f1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_f4f598c6df914f1f8aa29285cdff1517",
        "IPY_MODEL_94e41bdc87d94f709860a52fc45dbad0",
        "IPY_MODEL_e83c93e0df444af0b198ca5fcfa7f52c"
       ],
       "layout": "IPY_MODEL_7637083d8ce14ac092e6eb6de290ee55",
       "tabbable": null,
       "tooltip": null
      }
     },
     "89bb28ae19c94ff997283b21c56325f3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "94e41bdc87d94f709860a52fc45dbad0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_89bb28ae19c94ff997283b21c56325f3",
       "max": 357.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_fa9b9f829e184c0497b56b6ea9e2ffcd",
       "tabbable": null,
       "tooltip": null,
       "value": 357.0
      }
     },
     "98308b6137544a8da6d204815afb6c68": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_e5abb46d039342369c25a048e6e1abb9",
        "IPY_MODEL_74c3b08ec515477e98cbc91d63371754",
        "IPY_MODEL_00ac9679b563431db836cce8209e4f6d"
       ],
       "layout": "IPY_MODEL_5ac3ddf2c8364e0f95571e3ec420d476",
       "tabbable": null,
       "tooltip": null
      }
     },
     "b688cbfd774746038769b5cd9a575438": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "bd89b708b70643189ee6a756e3bec491": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "e5abb46d039342369c25a048e6e1abb9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_807b2acc437943098814419c2881d7d4",
       "placeholder": "​",
       "style": "IPY_MODEL_66a7c86c27404236b0f777aba4316b3c",
       "tabbable": null,
       "tooltip": null,
       "value": "Computing embeddings: 100%"
      }
     },
     "e72443ef0e7142c094fafd33c25e2c99": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e83c93e0df444af0b198ca5fcfa7f52c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_38b4d0f8d2ab4060a06708ed58c542fd",
       "placeholder": "​",
       "style": "IPY_MODEL_4305720f82db4438b1458fed139afe1e",
       "tabbable": null,
       "tooltip": null,
       "value": " 357/357 [11:48&lt;00:00,  2.01s/it]"
      }
     },
     "f4f598c6df914f1f8aa29285cdff1517": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b688cbfd774746038769b5cd9a575438",
       "placeholder": "​",
       "style": "IPY_MODEL_bd89b708b70643189ee6a756e3bec491",
       "tabbable": null,
       "tooltip": null,
       "value": "Computing embeddings: 100%"
      }
     },
     "fa9b9f829e184c0497b56b6ea9e2ffcd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
