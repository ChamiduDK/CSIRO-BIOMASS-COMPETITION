{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e5e57c5",
   "metadata": {
    "_cell_guid": "f53f0eae-4d26-4bb8-89a6-f7450ede2dea",
    "_uuid": "c0ddd839-4e54-4dc6-823b-d7db8db1f672",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-01-10T15:15:38.008854Z",
     "iopub.status.busy": "2026-01-10T15:15:38.008216Z",
     "iopub.status.idle": "2026-01-10T15:15:38.991419Z",
     "shell.execute_reply": "2026-01-10T15:15:38.990596Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.992496,
     "end_time": "2026-01-10T15:15:38.993869",
     "exception": false,
     "start_time": "2026-01-10T15:15:38.001373",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/google-siglip-so400m-patch14-384/transformers/default/1/config.json\n",
      "/kaggle/input/google-siglip-so400m-patch14-384/transformers/default/1/preprocessor_config.json\n",
      "/kaggle/input/google-siglip-so400m-patch14-384/transformers/default/1/spiece.model\n",
      "/kaggle/input/google-siglip-so400m-patch14-384/transformers/default/1/README.md\n",
      "/kaggle/input/google-siglip-so400m-patch14-384/transformers/default/1/tokenizer.json\n",
      "/kaggle/input/google-siglip-so400m-patch14-384/transformers/default/1/tokenizer_config.json\n",
      "/kaggle/input/google-siglip-so400m-patch14-384/transformers/default/1/gitattributes\n",
      "/kaggle/input/google-siglip-so400m-patch14-384/transformers/default/1/model.safetensors\n",
      "/kaggle/input/google-siglip-so400m-patch14-384/transformers/default/1/special_tokens_map.json\n",
      "/kaggle/input/csiro-biomass/sample_submission.csv\n",
      "/kaggle/input/csiro-biomass/train.csv\n",
      "/kaggle/input/csiro-biomass/test.csv\n",
      "/kaggle/input/csiro-biomass/test/ID1001187975.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2099464826.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2037861084.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1211362607.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1853508321.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID193102215.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID698608346.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1859251563.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1880764911.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID853954911.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1403107574.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1781353117.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID384648061.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1563418511.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2125100696.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID482555369.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID638711343.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID779628955.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1876271942.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1692894460.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID746335827.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1136169672.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1471216911.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID846154859.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1294770420.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1183807388.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID423506847.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1889150649.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1140993511.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1413758094.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1545077474.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID95050718.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID528010569.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1645161155.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID786365141.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID896386823.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1025234388.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID663006174.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1509266870.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1496750796.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID471758347.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID740402124.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1624268863.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1098771283.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID710341728.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2086966681.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1573329652.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID54128926.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID50027657.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1559189397.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID290369222.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1590632667.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID552040066.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID488873801.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID363069566.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1839139621.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1131079710.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2010625680.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID152157478.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1357758282.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1498398599.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID679913293.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID697718693.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID4464212.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1275072698.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1579942839.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID799079114.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1415329644.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1510574031.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1078930021.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1456861072.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID930534670.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID13162390.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID567744300.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID344618040.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID566966892.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1437386574.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID667059550.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID72895391.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1193692654.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1386202352.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID871463897.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2096636211.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2003438517.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID21377800.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID230058600.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1753847361.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1512751450.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID12390962.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1746343319.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID978026131.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID383231615.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID146920896.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1036339023.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1168534540.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1859792585.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1251029854.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1113329413.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1874904894.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1671844336.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1831254380.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1103883611.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID797502182.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1784585001.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1058383417.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1488408526.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID429799190.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1291116815.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1516374298.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1618597318.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1345375788.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID686797154.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1139866256.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1149598723.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID212206250.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID112966473.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1540480250.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID544444725.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1513184765.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID668330410.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1444674500.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1962379474.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID605134229.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID914754166.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID354528442.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID950496197.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1395011773.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1357768767.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID210865340.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID936984905.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1976436386.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1215977190.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID803479541.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1244346858.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID158170916.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1208644039.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1314135397.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1012260530.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1053972079.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID656251220.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1084819986.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1337107565.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1268934251.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID617132135.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1472525822.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID668475812.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID681680726.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1476045099.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1570190541.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1403078396.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2030696575.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1782608354.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID194823383.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID196516535.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID212206832.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1638922597.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1457700382.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1989506559.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID789169173.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1634731537.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1428837636.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2006686196.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID885388135.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1789853061.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1655778545.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID697059386.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID121331988.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2099742797.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID342818398.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID317990700.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID706288721.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1159071020.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID755710743.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1254829053.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID475010202.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1693880739.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1894998379.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID48303557.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1385921939.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID147528735.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID407646960.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1035947949.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1119761112.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1988033238.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1857489997.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID742198710.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID588120964.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID431471530.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID353424190.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID380752847.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2069766023.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID600602588.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID560946727.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1011485656.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID808079729.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1217108125.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1623964968.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID980878870.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID793526563.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID397994621.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID975115267.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1237349078.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID684383343.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID866684633.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1665142816.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2048645043.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1953171547.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1451025862.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID71885430.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID307060225.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID969218269.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID980538882.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1028611175.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID670276799.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2002797732.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1374789439.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID473494649.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1993907137.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1962197151.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID828217731.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID972274220.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1954669045.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1354190372.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1458758610.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID40849327.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1952813879.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID572336285.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1473228876.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1963715583.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1463690813.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1899025384.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID386216505.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1789265307.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID315357834.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2089023774.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID520514019.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1970522802.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1139918758.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1051144034.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1370004842.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID761508093.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2052993274.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1277756619.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID6269659.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1574125908.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID135365668.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1182523622.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID554314721.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1049634115.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1127246618.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID900012207.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID574213894.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID415656958.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID61833032.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2053315094.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID550623196.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID657448172.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1675365449.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2014192906.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID162394992.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID968643034.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID684062938.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID802547515.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID294150104.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1618145129.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID956512130.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID142751858.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID325799913.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID443091455.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID661372352.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1062837331.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID498304885.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID187238869.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1450399782.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2056023629.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID576621307.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1199150612.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1411613934.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID105271783.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1703304524.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID875119737.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1176292407.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1729002155.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2091439402.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID576137678.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1946311744.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1982662138.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID983582017.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID661817669.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID753699705.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1789834546.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID529933668.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID490139972.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID743847993.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID7850481.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1088965591.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID629980789.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1119739385.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1477176296.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1113121340.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2131261930.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2145635095.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1414371018.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1148666289.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID839432753.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID157479394.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1761544403.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID846984946.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID751517087.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID577112774.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID353997899.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID748979397.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1070112260.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1108283583.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1868719645.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1980675327.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1163061745.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1148528732.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID534966093.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1717006117.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1953218650.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID633775166.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID808093827.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1997244125.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1920959057.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1948354837.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID364856705.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID249042826.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID332742639.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1680597197.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1421714468.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID905397692.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1782509721.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID141370843.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2056982009.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID94564238.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID8209776.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID908524512.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID610397481.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID750820644.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1515990019.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1547945326.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID587125778.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1620371305.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1474775613.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID545360459.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1783499590.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1249094008.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1525817840.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID227847873.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1052620238.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1888700589.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2052442675.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID963903358.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1121692672.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1343327476.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1667778338.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID257822026.jpg\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_0/metrics.csv\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_0/swanlab_info.json\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_0/checkpoints/best_loss.pt\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_0/checkpoints/best_wr2.pt\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_0/checkpoints/last.pt\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_4/metrics.csv\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_4/swanlab_info.json\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_4/checkpoints/best_loss.pt\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_4/checkpoints/best_wr2.pt\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_4/checkpoints/last.pt\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_1/metrics.csv\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_1/swanlab_info.json\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_1/checkpoints/best_loss.pt\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_1/checkpoints/best_wr2.pt\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_1/checkpoints/last.pt\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_3/metrics.csv\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_3/swanlab_info.json\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_3/checkpoints/best_loss.pt\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_3/checkpoints/best_wr2.pt\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_3/checkpoints/last.pt\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_2/metrics.csv\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_2/swanlab_info.json\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_2/checkpoints/best_loss.pt\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_2/checkpoints/best_wr2.pt\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_2/checkpoints/last.pt\n",
      "/kaggle/input/dinov2/pytorch/giant/1/config.json\n",
      "/kaggle/input/dinov2/pytorch/giant/1/preprocessor_config.json\n",
      "/kaggle/input/dinov2/pytorch/giant/1/README.md\n",
      "/kaggle/input/dinov2/pytorch/giant/1/pytorch_model.bin\n",
      "/kaggle/input/dinov2/pytorch/giant/1/.gitattributes\n",
      "/kaggle/input/csiro-image-embeddings/train_siglip_embeddings.csv\n",
      "/kaggle/input/csiro-image-embeddings/train_dino_embeddings.csv\n",
      "/kaggle/input/csiro-biomass-competition-comprehensive-eda/__results__.html\n",
      "/kaggle/input/csiro-biomass-competition-comprehensive-eda/__notebook__.ipynb\n",
      "/kaggle/input/csiro-biomass-competition-comprehensive-eda/__output__.json\n",
      "/kaggle/input/csiro-biomass-competition-comprehensive-eda/custom.css\n",
      "/kaggle/input/csiro-biomass-competition-comprehensive-eda/__results___files/__results___15_0.png\n",
      "/kaggle/input/csiro-biomass-competition-comprehensive-eda/__results___files/__results___25_0.png\n",
      "/kaggle/input/csiro-biomass-competition-comprehensive-eda/__results___files/__results___19_0.png\n",
      "/kaggle/input/csiro-biomass-competition-comprehensive-eda/__results___files/__results___31_0.png\n",
      "/kaggle/input/csiro-biomass-competition-comprehensive-eda/__results___files/__results___22_0.png\n",
      "/kaggle/input/csiro-biomass-competition-comprehensive-eda/__results___files/__results___28_0.png\n",
      "/kaggle/input/csiro-datasplit/__results__.html\n",
      "/kaggle/input/csiro-datasplit/csiro_data_split.csv\n",
      "/kaggle/input/csiro-datasplit/__notebook__.ipynb\n",
      "/kaggle/input/csiro-datasplit/__output__.json\n",
      "/kaggle/input/csiro-datasplit/custom.css\n",
      "/kaggle/input/csiro-mvp-models/model1.pth\n",
      "/kaggle/input/csiro-mvp-models/model2.pth\n",
      "/kaggle/input/csiro-mvp-models/model10.pth\n",
      "/kaggle/input/csiro-mvp-models/model7.pth\n",
      "/kaggle/input/csiro-mvp-models/model6.pth\n",
      "/kaggle/input/csiro-mvp-models/model4.pth\n",
      "/kaggle/input/csiro-mvp-models/model5.pth\n",
      "/kaggle/input/csiro-mvp-models/model9.pth\n",
      "/kaggle/input/csiro-mvp-models/model3.pth\n",
      "/kaggle/input/csiro-mvp-models/model8.pth\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2685c0e7",
   "metadata": {
    "_cell_guid": "4bbbc35a-1b46-41f0-ae38-a4cd5616aeaa",
    "_uuid": "900028d6-5e27-4e09-a86f-dcd3d7d5a2b5",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-01-10T15:15:39.006699Z",
     "iopub.status.busy": "2026-01-10T15:15:39.006194Z",
     "iopub.status.idle": "2026-01-10T15:16:05.211847Z",
     "shell.execute_reply": "2026-01-10T15:16:05.211168Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 26.213698,
     "end_time": "2026-01-10T15:16:05.213756",
     "exception": false,
     "start_time": "2026-01-10T15:15:39.000058",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CSIRO BIOMASS COMPETITION - PROFESSIONAL EDA\n",
      "================================================================================\n",
      "\n",
      "Loading data...\n",
      "Train shape: (1785, 9)\n",
      "Test shape: (5, 3)\n",
      "Pivoted train shape: (357, 6)\n",
      "\n",
      "================================================================================\n",
      "Creating Visualization 1: Target Distributions with Normal Curves\n",
      "================================================================================\n",
      "✓ Saved: professional_1_distributions.png\n",
      "\n",
      "================================================================================\n",
      "Creating Visualization 2: Complete Correlation Matrix\n",
      "================================================================================\n",
      "✓ Saved: professional_2_correlation_matrix.png\n",
      "\n",
      "================================================================================\n",
      "Creating Visualization 3: Detailed Box Plots\n",
      "================================================================================\n",
      "✓ Saved: professional_3_boxplots.png\n",
      "\n",
      "================================================================================\n",
      "Creating Visualization 4: Scatter Plot Matrix\n",
      "================================================================================\n",
      "✓ Saved: professional_4_scatter_matrix.png\n",
      "\n",
      "================================================================================\n",
      "Creating Visualization 5: Detailed Sample Image Analysis\n",
      "================================================================================\n",
      "✓ Saved: professional_5_sample_images.png\n",
      "\n",
      "================================================================================\n",
      "Creating Visualization 6: Hierarchical Relationships\n",
      "================================================================================\n",
      "✓ Saved: professional_6_hierarchical.png\n",
      "\n",
      "================================================================================\n",
      "EDA COMPLETE - ALL VISUALIZATIONS SAVED\n",
      "================================================================================\n",
      "\n",
      "Generated Professional Visualizations:\n",
      "  1. professional_1_distributions.png - Target distributions with bell curves\n",
      "  2. professional_2_correlation_matrix.png - Complete correlation analysis\n",
      "  3. professional_3_boxplots.png - Detailed box plots with statistics\n",
      "  4. professional_4_scatter_matrix.png - Scatter plot matrix with regression\n",
      "  5. professional_5_sample_images.png - Detailed image analysis\n",
      "  6. professional_6_hierarchical.png - Hierarchical relationship validation\n",
      "\n",
      "All visualizations use consistent professional color scheme\n",
      "Statistical details and annotations included on all plots\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# CSIRO BIOMASS COMPETITION - PROFESSIONAL EDA\n",
    "# ================================================================\n",
    "# Professional visualizations with consistent color scheme\n",
    "# Complete statistical analysis with bell curves and annotations\n",
    "# ================================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import skew, kurtosis, normaltest, pearsonr, spearmanr, norm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# ================================================================\n",
    "# PROFESSIONAL STYLING CONFIGURATION\n",
    "# ================================================================\n",
    "\n",
    "# Professional color palette (consistent across all plots)\n",
    "COLORS = {\n",
    "    'primary': '#2E4057',      # Dark blue-gray (main)\n",
    "    'secondary': '#048A81',     # Teal (accent)\n",
    "    'tertiary': '#54C6EB',      # Light blue\n",
    "    'highlight': '#D95D39',     # Coral red (emphasis)\n",
    "    'neutral': '#8B8B8B',       # Gray\n",
    "    'background': '#F5F5F5',    # Light gray background\n",
    "    'grid': '#E0E0E0',          # Grid color\n",
    "    'text': '#2C3E50'           # Dark text\n",
    "}\n",
    "\n",
    "# Set professional matplotlib style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams.update({\n",
    "    'figure.figsize': (12, 7),\n",
    "    'font.size': 11,\n",
    "    'axes.labelsize': 12,\n",
    "    'axes.titlesize': 14,\n",
    "    'xtick.labelsize': 10,\n",
    "    'ytick.labelsize': 10,\n",
    "    'legend.fontsize': 10,\n",
    "    'figure.titlesize': 16,\n",
    "    'axes.facecolor': COLORS['background'],\n",
    "    'figure.facecolor': 'white',\n",
    "    'axes.edgecolor': COLORS['neutral'],\n",
    "    'grid.color': COLORS['grid'],\n",
    "    'grid.alpha': 0.3,\n",
    "    'text.color': COLORS['text'],\n",
    "    'axes.labelcolor': COLORS['text'],\n",
    "    'xtick.color': COLORS['text'],\n",
    "    'ytick.color': COLORS['text']\n",
    "})\n",
    "\n",
    "# ================================================================\n",
    "# CONFIGURATION\n",
    "# ================================================================\n",
    "\n",
    "class Config:\n",
    "    BASE_PATH = Path(\"/kaggle/input/csiro-biomass/\")\n",
    "    TRAIN_PATH = BASE_PATH / \"train\"\n",
    "    TEST_PATH = BASE_PATH / \"test\"\n",
    "    OUTPUT_PATH = Path(\"/kaggle/working/\")\n",
    "    \n",
    "    TARGET_COLS = ['Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g', 'GDM_g', 'Dry_Total_g']\n",
    "    TARGET_NAMES = {\n",
    "        'Dry_Green_g': 'Green Biomass',\n",
    "        'Dry_Dead_g': 'Dead Biomass',\n",
    "        'Dry_Clover_g': 'Clover Biomass',\n",
    "        'GDM_g': 'Green Dry Matter',\n",
    "        'Dry_Total_g': 'Total Biomass'\n",
    "    }\n",
    "    \n",
    "    TARGET_WEIGHTS = {\n",
    "        'Dry_Green_g': 0.1,\n",
    "        'Dry_Dead_g': 0.1,\n",
    "        'Dry_Clover_g': 0.1,\n",
    "        'GDM_g': 0.2,\n",
    "        'Dry_Total_g': 0.5,\n",
    "    }\n",
    "\n",
    "cfg = Config()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CSIRO BIOMASS COMPETITION - PROFESSIONAL EDA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ================================================================\n",
    "# LOAD DATA\n",
    "# ================================================================\n",
    "\n",
    "print(\"\\nLoading data...\")\n",
    "train_df = pd.read_csv(cfg.BASE_PATH / \"train.csv\")\n",
    "test_df = pd.read_csv(cfg.BASE_PATH / \"test.csv\")\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")\n",
    "\n",
    "# Pivot to get one row per image\n",
    "train_pivot = train_df.pivot_table(\n",
    "    values='target',\n",
    "    index='image_path',\n",
    "    columns='target_name',\n",
    "    aggfunc='mean'\n",
    ").reset_index()\n",
    "\n",
    "print(f\"Pivoted train shape: {train_pivot.shape}\")\n",
    "\n",
    "# ================================================================\n",
    "# 1. COMPREHENSIVE TARGET DISTRIBUTION WITH BELL CURVES\n",
    "# ================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Creating Visualization 1: Target Distributions with Normal Curves\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fig = plt.figure(figsize=(20, 12))\n",
    "gs = fig.add_gridspec(3, 2, hspace=0.3, wspace=0.25)\n",
    "\n",
    "for idx, col in enumerate(cfg.TARGET_COLS):\n",
    "    row = idx // 2\n",
    "    col_idx = idx % 2\n",
    "    ax = fig.add_subplot(gs[row, col_idx])\n",
    "    \n",
    "    values = train_pivot[col].dropna().values\n",
    "    \n",
    "    # Histogram\n",
    "    n, bins, patches = ax.hist(values, bins=40, alpha=0.6, color=COLORS['primary'], \n",
    "                                edgecolor='white', linewidth=1.5, density=True,\n",
    "                                label='Observed Distribution')\n",
    "    \n",
    "    # Fit normal distribution\n",
    "    mu, sigma = values.mean(), values.std()\n",
    "    x = np.linspace(values.min(), values.max(), 100)\n",
    "    normal_curve = norm.pdf(x, mu, sigma)\n",
    "    \n",
    "    # Plot normal curve\n",
    "    ax.plot(x, normal_curve, color=COLORS['highlight'], linewidth=3, \n",
    "            label=f'Normal Fit (μ={mu:.1f}, σ={sigma:.1f})', linestyle='--')\n",
    "    \n",
    "    # Statistics lines\n",
    "    ax.axvline(mu, color=COLORS['secondary'], linestyle='-', linewidth=2.5, \n",
    "               label=f'Mean: {mu:.2f}g', alpha=0.8)\n",
    "    ax.axvline(np.median(values), color=COLORS['tertiary'], linestyle='-', linewidth=2.5,\n",
    "               label=f'Median: {np.median(values):.2f}g', alpha=0.8)\n",
    "    \n",
    "    # Annotations\n",
    "    ax.text(0.98, 0.97, f'n = {len(values):,}', transform=ax.transAxes,\n",
    "            verticalalignment='top', horizontalalignment='right',\n",
    "            bbox=dict(boxstyle='round', facecolor='white', alpha=0.9, edgecolor=COLORS['neutral']),\n",
    "            fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # Statistical properties\n",
    "    skewness = skew(values)\n",
    "    kurt = kurtosis(values)\n",
    "    stats_text = f'Skewness: {skewness:.2f}\\nKurtosis: {kurt:.2f}\\nMin: {values.min():.2f}g\\nMax: {values.max():.2f}g'\n",
    "    ax.text(0.02, 0.97, stats_text, transform=ax.transAxes,\n",
    "            verticalalignment='top', horizontalalignment='left',\n",
    "            bbox=dict(boxstyle='round', facecolor='white', alpha=0.9, edgecolor=COLORS['neutral']),\n",
    "            fontsize=9)\n",
    "    \n",
    "    # Labels and title\n",
    "    ax.set_xlabel(f'{cfg.TARGET_NAMES[col]} (grams)', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Probability Density', fontsize=12, fontweight='bold')\n",
    "    ax.set_title(f'{cfg.TARGET_NAMES[col]} Distribution', \n",
    "                 fontsize=14, fontweight='bold', pad=15, color=COLORS['text'])\n",
    "    ax.legend(loc='upper right', framealpha=0.95, edgecolor=COLORS['neutral'])\n",
    "    ax.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "# Add overall title\n",
    "fig.suptitle('Target Variable Distributions with Statistical Analysis', \n",
    "             fontsize=18, fontweight='bold', y=0.995, color=COLORS['text'])\n",
    "\n",
    "plt.savefig(cfg.OUTPUT_PATH / 'professional_1_distributions.png', dpi=300, bbox_inches='tight', \n",
    "            facecolor='white', edgecolor='none')\n",
    "print(\"✓ Saved: professional_1_distributions.png\")\n",
    "plt.close()\n",
    "\n",
    "# ================================================================\n",
    "# 2. COMPREHENSIVE CORRELATION MATRIX  \n",
    "# ================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Creating Visualization 2: Complete Correlation Matrix\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate correlations\n",
    "correlation_matrix = train_pivot[cfg.TARGET_COLS].corr()\n",
    "\n",
    "# Create simple figure\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "# Professional RED-WHITE-GREEN diverging colormap\n",
    "# Red (negative) → White/Yellow (zero) → Green (positive)\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "colors_cmap = [\n",
    "    '#D73027',  # Dark red (strong negative)\n",
    "    '#FC8D59',  # Medium red\n",
    "    '#FEE090',  # Light red/orange\n",
    "    '#FFFFBF',  # White/Yellow (zero)\n",
    "    '#E0F3DB',  # Very light green\n",
    "    '#A8DDB5',  # Light green\n",
    "    '#66C2A4',  # Medium light green\n",
    "    '#2CA25F',  # Medium green\n",
    "    '#006D2C'   # Dark green (strong positive)\n",
    "]\n",
    "n_bins = 100\n",
    "cmap_diverging = LinearSegmentedColormap.from_list('red_white_green', colors_cmap, N=n_bins)\n",
    "\n",
    "# Create heatmap with full range -1 to 1\n",
    "im = ax.imshow(correlation_matrix, cmap=cmap_diverging, aspect='auto', vmin=-1, vmax=1)\n",
    "\n",
    "# Set ticks\n",
    "ax.set_xticks(np.arange(len(cfg.TARGET_COLS)))\n",
    "ax.set_yticks(np.arange(len(cfg.TARGET_COLS)))\n",
    "ax.set_xticklabels([cfg.TARGET_NAMES[col] for col in cfg.TARGET_COLS], \n",
    "                    rotation=45, ha='right', fontsize=11, fontweight='bold')\n",
    "ax.set_yticklabels([cfg.TARGET_NAMES[col] for col in cfg.TARGET_COLS], \n",
    "                    fontsize=11, fontweight='bold')\n",
    "\n",
    "# Add correlation values and significance\n",
    "for i in range(len(cfg.TARGET_COLS)):\n",
    "    for j in range(len(cfg.TARGET_COLS)):\n",
    "        corr_val = correlation_matrix.iloc[i, j]\n",
    "        \n",
    "        # Calculate p-value\n",
    "        if i != j:\n",
    "            _, p_val = pearsonr(train_pivot[cfg.TARGET_COLS[i]], \n",
    "                               train_pivot[cfg.TARGET_COLS[j]])\n",
    "            sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else ''\n",
    "        else:\n",
    "            sig = ''\n",
    "        \n",
    "        # Choose text color based on correlation value\n",
    "        # Dark text for light colors (near zero), white text for dark colors (strong correlations)\n",
    "        if abs(corr_val) > 0.6:\n",
    "            text_color = 'white'\n",
    "        else:\n",
    "            text_color = 'black'\n",
    "        \n",
    "        # Main correlation value\n",
    "        ax.text(j, i, f'{corr_val:.3f}', ha='center', va='center',\n",
    "                color=text_color, fontsize=12, fontweight='bold')\n",
    "        \n",
    "        # Significance stars\n",
    "        if sig:\n",
    "            ax.text(j, i + 0.35, sig, ha='center', va='center',\n",
    "                    color=text_color, fontsize=10, fontweight='bold')\n",
    "\n",
    "# Colorbar with professional labels\n",
    "cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "cbar.set_label('Pearson Correlation Coefficient (r)', rotation=270, labelpad=25, \n",
    "               fontsize=12, fontweight='bold')\n",
    "cbar.ax.tick_params(labelsize=10)\n",
    "\n",
    "# Add interpretive labels on colorbar\n",
    "cbar.ax.text(3.5, -0.9, 'Strong Negative', transform=cbar.ax.transData,\n",
    "            fontsize=9, fontweight='bold', color='#D73027', rotation=0)\n",
    "cbar.ax.text(3.5, 0.0, 'No Correlation', transform=cbar.ax.transData,\n",
    "            fontsize=9, fontweight='bold', color='#666666', rotation=0)\n",
    "cbar.ax.text(3.5, 0.9, 'Strong Positive', transform=cbar.ax.transData,\n",
    "            fontsize=9, fontweight='bold', color='#006D2C', rotation=0)\n",
    "\n",
    "# Title with significance legend\n",
    "title_text = 'Complete Correlation Matrix: All Target Variables\\n'\n",
    "title_text += 'Red = Negative | Yellow = Zero | Green = Positive  |  '\n",
    "title_text += '*** p<0.001, ** p<0.01, * p<0.05'\n",
    "ax.set_title(title_text, fontsize=13, fontweight='bold', pad=20, color=COLORS['text'])\n",
    "\n",
    "# Grid\n",
    "ax.set_xticks(np.arange(len(cfg.TARGET_COLS)) - 0.5, minor=True)\n",
    "ax.set_yticks(np.arange(len(cfg.TARGET_COLS)) - 0.5, minor=True)\n",
    "ax.grid(which='minor', color='white', linestyle='-', linewidth=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(cfg.OUTPUT_PATH / 'professional_2_correlation_matrix.png', dpi=300, bbox_inches='tight',\n",
    "            facecolor='white', edgecolor='none')\n",
    "print(\"✓ Saved: professional_2_correlation_matrix.png\")\n",
    "plt.close()\n",
    "\n",
    "# ================================================================\n",
    "# 3. DETAILED BOX PLOTS WITH STATISTICS\n",
    "# ================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Creating Visualization 3: Detailed Box Plots\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "# Prepare data\n",
    "data_for_box = [train_pivot[col].dropna().values for col in cfg.TARGET_COLS]\n",
    "labels = [cfg.TARGET_NAMES[col] for col in cfg.TARGET_COLS]\n",
    "\n",
    "# Create box plot\n",
    "bp = ax.boxplot(data_for_box, labels=labels, patch_artist=True,\n",
    "                widths=0.6, showmeans=True,\n",
    "                meanprops=dict(marker='D', markerfacecolor=COLORS['highlight'], \n",
    "                              markeredgecolor='white', markersize=10),\n",
    "                medianprops=dict(color=COLORS['secondary'], linewidth=2.5),\n",
    "                boxprops=dict(facecolor=COLORS['primary'], alpha=0.7, \n",
    "                             edgecolor=COLORS['text'], linewidth=1.5),\n",
    "                whiskerprops=dict(color=COLORS['text'], linewidth=1.5),\n",
    "                capprops=dict(color=COLORS['text'], linewidth=1.5),\n",
    "                flierprops=dict(marker='o', markerfacecolor=COLORS['highlight'], \n",
    "                               markersize=5, alpha=0.5, markeredgecolor='none'))\n",
    "\n",
    "# Add detailed statistics\n",
    "for idx, (col, data) in enumerate(zip(cfg.TARGET_COLS, data_for_box)):\n",
    "    # Calculate statistics\n",
    "    q1 = np.percentile(data, 25)\n",
    "    median = np.median(data)\n",
    "    q3 = np.percentile(data, 75)\n",
    "    mean = np.mean(data)\n",
    "    iqr = q3 - q1\n",
    "    lower_whisker = q1 - 1.5 * iqr\n",
    "    upper_whisker = q3 + 1.5 * iqr\n",
    "    outliers = len(data[(data < lower_whisker) | (data > upper_whisker)])\n",
    "    \n",
    "    # Add text annotation\n",
    "    stats_text = f'Mean: {mean:.1f}g\\nMedian: {median:.1f}g\\nIQR: {iqr:.1f}g\\nOutliers: {outliers}'\n",
    "    ax.text(idx + 1, ax.get_ylim()[1] * 0.95, stats_text,\n",
    "            ha='center', va='top', fontsize=9,\n",
    "            bbox=dict(boxstyle='round', facecolor='white', alpha=0.9, \n",
    "                     edgecolor=COLORS['neutral']))\n",
    "\n",
    "# Labels and formatting\n",
    "ax.set_ylabel('Biomass (grams)', fontsize=13, fontweight='bold')\n",
    "ax.set_xlabel('Target Variables', fontsize=13, fontweight='bold')\n",
    "ax.set_title('Box Plot Analysis: Target Variables with Statistical Details', \n",
    "             fontsize=15, fontweight='bold', pad=20, color=COLORS['text'])\n",
    "\n",
    "# Legend\n",
    "legend_elements = [\n",
    "    plt.Line2D([0], [0], marker='D', color='w', label='Mean',\n",
    "              markerfacecolor=COLORS['highlight'], markersize=10),\n",
    "    plt.Line2D([0], [0], color=COLORS['secondary'], linewidth=2.5, label='Median'),\n",
    "    plt.Line2D([0], [0], marker='o', color='w', label='Outliers',\n",
    "              markerfacecolor=COLORS['highlight'], markersize=7, alpha=0.5)\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc='upper right', framealpha=0.95, \n",
    "         edgecolor=COLORS['neutral'], fontsize=11)\n",
    "\n",
    "ax.grid(True, alpha=0.3, axis='y', linestyle='--')\n",
    "ax.set_axisbelow(True)\n",
    "plt.xticks(rotation=15, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(cfg.OUTPUT_PATH / 'professional_3_boxplots.png', dpi=300, bbox_inches='tight',\n",
    "            facecolor='white', edgecolor='none')\n",
    "print(\"✓ Saved: professional_3_boxplots.png\")\n",
    "plt.close()\n",
    "\n",
    "# ================================================================\n",
    "# 4. SCATTER PLOT MATRIX WITH REGRESSION LINES\n",
    "# ================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Creating Visualization 4: Scatter Plot Matrix\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Select subset of targets for clarity\n",
    "main_targets = ['Dry_Green_g', 'Dry_Dead_g', 'GDM_g', 'Dry_Total_g']\n",
    "n_targets = len(main_targets)\n",
    "\n",
    "fig, axes = plt.subplots(n_targets, n_targets, figsize=(18, 18))\n",
    "\n",
    "for i, target1 in enumerate(main_targets):\n",
    "    for j, target2 in enumerate(main_targets):\n",
    "        ax = axes[i, j]\n",
    "        \n",
    "        if i == j:\n",
    "            # Diagonal: histogram with KDE\n",
    "            data = train_pivot[target1].dropna().values\n",
    "            ax.hist(data, bins=30, alpha=0.6, color=COLORS['primary'], \n",
    "                   edgecolor='white', density=True)\n",
    "            \n",
    "            # KDE\n",
    "            from scipy.stats import gaussian_kde\n",
    "            kde = gaussian_kde(data)\n",
    "            x_range = np.linspace(data.min(), data.max(), 100)\n",
    "            ax.plot(x_range, kde(x_range), color=COLORS['highlight'], \n",
    "                   linewidth=2.5, label='KDE')\n",
    "            \n",
    "            ax.set_ylabel('Density', fontsize=9)\n",
    "            if i == n_targets - 1:\n",
    "                ax.set_xlabel(cfg.TARGET_NAMES[target1], fontsize=10, fontweight='bold')\n",
    "        else:\n",
    "            # Off-diagonal: scatter plot\n",
    "            x_data = train_pivot[target2].dropna()\n",
    "            y_data = train_pivot[target1].dropna()\n",
    "            \n",
    "            # Align data\n",
    "            common_idx = x_data.index.intersection(y_data.index)\n",
    "            x_vals = x_data.loc[common_idx].values\n",
    "            y_vals = y_data.loc[common_idx].values\n",
    "            \n",
    "            # Scatter\n",
    "            ax.scatter(x_vals, y_vals, alpha=0.4, s=20, color=COLORS['primary'], \n",
    "                      edgecolors='none')\n",
    "            \n",
    "            # Regression line\n",
    "            if len(x_vals) > 1:\n",
    "                z = np.polyfit(x_vals, y_vals, 1)\n",
    "                p = np.poly1d(z)\n",
    "                x_line = np.linspace(x_vals.min(), x_vals.max(), 100)\n",
    "                ax.plot(x_line, p(x_line), color=COLORS['highlight'], \n",
    "                       linewidth=2.5, linestyle='--')\n",
    "                \n",
    "                # Correlation\n",
    "                r, p_val = pearsonr(x_vals, y_vals)\n",
    "                ax.text(0.05, 0.95, f'r={r:.3f}', transform=ax.transAxes,\n",
    "                       verticalalignment='top', fontsize=9, fontweight='bold',\n",
    "                       bbox=dict(boxstyle='round', facecolor='white', alpha=0.9))\n",
    "        \n",
    "        # Labels\n",
    "        if j == 0:\n",
    "            ax.set_ylabel(cfg.TARGET_NAMES[target1], fontsize=10, fontweight='bold')\n",
    "        if i == n_targets - 1:\n",
    "            ax.set_xlabel(cfg.TARGET_NAMES[target2], fontsize=10, fontweight='bold')\n",
    "        \n",
    "        ax.grid(True, alpha=0.2, linestyle='--')\n",
    "        ax.tick_params(labelsize=8)\n",
    "\n",
    "fig.suptitle('Scatter Plot Matrix: Pairwise Relationships Between Targets', \n",
    "             fontsize=16, fontweight='bold', y=0.995)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(cfg.OUTPUT_PATH / 'professional_4_scatter_matrix.png', dpi=300, bbox_inches='tight',\n",
    "            facecolor='white', edgecolor='none')\n",
    "print(\"✓ Saved: professional_4_scatter_matrix.png\")\n",
    "plt.close()\n",
    "\n",
    "# ================================================================\n",
    "# 5. DETAILED SAMPLE IMAGE ANALYSIS\n",
    "# ================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Creating Visualization 5: Detailed Sample Image Analysis\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Select 6 sample images\n",
    "sample_images = train_pivot['image_path'].sample(min(6, len(train_pivot)), random_state=42).tolist()\n",
    "\n",
    "fig = plt.figure(figsize=(20, 16))\n",
    "gs = fig.add_gridspec(6, 4, hspace=0.35, wspace=0.3)  # 6 rows, 4 columns\n",
    "\n",
    "for idx, img_path in enumerate(sample_images):\n",
    "    img_name = os.path.basename(img_path)\n",
    "    full_path = cfg.TRAIN_PATH / img_name\n",
    "    \n",
    "    img = cv2.imread(str(full_path))\n",
    "    if img is None:\n",
    "        continue\n",
    "    \n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img_hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    # Main image (left column)\n",
    "    ax_img = fig.add_subplot(gs[idx, 0])\n",
    "    ax_img.imshow(img_rgb)\n",
    "    ax_img.set_title(f'Sample {idx+1}', fontsize=12, fontweight='bold')\n",
    "    ax_img.axis('off')\n",
    "    \n",
    "    # Add image properties\n",
    "    h, w, _ = img_rgb.shape\n",
    "    green_ratio = img_rgb[:,:,1] / (img_rgb.sum(axis=2) + 1)\n",
    "    brightness = img_rgb.mean()\n",
    "    \n",
    "    props_text = f'Size: {w}×{h}\\nBrightness: {brightness:.1f}\\nGreen Ratio: {green_ratio.mean():.3f}'\n",
    "    ax_img.text(0.02, 0.98, props_text, transform=ax_img.transAxes,\n",
    "               verticalalignment='top', fontsize=9,\n",
    "               bbox=dict(boxstyle='round', facecolor='white', alpha=0.9))\n",
    "    \n",
    "    # RGB histogram\n",
    "    ax_hist = fig.add_subplot(gs[idx, 1])\n",
    "    colors_rgb = ['#E74C3C', '#27AE60', '#3498DB']  # Red, Green, Blue\n",
    "    labels_rgb = ['Red', 'Green', 'Blue']\n",
    "    \n",
    "    for c, color, label in zip(range(3), colors_rgb, labels_rgb):\n",
    "        hist = cv2.calcHist([img_rgb], [c], None, [256], [0, 256])\n",
    "        ax_hist.plot(hist, color=color, linewidth=2, label=label, alpha=0.7)\n",
    "    \n",
    "    ax_hist.set_xlabel('Pixel Value', fontsize=9)\n",
    "    ax_hist.set_ylabel('Frequency', fontsize=9)\n",
    "    ax_hist.set_title('RGB Distribution', fontsize=10, fontweight='bold')\n",
    "    ax_hist.legend(fontsize=8)\n",
    "    ax_hist.grid(True, alpha=0.3)\n",
    "    ax_hist.set_xlim([0, 256])\n",
    "    \n",
    "    # Green channel analysis\n",
    "    ax_green = fig.add_subplot(gs[idx, 2])\n",
    "    green_channel = img_rgb[:,:,1]\n",
    "    ax_green.imshow(green_channel, cmap='Greens')\n",
    "    ax_green.set_title('Green Channel', fontsize=10, fontweight='bold')\n",
    "    ax_green.axis('off')\n",
    "    \n",
    "    # Vegetation index\n",
    "    ax_veg = fig.add_subplot(gs[idx, 3])\n",
    "    ax_veg.imshow(green_ratio, cmap='RdYlGn', vmin=0, vmax=0.5)\n",
    "    ax_veg.set_title('Vegetation Index', fontsize=10, fontweight='bold')\n",
    "    ax_veg.axis('off')\n",
    "\n",
    "fig.suptitle('Detailed Sample Image Analysis: RGB, Green Channel, and Vegetation Index', \n",
    "             fontsize=16, fontweight='bold', y=0.995)\n",
    "\n",
    "plt.savefig(cfg.OUTPUT_PATH / 'professional_5_sample_images.png', dpi=300, bbox_inches='tight',\n",
    "            facecolor='white', edgecolor='none')\n",
    "print(\"✓ Saved: professional_5_sample_images.png\")\n",
    "plt.close()\n",
    "\n",
    "# ================================================================\n",
    "# 6. HIERARCHICAL RELATIONSHIP VALIDATION\n",
    "# ================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Creating Visualization 6: Hierarchical Relationships\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
    "\n",
    "# Green + Clover = GDM\n",
    "ax = axes[0]\n",
    "calculated = train_pivot['Dry_Green_g'] + train_pivot['Dry_Clover_g']\n",
    "actual = train_pivot['GDM_g']\n",
    "\n",
    "ax.scatter(calculated, actual, alpha=0.5, s=40, color=COLORS['primary'], \n",
    "          edgecolors='white', linewidth=0.5)\n",
    "\n",
    "# Perfect match line\n",
    "max_val = max(calculated.max(), actual.max())\n",
    "ax.plot([0, max_val], [0, max_val], color=COLORS['highlight'], \n",
    "       linewidth=3, linestyle='--', label='Perfect Match (y=x)')\n",
    "\n",
    "# Regression line\n",
    "z = np.polyfit(calculated, actual, 1)\n",
    "p = np.poly1d(z)\n",
    "x_line = np.linspace(calculated.min(), calculated.max(), 100)\n",
    "ax.plot(x_line, p(x_line), color=COLORS['secondary'], \n",
    "       linewidth=2.5, label=f'Regression: y={z[0]:.3f}x+{z[1]:.2f}')\n",
    "\n",
    "# Statistics\n",
    "r, p_val = pearsonr(calculated, actual)\n",
    "rmse = np.sqrt(((calculated - actual) ** 2).mean())\n",
    "\n",
    "stats_text = f'Pearson r: {r:.4f}\\np-value: {p_val:.2e}\\nRMSE: {rmse:.2f}g\\nn: {len(calculated):,}'\n",
    "ax.text(0.05, 0.95, stats_text, transform=ax.transAxes,\n",
    "       verticalalignment='top', fontsize=11, fontweight='bold',\n",
    "       bbox=dict(boxstyle='round', facecolor='white', alpha=0.95, \n",
    "                edgecolor=COLORS['neutral'], linewidth=2))\n",
    "\n",
    "ax.set_xlabel('Green + Clover (calculated, grams)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('GDM (actual, grams)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Hierarchical Validation: Green + Clover = GDM', \n",
    "            fontsize=13, fontweight='bold', pad=15)\n",
    "ax.legend(loc='lower right', fontsize=11, framealpha=0.95)\n",
    "ax.grid(True, alpha=0.3, linestyle='--')\n",
    "\n",
    "# GDM + Dead = Total\n",
    "ax = axes[1]\n",
    "calculated = train_pivot['GDM_g'] + train_pivot['Dry_Dead_g']\n",
    "actual = train_pivot['Dry_Total_g']\n",
    "\n",
    "ax.scatter(calculated, actual, alpha=0.5, s=40, color=COLORS['primary'], \n",
    "          edgecolors='white', linewidth=0.5)\n",
    "\n",
    "max_val = max(calculated.max(), actual.max())\n",
    "ax.plot([0, max_val], [0, max_val], color=COLORS['highlight'], \n",
    "       linewidth=3, linestyle='--', label='Perfect Match (y=x)')\n",
    "\n",
    "z = np.polyfit(calculated, actual, 1)\n",
    "p = np.poly1d(z)\n",
    "x_line = np.linspace(calculated.min(), calculated.max(), 100)\n",
    "ax.plot(x_line, p(x_line), color=COLORS['secondary'], \n",
    "       linewidth=2.5, label=f'Regression: y={z[0]:.3f}x+{z[1]:.2f}')\n",
    "\n",
    "r, p_val = pearsonr(calculated, actual)\n",
    "rmse = np.sqrt(((calculated - actual) ** 2).mean())\n",
    "\n",
    "stats_text = f'Pearson r: {r:.4f}\\np-value: {p_val:.2e}\\nRMSE: {rmse:.2f}g\\nn: {len(calculated):,}'\n",
    "ax.text(0.05, 0.95, stats_text, transform=ax.transAxes,\n",
    "       verticalalignment='top', fontsize=11, fontweight='bold',\n",
    "       bbox=dict(boxstyle='round', facecolor='white', alpha=0.95, \n",
    "                edgecolor=COLORS['neutral'], linewidth=2))\n",
    "\n",
    "ax.set_xlabel('GDM + Dead (calculated, grams)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Total Biomass (actual, grams)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Hierarchical Validation: GDM + Dead = Total', \n",
    "            fontsize=13, fontweight='bold', pad=15)\n",
    "ax.legend(loc='lower right', fontsize=11, framealpha=0.95)\n",
    "ax.grid(True, alpha=0.3, linestyle='--')\n",
    "\n",
    "fig.suptitle('Hierarchical Relationship Validation with Statistical Analysis', \n",
    "             fontsize=15, fontweight='bold', y=1.00)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(cfg.OUTPUT_PATH / 'professional_6_hierarchical.png', dpi=300, bbox_inches='tight',\n",
    "            facecolor='white', edgecolor='none')\n",
    "print(\"✓ Saved: professional_6_hierarchical.png\")\n",
    "plt.close()\n",
    "\n",
    "# ================================================================\n",
    "# SUMMARY\n",
    "# ================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EDA COMPLETE - ALL VISUALIZATIONS SAVED\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nGenerated Professional Visualizations:\")\n",
    "print(\"  1. professional_1_distributions.png - Target distributions with bell curves\")\n",
    "print(\"  2. professional_2_correlation_matrix.png - Complete correlation analysis\")\n",
    "print(\"  3. professional_3_boxplots.png - Detailed box plots with statistics\")\n",
    "print(\"  4. professional_4_scatter_matrix.png - Scatter plot matrix with regression\")\n",
    "print(\"  5. professional_5_sample_images.png - Detailed image analysis\")\n",
    "print(\"  6. professional_6_hierarchical.png - Hierarchical relationship validation\")\n",
    "print(\"\\nAll visualizations use consistent professional color scheme\")\n",
    "print(\"Statistical details and annotations included on all plots\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5799404c",
   "metadata": {
    "_cell_guid": "a181f50e-d2db-476c-812c-be6a36866fdd",
    "_uuid": "ea88a7f0-413c-4af6-97d4-9c2453b145ab",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-01-10T15:16:05.231464Z",
     "iopub.status.busy": "2026-01-10T15:16:05.231206Z",
     "iopub.status.idle": "2026-01-10T15:18:32.159318Z",
     "shell.execute_reply": "2026-01-10T15:18:32.158366Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 146.941161,
     "end_time": "2026-01-10T15:18:32.160823",
     "exception": false,
     "start_time": "2026-01-10T15:16:05.219662",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-10 15:16:29.375926: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1768058189.563517      24 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1768058189.616597      24 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1768058190.090284      24 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768058190.090313      24 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768058190.090316      24 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768058190.090319      24 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TIME] Started execution | Elapsed: 0.00h | Remaining: 8.50h\n",
      "Created fallback submission from sample\n",
      "\n",
      "================================================================================\n",
      "PART 1: SigLIP/Ensemble Model\n",
      "================================================================================\n",
      "Loaded pre-computed training embeddings\n",
      "[TIME] Loaded training data | Elapsed: 0.00h | Remaining: 8.50h\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b53d93ec3b74158b709f49ad1170954",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing embeddings:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TIME] Computed test embeddings | Elapsed: 0.01h | Remaining: 8.49h\n",
      "[TIME] Generated semantic features | Elapsed: 0.01h | Remaining: 8.49h\n",
      "\n",
      "Running LightGBM...\n",
      "Full CV Score: 0.669802\n",
      "Raw CV Score: 0.669802\n",
      "Processed CV Score: 0.679134\n",
      "Improvement: -0.009332\n",
      "[TIME] Completed LightGBM | Elapsed: 0.01h | Remaining: 8.49h\n",
      "\n",
      "Running CatBoost...\n",
      "Full CV Score: 0.654876\n",
      "Raw CV Score: 0.654876\n",
      "Processed CV Score: 0.669611\n",
      "Improvement: -0.014735\n",
      "[TIME] Completed CatBoost | Elapsed: 0.01h | Remaining: 8.49h\n",
      "✓ SigLIP/Ensemble submission created successfully\n",
      "[TIME] Completed SigLIP ensemble | Elapsed: 0.01h | Remaining: 8.49h\n",
      "\n",
      "================================================================================\n",
      "PART 2: DINO Model 1 (CrossPVT)\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e191e032f35e46e0822d32bd0a04147e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Predicting:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23e4dfa464f64e1dbd74471a91662350",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Predicting:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ DINO Model 1 submission created successfully\n",
      "[TIME] Completed DINO Model 1 | Elapsed: 0.02h | Remaining: 8.48h\n",
      "\n",
      "================================================================================\n",
      "PART 3: DINO Model 2 (MVP)\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43a4623716794320a995760bc61298eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Infer MVP:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c87bcd3092ab463daf2e96cbbb7387d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Infer MVP:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "670175fae3384871a2572ff4191d9628",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Infer MVP:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20b822f8c1cc45b6971b541a26a572d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Infer MVP:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ DINO Model 2 submission created successfully\n",
      "[TIME] Completed DINO Model 2 | Elapsed: 0.03h | Remaining: 8.47h\n",
      "\n",
      "================================================================================\n",
      "PART 4: Creating Final Ensemble\n",
      "================================================================================\n",
      "✓ Final ensemble submission created successfully\n",
      "[TIME] Completed final ensemble | Elapsed: 0.03h | Remaining: 8.47h\n",
      "[TIME] Execution completed | Elapsed: 0.03h | Remaining: 8.47h\n",
      "\n",
      "================================================================================\n",
      "EXECUTION COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import math\n",
    "import json\n",
    "import random\n",
    "import argparse\n",
    "import shutil\n",
    "import time\n",
    "from copy import deepcopy\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, List, Tuple, Dict\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import timm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import albumentations as A\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, BayesianRidge\n",
    "from sklearn.svm import SVR, LinearSVR\n",
    "from sklearn.ensemble import GradientBoostingRegressor, HistGradientBoostingRegressor, ExtraTreesRegressor\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "from transformers import AutoProcessor, AutoImageProcessor, AutoModel, AutoTokenizer\n",
    "\n",
    "# ============================================================================\n",
    "# TIMEOUT MANAGER\n",
    "# ============================================================================\n",
    "class TimeoutManager:\n",
    "    \"\"\"Manages execution time to prevent timeout\"\"\"\n",
    "    def __init__(self, max_time_seconds):\n",
    "        self.start_time = time.time()\n",
    "        self.max_time = max_time_seconds\n",
    "        \n",
    "    def time_elapsed(self):\n",
    "        return time.time() - self.start_time\n",
    "        \n",
    "    def time_remaining(self):\n",
    "        return self.max_time - self.time_elapsed()\n",
    "    \n",
    "    def should_continue(self, buffer_seconds=300):\n",
    "        \"\"\"Check if we have enough time to continue (with 5 min buffer)\"\"\"\n",
    "        return self.time_remaining() > buffer_seconds\n",
    "    \n",
    "    def log_time(self, message=\"\"):\n",
    "        elapsed = self.time_elapsed()\n",
    "        remaining = self.time_remaining()\n",
    "        print(f\"[TIME] {message} | Elapsed: {elapsed/3600:.2f}h | Remaining: {remaining/3600:.2f}h\")\n",
    "\n",
    "# ============================================================================\n",
    "# UTILITY FUNCTIONS\n",
    "# ============================================================================\n",
    "def seeding(SEED):\n",
    "    np.random.seed(SEED)\n",
    "    random.seed(SEED)\n",
    "    os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available(): \n",
    "        torch.cuda.manual_seed(SEED)\n",
    "        torch.cuda.manual_seed_all(SEED)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def flush():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    DATA_PATH: Path = Path(\"/kaggle/input/csiro-biomass/\")\n",
    "    TRAIN_DATA_PATH: Path = DATA_PATH/'train'\n",
    "    TEST_DATA_PATH: Path = DATA_PATH/'test'\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    seed = 42\n",
    "\n",
    "cfg = Config()\n",
    "seeding(cfg.seed)\n",
    "\n",
    "TARGET_NAMES = ['Dry_Clover_g', 'Dry_Dead_g', 'Dry_Green_g', 'Dry_Total_g', 'GDM_g']\n",
    "weights = {\n",
    "    'Dry_Green_g': 0.1,\n",
    "    'Dry_Dead_g': 0.1,\n",
    "    'Dry_Clover_g': 0.1,\n",
    "    'GDM_g': 0.2,\n",
    "    'Dry_Total_g': 0.5,\n",
    "}\n",
    "TARGET_MAX = {\n",
    "    \"Dry_Clover_g\": 71.7865,\n",
    "    \"Dry_Dead_g\": 83.8407,\n",
    "    \"Dry_Green_g\": 157.9836,\n",
    "    \"Dry_Total_g\": 185.70,\n",
    "    \"GDM_g\": 157.9836,\n",
    "}\n",
    "\n",
    "def competition_metric(y_true, y_pred) -> float:\n",
    "    y_weighted = 0\n",
    "    for l, label in enumerate(TARGET_NAMES):\n",
    "        y_weighted = y_weighted + y_true[:, l].mean() * weights[label]\n",
    "    ss_res = 0\n",
    "    ss_tot = 0\n",
    "    for l, label in enumerate(TARGET_NAMES):\n",
    "        ss_res = ss_res + ((y_true[:, l] - y_pred[:, l])**2).mean() * weights[label]\n",
    "        ss_tot = ss_tot + ((y_true[:, l] - y_weighted)**2).mean() * weights[label]\n",
    "    return 1 - ss_res / ss_tot\n",
    "\n",
    "def pivot_table(df: pd.DataFrame)->pd.DataFrame:\n",
    "    if 'target' in df.columns.tolist():\n",
    "        df_pt = pd.pivot_table(\n",
    "            df, \n",
    "            values='target', \n",
    "            index=['image_path', 'Sampling_Date', 'State', 'Species', 'Pre_GSHH_NDVI', 'Height_Ave_cm'], \n",
    "            columns='target_name', \n",
    "            aggfunc='mean'\n",
    "        ).reset_index()\n",
    "    else:\n",
    "        df['target'] = 0\n",
    "        df_pt = pd.pivot_table(\n",
    "            df, \n",
    "            values='target', \n",
    "            index='image_path', \n",
    "            columns='target_name', \n",
    "            aggfunc='mean'\n",
    "        ).reset_index()\n",
    "    return df_pt\n",
    "\n",
    "def melt_table(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    melted = df.melt(\n",
    "        id_vars='image_path',\n",
    "        value_vars=TARGET_NAMES,\n",
    "        var_name='target_name',\n",
    "        value_name='target'\n",
    "    )\n",
    "    melted['sample_id'] = (\n",
    "        melted['image_path']\n",
    "        .str.replace(r'^.*/', '', regex=True)\n",
    "        .str.replace('.jpg', '', regex=False)\n",
    "        + '__' + melted['target_name']\n",
    "    )\n",
    "    return melted[['sample_id', 'image_path', 'target_name', 'target']]\n",
    "\n",
    "def post_process_biomass(df_preds):\n",
    "    ordered_cols = [\"Dry_Green_g\", \"Dry_Clover_g\", \"Dry_Dead_g\", \"GDM_g\", \"Dry_Total_g\"]\n",
    "    Y = df_preds[ordered_cols].values.T\n",
    "    C = np.array([[1, 1, 0, -1,  0], [0, 0, 1,  1, -1]])\n",
    "    C_T = C.T\n",
    "    inv_CCt = np.linalg.inv(C @ C_T)\n",
    "    P = np.eye(5) - C_T @ inv_CCt @ C\n",
    "    Y_reconciled = P @ Y\n",
    "    Y_reconciled = Y_reconciled.T.clip(min=0)\n",
    "    df_out = df_preds.copy()\n",
    "    df_out[ordered_cols] = Y_reconciled\n",
    "    return df_out\n",
    "\n",
    "def split_image(image, patch_size=520, overlap=16):\n",
    "    h, w, c = image.shape\n",
    "    stride = patch_size - overlap\n",
    "    patches, coords = [], []\n",
    "    for y in range(0, h, stride):\n",
    "        for x in range(0, w, stride):\n",
    "            y1, x1, y2, x2 = y, x, y + patch_size, x + patch_size\n",
    "            patch = image[y1:y2, x1:x2, :]\n",
    "            if patch.shape[0] < patch_size or patch.shape[1] < patch_size:\n",
    "                pad_h = patch_size - patch.shape[0]\n",
    "                pad_w = patch_size - patch.shape[1]\n",
    "                patch = np.pad(patch, ((0,pad_h), (0,pad_w), (0,0)), mode='reflect')\n",
    "            patches.append(patch)\n",
    "            coords.append((y1, x1, y2, x2))\n",
    "    return patches, coords\n",
    "\n",
    "def get_model(model_path: str, device: str = 'cpu'):\n",
    "    model = AutoModel.from_pretrained(model_path, local_files_only=True)\n",
    "    processor = AutoImageProcessor.from_pretrained(model_path)\n",
    "    return model.eval().to(device), processor\n",
    "\n",
    "def compute_embeddings(model_path, df, patch_size=520):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model, processor = get_model(model_path=model_path, device=device)\n",
    "    IMAGE_PATHS, EMBEDDINGS = [], []\n",
    "    for i, row in tqdm(df.iterrows(), total=len(df), desc=\"Computing embeddings\"):\n",
    "        img_path = row['image_path']\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "        patches, coords = split_image(img, patch_size=patch_size)\n",
    "        images = [Image.fromarray(p).convert(\"RGB\") for p in patches]\n",
    "        inputs = processor(images=images, return_tensors=\"pt\").to(model.device)\n",
    "        with torch.no_grad():\n",
    "            if 'siglip' in model_path:\n",
    "                features = model.get_image_features(**inputs)\n",
    "            elif 'dino' in model_path:\n",
    "                features = model(**inputs).pooler_output\n",
    "            else:\n",
    "                raise Exception(\"Model should be dino or siglip\")\n",
    "        embeds = features.mean(dim=0).detach().cpu().numpy()\n",
    "        EMBEDDINGS.append(embeds)\n",
    "        IMAGE_PATHS.append(img_path)\n",
    "    embeddings = np.stack(EMBEDDINGS, axis=0)\n",
    "    n_features = embeddings.shape[1]\n",
    "    emb_columns = [f\"emb{i+1}\" for i in range(n_features)]\n",
    "    emb_df = pd.DataFrame(embeddings, columns=emb_columns)\n",
    "    emb_df['image_path'] = IMAGE_PATHS\n",
    "    df_final = df.merge(emb_df, on='image_path', how='left')\n",
    "    flush()\n",
    "    return df_final\n",
    "\n",
    "def generate_semantic_features(image_embeddings, model_path):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    try:\n",
    "        model = AutoModel.from_pretrained(model_path).to(device)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    except Exception as e:\n",
    "        return None\n",
    "    concept_groups = {\n",
    "        \"bare\": [\"bare soil\", \"dirt ground\", \"sparse vegetation\", \"exposed earth\"],\n",
    "        \"sparse\": [\"low density pasture\", \"thin grass\", \"short clipped grass\"],\n",
    "        \"medium\": [\"average pasture cover\", \"medium height grass\", \"grazed pasture\"],\n",
    "        \"dense\": [\"dense tall pasture\", \"thick grassy volume\", \"high biomass\", \"overgrown vegetation\"],\n",
    "        \"green\": [\"lush green vibrant pasture\", \"photosynthesizing leaves\", \"fresh growth\"],\n",
    "        \"dead\": [\"dry brown dead grass\", \"yellow straw\", \"senesced material\", \"standing hay\"],\n",
    "        \"clover\": [\"white clover\", \"trifolium repens\", \"broadleaf legume\", \"clover flowers\"],\n",
    "        \"grass\": [\"ryegrass\", \"blade-like leaves\", \"fescue\", \"grassy sward\"],\n",
    "        \"weeds\": [\"broadleaf weeds\", \"thistles\", \"non-pasture vegetation\"]\n",
    "    }\n",
    "    concept_vectors = {}\n",
    "    with torch.no_grad():\n",
    "        for name, prompts in concept_groups.items():\n",
    "            inputs = tokenizer(prompts, padding=\"max_length\", return_tensors=\"pt\").to(device)\n",
    "            emb = model.get_text_features(**inputs)\n",
    "            emb = emb / emb.norm(p=2, dim=-1, keepdim=True)\n",
    "            concept_vectors[name] = emb.mean(dim=0, keepdim=True)\n",
    "    if isinstance(image_embeddings, np.ndarray):\n",
    "        img_tensor = torch.tensor(image_embeddings, dtype=torch.float32).to(device)\n",
    "    else:\n",
    "        img_tensor = image_embeddings.to(device)\n",
    "    img_tensor = img_tensor / img_tensor.norm(p=2, dim=-1, keepdim=True)\n",
    "    scores = {}\n",
    "    for name, vec in concept_vectors.items():\n",
    "        scores[name] = torch.matmul(img_tensor, vec.T).cpu().numpy().flatten()\n",
    "    df_scores = pd.DataFrame(scores)\n",
    "    df_scores['ratio_greenness'] = df_scores['green'] / (df_scores['green'] + df_scores['dead'] + 1e-6)\n",
    "    df_scores['ratio_clover'] = df_scores['clover'] / (df_scores['clover'] + df_scores['grass'] + 1e-6)\n",
    "    df_scores['ratio_cover'] = (df_scores['dense'] + df_scores['medium']) / (df_scores['bare'] + df_scores['sparse'] + 1e-6)\n",
    "    df_scores['max_density'] = df_scores[['bare', 'sparse', 'medium', 'dense']].max(axis=1)\n",
    "    return df_scores.values\n",
    "\n",
    "class SupervisedEmbeddingEngine(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, n_pca=0.98, n_pls=8, n_gmm=5, random_state=42):\n",
    "        self.n_pca = n_pca\n",
    "        self.n_pls = n_pls\n",
    "        self.n_gmm = n_gmm\n",
    "        self.random_state = random_state\n",
    "        self.scaler = StandardScaler()\n",
    "        self.pca = PCA(n_components=n_pca, random_state=random_state)\n",
    "        self.pls = PLSRegression(n_components=n_pls, scale=False)\n",
    "        self.gmm = GaussianMixture(n_components=n_gmm, covariance_type='diag', random_state=random_state)\n",
    "        self.pls_fitted_ = False\n",
    "\n",
    "    def fit(self, X, y=None, X_semantic=None):\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        self.pca.fit(X_scaled)\n",
    "        self.gmm.fit(X_scaled)\n",
    "        if y is not None:\n",
    "            y_clean = y.values if hasattr(y, 'values') else y\n",
    "            self.pls.fit(X_scaled, y_clean)\n",
    "            self.pls_fitted_ = True\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, X_semantic=None):\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        return self._generate_features(X_scaled, X_semantic)\n",
    "\n",
    "    def _generate_features(self, X_scaled, X_semantic=None):\n",
    "        features = []\n",
    "        f_pca = self.pca.transform(X_scaled)\n",
    "        features.append(f_pca)\n",
    "        if self.pls_fitted_:\n",
    "            f_pls = self.pls.transform(X_scaled)\n",
    "            features.append(f_pls)\n",
    "        f_gmm = self.gmm.predict_proba(X_scaled)\n",
    "        features.append(f_gmm)\n",
    "        if X_semantic is not None:\n",
    "            sem_norm = (X_semantic - np.mean(X_semantic, axis=0)) / (np.std(X_semantic, axis=0) + 1e-6)\n",
    "            features.append(sem_norm)\n",
    "        return np.hstack(features)\n",
    "\n",
    "def compare_results(oof, train_data):\n",
    "    y_oof_df = pd.DataFrame(oof, columns=TARGET_NAMES)\n",
    "    raw_score = competition_metric(train_data[TARGET_NAMES].values, y_oof_df.values)\n",
    "    print(f\"Raw CV Score: {raw_score:.6f}\")\n",
    "    y_oof_proc = post_process_biomass(y_oof_df)\n",
    "    proc_score = competition_metric(train_data[TARGET_NAMES].values, y_oof_proc.values)\n",
    "    print(f\"Processed CV Score: {proc_score:.6f}\")\n",
    "    print(f\"Improvement: {raw_score - proc_score:.6f}\")\n",
    "\n",
    "def cross_validate(model, train_data, test_data, feature_engine, semantic_train=None, semantic_test=None, target_transform='max', seed=42):\n",
    "    n_splits = train_data['fold'].nunique()\n",
    "    target_max_arr = np.array([TARGET_MAX[t] for t in TARGET_NAMES], dtype=float)\n",
    "    y_true = train_data[TARGET_NAMES]\n",
    "    y_pred = pd.DataFrame(0.0, index=train_data.index, columns=TARGET_NAMES)\n",
    "    y_pred_test = np.zeros([len(test_data), len(TARGET_NAMES)], dtype=float)\n",
    "    \n",
    "    COLUMNS = [col for col in train_data.columns if col.startswith('emb')]\n",
    "    \n",
    "    for fold in range(n_splits):\n",
    "        seeding(seed*(seed//2 + fold))\n",
    "        train_mask = train_data['fold'] != fold\n",
    "        valid_mask = train_data['fold'] == fold\n",
    "        val_idx = train_data[valid_mask].index\n",
    "        X_train_raw = train_data[train_mask][COLUMNS].values\n",
    "        X_valid_raw = train_data[valid_mask][COLUMNS].values\n",
    "        X_test_raw = test_data[COLUMNS].values\n",
    "        sem_train_fold = semantic_train[train_mask] if semantic_train is not None else None\n",
    "        sem_valid_fold = semantic_train[valid_mask] if semantic_train is not None else None\n",
    "        y_train = train_data[train_mask][TARGET_NAMES].values\n",
    "        y_valid = train_data[valid_mask][TARGET_NAMES].values\n",
    "        if target_transform == 'log':\n",
    "            y_train_proc = np.log1p(y_train)\n",
    "        elif target_transform == 'max':\n",
    "            y_train_proc = y_train / target_max_arr\n",
    "        else:\n",
    "            y_train_proc = y_train\n",
    "        engine = deepcopy(feature_engine)\n",
    "        engine.fit(X_train_raw, y=y_train_proc, X_semantic=sem_train_fold)\n",
    "        x_train_eng = engine.transform(X_train_raw, X_semantic=sem_train_fold)\n",
    "        x_valid_eng = engine.transform(X_valid_raw, X_semantic=sem_valid_fold)\n",
    "        x_test_eng = engine.transform(X_test_raw, X_semantic=semantic_test)\n",
    "        fold_valid_pred = np.zeros_like(y_valid)\n",
    "        fold_test_pred = np.zeros([len(test_data), len(TARGET_NAMES)])\n",
    "        for k in range(len(TARGET_NAMES)):\n",
    "            regr = deepcopy(model)\n",
    "            regr.fit(x_train_eng, y_train_proc[:, k])\n",
    "            pred_valid_raw = regr.predict(x_valid_eng)\n",
    "            pred_test_raw = regr.predict(x_test_eng)\n",
    "            if target_transform == 'log':\n",
    "                pred_valid_inv = np.expm1(pred_valid_raw)\n",
    "                pred_test_inv = np.expm1(pred_test_raw)\n",
    "            elif target_transform == 'max':\n",
    "                pred_valid_inv = (pred_valid_raw * target_max_arr[k])\n",
    "                pred_test_inv = (pred_test_raw * target_max_arr[k])\n",
    "            else:\n",
    "                pred_valid_inv = pred_valid_raw\n",
    "                pred_test_inv = pred_test_raw\n",
    "            fold_valid_pred[:, k] = pred_valid_inv\n",
    "            fold_test_pred[:, k] = pred_test_inv\n",
    "        y_pred.loc[val_idx] = fold_valid_pred\n",
    "        y_pred_test += fold_test_pred / n_splits\n",
    "    full_cv = competition_metric(y_true.values, y_pred.values)\n",
    "    print(f\"Full CV Score: {full_cv:.6f}\")\n",
    "    return y_pred.values, y_pred_test\n",
    "\n",
    "# ============================================================================\n",
    "# DINO MODEL ARCHITECTURES\n",
    "# ============================================================================\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, mlp_ratio=4.0, dropout=0.0):\n",
    "        super().__init__()\n",
    "        hid = int(dim * mlp_ratio)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hid), nn.GELU(), nn.Dropout(dropout),\n",
    "            nn.Linear(hid, dim), nn.Dropout(dropout))\n",
    "\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, dim, heads=8, dropout=0.0, mlp_ratio=4.0):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = nn.MultiheadAttention(dim, heads, dropout=dropout, batch_first=True)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.ff = FeedForward(dim, mlp_ratio=mlp_ratio, dropout=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.norm1(x)\n",
    "        attn_out, _ = self.attn(h, h, h, need_weights=False)\n",
    "        x = x + attn_out\n",
    "        x = x + self.ff(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class MobileViTBlock(nn.Module):\n",
    "    def __init__(self, dim, heads=4, depth=2, patch=(2, 2), dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.local = nn.Sequential(\n",
    "            nn.Conv2d(dim, dim, 3, padding=1, groups=dim),\n",
    "            nn.Conv2d(dim, dim, 1), nn.GELU())\n",
    "        self.patch = patch\n",
    "        self.transformer = nn.ModuleList([AttentionBlock(dim, heads=heads, dropout=dropout, mlp_ratio=2.0) for _ in range(depth)])\n",
    "        self.fuse = nn.Conv2d(dim * 2, dim, kernel_size=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        local_feat = self.local(x)\n",
    "        B, C, H, W = local_feat.shape\n",
    "        ph, pw = self.patch\n",
    "        new_h = math.ceil(H / ph) * ph\n",
    "        new_w = math.ceil(W / pw) * pw\n",
    "        if new_h != H or new_w != W:\n",
    "            local_feat = F.interpolate(local_feat, size=(new_h, new_w), mode=\"bilinear\", align_corners=False)\n",
    "            H, W = new_h, new_w\n",
    "        tokens = local_feat.unfold(2, ph, ph).unfold(3, pw, pw)\n",
    "        tokens = tokens.contiguous().view(B, C, -1, ph, pw)\n",
    "        tokens = tokens.permute(0, 2, 3, 4, 1).reshape(B, -1, C)\n",
    "        for blk in self.transformer: tokens = blk(tokens)\n",
    "        feat = tokens.view(B, -1, ph * pw, C).permute(0, 3, 1, 2)\n",
    "        nh = H // ph\n",
    "        nw = W // pw\n",
    "        feat = feat.view(B, C, nh, nw, ph, pw).permute(0, 1, 2, 4, 3, 5)\n",
    "        feat = feat.reshape(B, C, H, W)\n",
    "        if feat.shape[-2:] != x.shape[-2:]:\n",
    "            feat = F.interpolate(feat, size=x.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "        return self.fuse(torch.cat([x, feat], dim=1))\n",
    "\n",
    "class SpatialReductionAttention(nn.Module):\n",
    "    def __init__(self, dim, heads=8, sr_ratio=2, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.scale = (dim // heads) ** -0.5\n",
    "        self.q = nn.Linear(dim, dim)\n",
    "        self.kv = nn.Linear(dim, dim * 2)\n",
    "        self.sr_ratio = sr_ratio\n",
    "        if sr_ratio > 1:\n",
    "            self.sr = nn.Conv2d(dim, dim, kernel_size=sr_ratio, stride=sr_ratio)\n",
    "            self.norm = nn.LayerNorm(dim)\n",
    "        else: self.sr = None\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, hw: Tuple[int, int]):\n",
    "        B, N, C = x.shape\n",
    "        q = self.q(x).reshape(B, N, self.heads, C // self.heads).permute(0, 2, 1, 3)\n",
    "        if self.sr is not None:\n",
    "            H, W = hw\n",
    "            feat = x.transpose(1, 2).reshape(B, C, H, W)\n",
    "            feat = self.sr(feat)\n",
    "            feat = feat.reshape(B, C, -1).transpose(1, 2)\n",
    "            feat = self.norm(feat)\n",
    "        else: feat = x\n",
    "        kv = self.kv(feat)\n",
    "        k, v = kv.chunk(2, dim=-1)\n",
    "        k = k.reshape(B, -1, self.heads, C // self.heads).permute(0, 2, 3, 1)\n",
    "        v = v.reshape(B, -1, self.heads, C // self.heads).permute(0, 2, 1, 3)\n",
    "        attn = torch.matmul(q, k) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.drop(attn)\n",
    "        out = torch.matmul(attn, v).permute(0, 2, 1, 3).reshape(B, N, C)\n",
    "        return self.proj(out)\n",
    "\n",
    "class PVTBlock(nn.Module):\n",
    "    def __init__(self, dim, heads=8, sr_ratio=2, dropout=0.0, mlp_ratio=4.0):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.sra = SpatialReductionAttention(dim, heads=heads, sr_ratio=sr_ratio, dropout=dropout)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.ff = FeedForward(dim, mlp_ratio=mlp_ratio, dropout=dropout)\n",
    "\n",
    "    def forward(self, x, hw: Tuple[int, int]):\n",
    "        x = x + self.sra(self.norm1(x), hw)\n",
    "        x = x + self.ff(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class LocalMambaBlock(nn.Module):\n",
    "    def __init__(self, dim, kernel_size=5, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.dwconv = nn.Conv1d(dim, dim, kernel_size=kernel_size, padding=kernel_size//2, groups=dim)\n",
    "        self.gate = nn.Linear(dim, dim)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        x = self.norm(x)\n",
    "        g = torch.sigmoid(self.gate(x))\n",
    "        x = (x * g).transpose(1, 2)\n",
    "        x = self.dwconv(x).transpose(1, 2)\n",
    "        x = self.proj(x)\n",
    "        x = self.drop(x)\n",
    "        return shortcut + x\n",
    "\n",
    "class T2TRetokenizer(nn.Module):\n",
    "    def __init__(self, dim, depth=2, heads=4, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList([AttentionBlock(dim, heads=heads, dropout=dropout, mlp_ratio=2.0) for _ in range(depth)])\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor, grid_hw: Tuple[int, int]):\n",
    "        B, T, C = tokens.shape\n",
    "        H, W = grid_hw\n",
    "        feat_map = tokens.transpose(1, 2).reshape(B, C, H, W)\n",
    "        seq = feat_map.flatten(2).transpose(1, 2)\n",
    "        for blk in self.blocks: seq = blk(seq)\n",
    "        seq_map = seq.transpose(1, 2).reshape(B, C, H, W)\n",
    "        pooled = F.adaptive_avg_pool2d(seq_map, (2, 2))\n",
    "        retokens = pooled.flatten(2).transpose(1, 2)\n",
    "        return retokens, seq_map\n",
    "\n",
    "class CrossScaleFusion(nn.Module):\n",
    "    def __init__(self, dim, heads=6, dropout=0.0, layers=2):\n",
    "        super().__init__()\n",
    "        self.layers_s = nn.ModuleList([AttentionBlock(dim, heads=heads, dropout=dropout, mlp_ratio=2.0) for _ in range(layers)])\n",
    "        self.layers_b = nn.ModuleList([AttentionBlock(dim, heads=heads, dropout=dropout, mlp_ratio=2.0) for _ in range(layers)])\n",
    "        self.cross_s = nn.ModuleList([nn.MultiheadAttention(dim, heads, dropout=dropout, batch_first=True, kdim=dim, vdim=dim) for _ in range(layers)])\n",
    "        self.cross_b = nn.ModuleList([nn.MultiheadAttention(dim, heads, dropout=dropout, batch_first=True, kdim=dim, vdim=dim) for _ in range(layers)])\n",
    "        self.norm_s = nn.LayerNorm(dim)\n",
    "        self.norm_b = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, tok_s: torch.Tensor, tok_b: torch.Tensor):\n",
    "        B, Ts, C = tok_s.shape\n",
    "        Tb = tok_b.shape[1]\n",
    "        cls_s = tok_s.new_zeros(B, 1, C)\n",
    "        cls_b = tok_b.new_zeros(B, 1, C)\n",
    "        tok_s = torch.cat([cls_s, tok_s], dim=1)\n",
    "        tok_b = torch.cat([cls_b, tok_b], dim=1)\n",
    "        for ls, lb, cs, cb in zip(self.layers_s, self.layers_b, self.cross_s, self.cross_b):\n",
    "            tok_s = ls(tok_s)\n",
    "            tok_b = lb(tok_b)\n",
    "            q_s = self.norm_s(tok_s[:, :1])\n",
    "            q_b = self.norm_b(tok_b[:, :1])\n",
    "            cls_s_upd, _ = cs(q_s, torch.cat([tok_b, q_b], dim=1), torch.cat([tok_b, q_b], dim=1), need_weights=False)\n",
    "            cls_b_upd, _ = cb(q_b, torch.cat([tok_s, q_s], dim=1), torch.cat([tok_s, q_s], dim=1), need_weights=False)\n",
    "            tok_s = torch.cat([tok_s[:, :1] + cls_s_upd, tok_s[:, 1:]], dim=1)\n",
    "            tok_b = torch.cat([tok_b[:, :1] + cls_b_upd, tok_b[:, 1:]], dim=1)\n",
    "        tokens = torch.cat([tok_s[:, :1], tok_b[:, :1], tok_s[:, 1:], tok_b[:, 1:]], dim=1)\n",
    "        return tokens\n",
    "\n",
    "class TileEncoder(nn.Module):\n",
    "    def __init__(self, backbone: nn.Module, input_res: int):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.input_res = input_res\n",
    "\n",
    "    def forward(self, x: torch.Tensor, grid: Tuple[int, int]):\n",
    "        B, C, H, W = x.shape\n",
    "        r, c = grid\n",
    "        hs = torch.linspace(0, H, steps=r + 1, device=x.device).round().long()\n",
    "        ws = torch.linspace(0, W, steps=c + 1, device=x.device).round().long()\n",
    "        tiles = []\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                rs, re = hs[i].item(), hs[i + 1].item()\n",
    "                cs, ce = ws[j].item(), ws[j + 1].item()\n",
    "                xt = x[:, :, rs:re, cs:ce]\n",
    "                if xt.shape[-2:] != (self.input_res, self.input_res):\n",
    "                    xt = F.interpolate(xt, size=(self.input_res, self.input_res), mode=\"bilinear\", align_corners=False)\n",
    "                tiles.append(xt)\n",
    "        tiles = torch.stack(tiles, dim=1)\n",
    "        flat = tiles.view(-1, C, self.input_res, self.input_res)\n",
    "        feats = self.backbone(flat)\n",
    "        return feats.view(B, -1, feats.shape[-1])\n",
    "\n",
    "class PyramidMixer(nn.Module):\n",
    "    def __init__(self, dim_in: int, dims: Tuple[int, int, int], mobilevit_heads=4, mobilevit_depth=2, sra_heads=6, sra_ratio=2, mamba_depth=3, mamba_kernel=5, dropout=0.0):\n",
    "        super().__init__()\n",
    "        c1, c2, c3 = dims\n",
    "        self.proj1 = nn.Linear(dim_in, c1)\n",
    "        self.mobilevit = MobileViTBlock(c1, heads=mobilevit_heads, depth=mobilevit_depth, dropout=dropout)\n",
    "        self.proj2 = nn.Linear(c1, c2)\n",
    "        self.pvt = PVTBlock(c2, heads=sra_heads, sr_ratio=sra_ratio, dropout=dropout, mlp_ratio=3.0)\n",
    "        self.mamba_local = LocalMambaBlock(c2, kernel_size=mamba_kernel, dropout=dropout)\n",
    "        self.proj3 = nn.Linear(c2, c3)\n",
    "        self.mamba_global = nn.ModuleList([LocalMambaBlock(c3, kernel_size=mamba_kernel, dropout=dropout) for _ in range(mamba_depth)])\n",
    "        self.final_attn = AttentionBlock(c3, heads=min(8, c3//64+1), dropout=dropout, mlp_ratio=2.0)\n",
    "\n",
    "    def _tokens_to_map(self, tokens: torch.Tensor, target_hw: Tuple[int, int]):\n",
    "        B, N, C = tokens.shape\n",
    "        H, W = target_hw\n",
    "        need = H * W\n",
    "        if N < need:\n",
    "            pad = tokens.new_zeros(B, need-N, C)\n",
    "            tokens = torch.cat([tokens, pad], dim=1)\n",
    "        tokens = tokens[:, :need, :]\n",
    "        return tokens.transpose(1, 2).reshape(B, C, H, W)\n",
    "\n",
    "    @staticmethod\n",
    "    def _fit_hw(n_tokens: int) -> Tuple[int, int]:\n",
    "        h = int(math.sqrt(n_tokens))\n",
    "        w = h\n",
    "        while h * w < n_tokens:\n",
    "            w += 1\n",
    "            if h * w < n_tokens: h += 1\n",
    "        return h, w\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor):\n",
    "        B, N, C = tokens.shape\n",
    "        map_hw = (3, 4)\n",
    "        feat_map = self._tokens_to_map(tokens, map_hw)\n",
    "        t1 = self.proj1(tokens)\n",
    "        m1 = self._tokens_to_map(t1, map_hw)\n",
    "        m1 = self.mobilevit(m1)\n",
    "        t1_out = m1.flatten(2).transpose(1, 2)[:, :N]\n",
    "        t2 = self.proj2(t1_out)\n",
    "        new_len = max(4, N//2)\n",
    "        t2 = t2[:, :new_len] + F.adaptive_avg_pool1d(t2.transpose(1, 2), new_len).transpose(1, 2)\n",
    "        hw2 = self._fit_hw(t2.size(1))\n",
    "        if t2.size(1) < hw2[0] * hw2[1]:\n",
    "            pad = t2.new_zeros(B, hw2[0]*hw2[1]-t2.size(1), t2.size(2))\n",
    "            t2 = torch.cat([t2, pad], dim=1)\n",
    "        t2 = self.pvt(t2, hw2)\n",
    "        t2 = self.mamba_local(t2)\n",
    "        t3 = self.proj3(t2)\n",
    "        pooled = torch.stack([t3.mean(dim=1), t3.max(dim=1).values], dim=1)\n",
    "        t3 = pooled\n",
    "        for blk in self.mamba_global: t3 = blk(t3)\n",
    "        t3 = self.final_attn(t3)\n",
    "        return t3.mean(dim=1), {\"stage1_map\": m1.detach(), \"stage2_tokens\": t2.detach(), \"stage3_tokens\": t3.detach()}\n",
    "\n",
    "@dataclass\n",
    "class TrainCFG:\n",
    "    dropout: float = 0.1\n",
    "    hidden_ratio: float = 0.35\n",
    "    dino_candidates: Tuple[str, ...] = (\"vit_base_patch14_dinov2\", \"vit_base_patch14_reg4_dinov2\", \"vit_small_patch14_dinov2\")\n",
    "    small_grid: Tuple[int, int] = (4, 4)\n",
    "    big_grid: Tuple[int, int] = (2, 2)\n",
    "    t2t_depth: int = 2\n",
    "    cross_layers: int = 2\n",
    "    cross_heads: int = 6\n",
    "    pyramid_dims: Tuple[int, int, int] = (384, 512, 640)\n",
    "    mobilevit_heads: int = 4\n",
    "    mobilevit_depth: int = 2\n",
    "    sra_heads: int = 8\n",
    "    sra_ratio: int = 2\n",
    "    mamba_depth: int = 3\n",
    "    mamba_kernel: int = 5\n",
    "    aux_head: bool = True\n",
    "    aux_loss_weight: float = 0.4\n",
    "    ALL_TARGET_COLS: Tuple[str, ...] = (\"Dry_Green_g\", \"Dry_Dead_g\", \"Dry_Clover_g\", \"GDM_g\", \"Dry_Total_g\")\n",
    "\n",
    "CFG = TrainCFG()\n",
    "\n",
    "def update_cfg_from_checkpoint(cfg_dict: dict):\n",
    "    global CFG\n",
    "    if not cfg_dict: return\n",
    "    for k, v in cfg_dict.items():\n",
    "        if hasattr(CFG, k): setattr(CFG, k, v)\n",
    "\n",
    "class CrossPVT_T2T_MambaDINO(nn.Module):\n",
    "    def __init__(self, dropout: float = 0.1, hidden_ratio: float = 0.35):\n",
    "        super().__init__()\n",
    "        self.backbone, self.feat_dim, self.backbone_name, self.input_res = self._build_dino_backbone()\n",
    "        self.tile_encoder = TileEncoder(self.backbone, self.input_res)\n",
    "        self.t2t = T2TRetokenizer(self.feat_dim, depth=CFG.t2t_depth, heads=CFG.cross_heads, dropout=dropout)\n",
    "        self.cross = CrossScaleFusion(self.feat_dim, heads=CFG.cross_heads, dropout=dropout, layers=CFG.cross_layers)\n",
    "        self.pyramid = PyramidMixer(dim_in=self.feat_dim, dims=CFG.pyramid_dims, mobilevit_heads=CFG.mobilevit_heads, mobilevit_depth=CFG.mobilevit_depth, sra_heads=CFG.sra_heads, sra_ratio=CFG.sra_ratio, mamba_depth=CFG.mamba_depth, mamba_kernel=CFG.mamba_kernel, dropout=dropout)\n",
    "        self.combined_dim = CFG.pyramid_dims[-1] * 2\n",
    "        hidden = max(32, int(self.combined_dim * hidden_ratio))\n",
    "        def head(): return nn.Sequential(nn.Linear(self.combined_dim, hidden), nn.GELU(), nn.Dropout(dropout), nn.Linear(hidden, 1))\n",
    "        self.head_green = head()\n",
    "        self.head_clover = head()\n",
    "        self.head_dead = head()\n",
    "        self.score_head = nn.Sequential(nn.LayerNorm(self.combined_dim), nn.Linear(self.combined_dim, 1))\n",
    "        self.aux_head = nn.Sequential(nn.LayerNorm(CFG.pyramid_dims[1]), nn.Linear(CFG.pyramid_dims[1], 5)) if CFG.aux_head else None\n",
    "        self.softplus = nn.Softplus(beta=1.0)\n",
    "        self.cross_gate_left = nn.Linear(CFG.pyramid_dims[-1], CFG.pyramid_dims[-1])\n",
    "        self.cross_gate_right = nn.Linear(CFG.pyramid_dims[-1], CFG.pyramid_dims[-1])\n",
    "\n",
    "    def _build_dino_backbone(self):\n",
    "        last_err = None\n",
    "        for name in CFG.dino_candidates:\n",
    "            for gp in [\"token\", \"avg\", \"__default__\"]:\n",
    "                try:\n",
    "                    if gp == \"__default__\":\n",
    "                        m = timm.create_model(name, pretrained=False, num_classes=0)\n",
    "                        gp_str = \"default\"\n",
    "                    else:\n",
    "                        m = timm.create_model(name, pretrained=False, num_classes=0, global_pool=gp)\n",
    "                        gp_str = gp\n",
    "                    feat = m.num_features\n",
    "                    input_res = self._infer_input_res(m)\n",
    "                    if hasattr(m, \"set_grad_checkpointing\"):\n",
    "                        m.set_grad_checkpointing(True)\n",
    "                    return m, feat, name, int(input_res)\n",
    "                except Exception as e: last_err = e; continue\n",
    "        raise RuntimeError(f\"Cannot create any DINO backbone. Last error: {last_err}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _infer_input_res(m) -> int:\n",
    "        if hasattr(m, \"patch_embed\") and hasattr(m.patch_embed, \"img_size\"):\n",
    "            isz = m.patch_embed.img_size\n",
    "            return int(isz if isinstance(isz, (int, float)) else isz[0])\n",
    "        if hasattr(m, \"img_size\"):\n",
    "            isz = m.img_size\n",
    "            return int(isz if isinstance(isz, (int, float)) else isz[0])\n",
    "        dc = getattr(m, \"default_cfg\", {}) or {}\n",
    "        ins = dc.get(\"input_size\", None)\n",
    "        if ins:\n",
    "            if isinstance(ins, (tuple, list)) and len(ins) >= 2:\n",
    "                return int(ins[1])\n",
    "            return int(ins if isinstance(ins, (int, float)) else 224)\n",
    "        return 518\n",
    "\n",
    "    def _half_forward(self, x_half: torch.Tensor):\n",
    "        tiles_small = self.tile_encoder(x_half, CFG.small_grid)\n",
    "        tiles_big = self.tile_encoder(x_half, CFG.big_grid)\n",
    "        t2, stage1_map = self.t2t(tiles_small, CFG.small_grid)\n",
    "        fused = self.cross(t2, tiles_big)\n",
    "        feat, feat_maps = self.pyramid(fused)\n",
    "        feat_maps[\"stage1_map\"] = stage1_map\n",
    "        return feat, feat_maps\n",
    "\n",
    "    def _merge_heads(self, f_l: torch.Tensor, f_r: torch.Tensor):\n",
    "        g_l = torch.sigmoid(self.cross_gate_left(f_r))\n",
    "        g_r = torch.sigmoid(self.cross_gate_right(f_l))\n",
    "        f_l = f_l * g_l\n",
    "        f_r = f_r * g_r\n",
    "        f = torch.cat([f_l, f_r], dim=1)\n",
    "        green_pos = self.softplus(self.head_green(f))\n",
    "        clover_pos = self.softplus(self.head_clover(f))\n",
    "        dead_pos = self.softplus(self.head_dead(f))\n",
    "        gdm = green_pos + clover_pos\n",
    "        total = gdm + dead_pos\n",
    "        return total, gdm, green_pos, f\n",
    "\n",
    "    def forward(self, *inputs, x_left=None, x_right=None, return_features: bool = False):\n",
    "        if inputs:\n",
    "            if len(inputs) == 1:\n",
    "                first = inputs[0]\n",
    "                if isinstance(first, (tuple, list)):\n",
    "                    if len(first) >= 1: x_left = first[0]\n",
    "                    if len(first) >= 2: x_right = first[1]\n",
    "                else: x_left = first\n",
    "            else: x_left = inputs[0]; x_right = inputs[1]\n",
    "        if x_left is None or (isinstance(x_left, torch.Tensor) and x_left.shape[0] == 0):\n",
    "            device = next(self.parameters()).device\n",
    "            dtype = next(self.parameters()).dtype\n",
    "            zero = torch.zeros(0, 1, device=device, dtype=dtype)\n",
    "            out = {\"total\": zero, \"gdm\": zero, \"green\": zero, \"score_feat\": torch.zeros(0, self.combined_dim, device=device, dtype=dtype)}\n",
    "            if self.aux_head is not None:\n",
    "                out[\"aux\"] = torch.zeros(0, len(CFG.ALL_TARGET_COLS), device=device, dtype=dtype)\n",
    "            if return_features: out[\"feature_maps\"] = {}\n",
    "            return out\n",
    "        if x_right is None:\n",
    "            if isinstance(x_left, torch.Tensor) and x_left.shape[1] % 2 == 0:\n",
    "                x_left, x_right = torch.chunk(x_left, 2, dim=1)\n",
    "            else: raise ValueError(\"Missing x_right input.\")\n",
    "        feat_l, feats_l = self._half_forward(x_left)\n",
    "        feat_r, feats_r = self._half_forward(x_right)\n",
    "        total, gdm, green, f_concat = self._merge_heads(feat_l, feat_r)\n",
    "        out = {\"total\": total, \"gdm\": gdm, \"green\": green, \"score_feat\": f_concat}\n",
    "        if self.aux_head is not None:\n",
    "            aux_tokens = torch.cat([feats_l[\"stage2_tokens\"], feats_r[\"stage2_tokens\"]], dim=1)\n",
    "            aux_pred = self.softplus(self.aux_head(aux_tokens.mean(dim=1)))\n",
    "            out[\"aux\"] = aux_pred\n",
    "        if return_features:\n",
    "            out[\"feature_maps\"] = {\"stage1_left\": feats_l.get(\"stage1_map\"), \"stage1_right\": feats_r.get(\"stage1_map\"), \"stage3_left\": feats_l.get(\"stage3_tokens\"), \"stage3_right\": feats_r.get(\"stage3_tokens\")}\n",
    "        return out\n",
    "\n",
    "class INF_CFG:\n",
    "    BASE_PATH = \"/kaggle/input/csiro-biomass\"\n",
    "    TEST_CSV = os.path.join(BASE_PATH, \"test.csv\")\n",
    "    TEST_IMAGE_DIR = os.path.join(BASE_PATH, \"test\")\n",
    "    EXPERIMENT_DIR = \"/kaggle/input/csiro/pytorch/default/12\"\n",
    "    CKPT_PATTERN_FOLD_X = os.path.join(EXPERIMENT_DIR, \"fold_{fold}\", \"checkpoints\", \"best_wr2.pt\")\n",
    "    CKPT_PATTERN_FOLDX = os.path.join(EXPERIMENT_DIR, \"fold{fold}\", \"checkpoints\", \"best_wr2.pt\")\n",
    "    N_FOLDS = 3  # Reduced from 5 to 3 for speed\n",
    "    SUBMISSION_FILE = \"submission3.csv\"\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    BATCH_SIZE = 4  # Increased from 1 to 4\n",
    "    NUM_WORKERS = 0\n",
    "    MIXED_PRECISION = True\n",
    "    USE_TTA = True\n",
    "    TTA_TRANSFORMS = [\"original\", \"hflip\"]  # Reduced from 3 to 2\n",
    "    ALL_TARGET_COLS = [\"Dry_Green_g\", \"Dry_Dead_g\", \"Dry_Clover_g\", \"GDM_g\", \"Dry_Total_g\"]\n",
    "\n",
    "class TestBiomassDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, transform, image_dir: str):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "        self.image_dir = image_dir\n",
    "        self.paths = self.df[\"image_path\"].values\n",
    "\n",
    "    def __len__(self): return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename = os.path.basename(self.paths[idx])\n",
    "        full_path = os.path.join(self.image_dir, filename)\n",
    "        img = cv2.imread(full_path)\n",
    "        if img is None: img = np.zeros((1000, 2000, 3), dtype=np.uint8)\n",
    "        else: img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        h, w, _ = img.shape\n",
    "        mid = w // 2\n",
    "        left = img[:, :mid]\n",
    "        right = img[:, mid:]\n",
    "        left_t = self.transform(image=left)[\"image\"]\n",
    "        right_t = self.transform(image=right)[\"image\"]\n",
    "        return left_t, right_t\n",
    "\n",
    "def get_tta_transforms(img_size: int) -> List[A.Compose]:\n",
    "    base = [A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), ToTensorV2()]\n",
    "    transforms = []\n",
    "    transforms.append(A.Compose([A.Resize(img_size, img_size, interpolation=cv2.INTER_AREA), *base]))\n",
    "    transforms.append(A.Compose([A.HorizontalFlip(p=1.0), A.Resize(img_size, img_size, interpolation=cv2.INTER_AREA), *base]))\n",
    "    # Removed vertical flip for speed\n",
    "    return transforms\n",
    "\n",
    "def strip_module_prefix(state_dict: dict) -> dict:\n",
    "    if not state_dict: return state_dict\n",
    "    keys = list(state_dict.keys())\n",
    "    if all(k.startswith(\"module.\") for k in keys):\n",
    "        return {k[len(\"module.\"):]: v for k, v in state_dict.items()}\n",
    "    return state_dict\n",
    "\n",
    "def load_checkpoint(path: str) -> dict:\n",
    "    if not os.path.exists(path): raise FileNotFoundError(f\"Checkpoint not found: {path}\")\n",
    "    try: state = torch.load(path, map_location=\"cpu\", weights_only=False)\n",
    "    except TypeError: state = torch.load(path, map_location=\"cpu\")\n",
    "    return state\n",
    "\n",
    "def load_model_from_checkpoint(ckpt_path: str) -> nn.Module:\n",
    "    state = load_checkpoint(ckpt_path)\n",
    "    cfg_dict = state.get(\"cfg\", {})\n",
    "    update_cfg_from_checkpoint(cfg_dict)\n",
    "    dropout = cfg_dict.get(\"dropout\", CFG.dropout)\n",
    "    hidden_ratio = cfg_dict.get(\"hidden_ratio\", CFG.hidden_ratio)\n",
    "    model = CrossPVT_T2T_MambaDINO(dropout=dropout, hidden_ratio=hidden_ratio)\n",
    "    model_state = state.get(\"model_state\", state)\n",
    "    model_state = strip_module_prefix(model_state)\n",
    "    missing_keys, unexpected_keys = model.load_state_dict(model_state, strict=False)\n",
    "    model.to(INF_CFG.DEVICE)\n",
    "    model.eval()\n",
    "    input_res = getattr(model, \"input_res\", 518)\n",
    "    return model\n",
    "\n",
    "def pack5_targets(total: torch.Tensor, gdm: torch.Tensor, green: torch.Tensor) -> torch.Tensor:\n",
    "    clover = gdm - green\n",
    "    dead = total - gdm\n",
    "    return torch.cat([green, dead, clover, gdm, total], dim=1)\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_one_view(models: List[nn.Module], loader: DataLoader) -> np.ndarray:\n",
    "    preds_list = []\n",
    "    amp_dtype = \"cuda\" if INF_CFG.DEVICE.type == \"cuda\" else \"cpu\"\n",
    "    for xl, xr in tqdm(loader, desc=\"  Predicting\", leave=False):\n",
    "        xl = xl.to(INF_CFG.DEVICE, non_blocking=True)\n",
    "        xr = xr.to(INF_CFG.DEVICE, non_blocking=True)\n",
    "        x_cat = torch.cat([xl, xr], dim=1)\n",
    "        per_model_preds = []\n",
    "        with torch.amp.autocast(amp_dtype, enabled=INF_CFG.MIXED_PRECISION):\n",
    "            for model in models:\n",
    "                out = model(x_cat, return_features=False)\n",
    "                total = out[\"total\"]\n",
    "                gdm = out[\"gdm\"]\n",
    "                green = out[\"green\"]\n",
    "                five = pack5_targets(total, gdm, green)\n",
    "                five = torch.clamp(five, min=0.0)\n",
    "                per_model_preds.append(five.float().cpu())\n",
    "        stacked = torch.mean(torch.stack(per_model_preds, dim=0), dim=0)\n",
    "        preds_list.append(stacked.numpy())\n",
    "    return np.concatenate(preds_list, axis=0)\n",
    "\n",
    "def run_inference(test_df: pd.DataFrame, image_dir: str) -> np.ndarray:\n",
    "    models = []\n",
    "    input_res = None\n",
    "    for fold in range(INF_CFG.N_FOLDS):\n",
    "        ckpt_path = INF_CFG.CKPT_PATTERN_FOLD_X.format(fold=fold)\n",
    "        if not os.path.exists(ckpt_path):\n",
    "            ckpt_path = INF_CFG.CKPT_PATTERN_FOLDX.format(fold=fold)\n",
    "        if not os.path.exists(ckpt_path): continue\n",
    "        model = load_model_from_checkpoint(ckpt_path)\n",
    "        models.append(model)\n",
    "        if input_res is None: input_res = getattr(model, \"input_res\", 518)\n",
    "    if len(models) == 0:\n",
    "        raise RuntimeError(\"No checkpoints found!\")\n",
    "    if INF_CFG.USE_TTA:\n",
    "        tta_transforms = get_tta_transforms(input_res)\n",
    "        per_view_preds = []\n",
    "        for transform in tta_transforms:\n",
    "            ds = TestBiomassDataset(test_df, transform, image_dir)\n",
    "            dl = DataLoader(ds, batch_size=INF_CFG.BATCH_SIZE, shuffle=False, num_workers=INF_CFG.NUM_WORKERS, pin_memory=True)\n",
    "            view_pred = predict_one_view(models, dl)\n",
    "            per_view_preds.append(view_pred)\n",
    "        final_pred = np.mean(per_view_preds, axis=0)\n",
    "    else:\n",
    "        transform = get_tta_transforms(input_res)[0]\n",
    "        ds = TestBiomassDataset(test_df, transform, image_dir)\n",
    "        dl = DataLoader(ds, batch_size=INF_CFG.BATCH_SIZE, shuffle=False, num_workers=INF_CFG.NUM_WORKERS, pin_memory=True)\n",
    "        final_pred = predict_one_view(models, dl)\n",
    "    return final_pred\n",
    "\n",
    "def create_submission(final_pred: np.ndarray, test_long: pd.DataFrame, test_unique: pd.DataFrame) -> pd.DataFrame:\n",
    "    def clean(x):\n",
    "        x = np.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        return np.maximum(0, x)\n",
    "    green, dead, clover, gdm, total = map(clean, [final_pred[:,0], final_pred[:,1], final_pred[:,2], final_pred[:,3], final_pred[:,4]])\n",
    "    wide = pd.DataFrame({\"image_path\": test_unique[\"image_path\"], \"Dry_Green_g\": green, \"Dry_Dead_g\": dead, \"Dry_Clover_g\": clover, \"GDM_g\": gdm, \"Dry_Total_g\": total})\n",
    "    long_preds = wide.melt(id_vars=[\"image_path\"], value_vars=INF_CFG.ALL_TARGET_COLS, var_name=\"target_name\", value_name=\"target\")\n",
    "    sub = pd.merge(test_long[[\"sample_id\", \"image_path\", \"target_name\"]], long_preds, on=[\"image_path\", \"target_name\"], how=\"left\")[[\"sample_id\", \"target\"]]\n",
    "    sub[\"target\"] = np.nan_to_num(sub[\"target\"], nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    sub.to_csv(INF_CFG.SUBMISSION_FILE, index=False)\n",
    "    return sub\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"CSIRO v4 CrossPVT T2T Mamba Inference\")\n",
    "    parser.add_argument(\"--test-csv\", type=str, default=None)\n",
    "    parser.add_argument(\"--test-image-dir\", type=str, default=None)\n",
    "    parser.add_argument(\"--experiment-dir\", type=str, default=None)\n",
    "    parser.add_argument(\"--output\", type=str, default=None)\n",
    "    parser.add_argument(\"--batch-size\", type=int, default=None)\n",
    "    parser.add_argument(\"--no-tta\", action=\"store_true\")\n",
    "    args, _ = parser.parse_known_args()\n",
    "    return args\n",
    "\n",
    "def run_dino_inference():\n",
    "    args = parse_args()\n",
    "    if args.test_csv: INF_CFG.TEST_CSV = args.test_csv\n",
    "    if args.test_image_dir: INF_CFG.TEST_IMAGE_DIR = args.test_image_dir\n",
    "    if args.experiment_dir:\n",
    "        INF_CFG.EXPERIMENT_DIR = args.experiment_dir\n",
    "        INF_CFG.CKPT_PATTERN_FOLD_X = os.path.join(INF_CFG.EXPERIMENT_DIR, \"fold_{fold}\", \"checkpoints\", \"best_wr2.pt\")\n",
    "        INF_CFG.CKPT_PATTERN_FOLDX = os.path.join(INF_CFG.EXPERIMENT_DIR, \"fold{fold}\", \"checkpoints\", \"best_wr2.pt\")\n",
    "    if args.output: INF_CFG.SUBMISSION_FILE = args.output\n",
    "    if args.batch_size: INF_CFG.BATCH_SIZE = args.batch_size\n",
    "    if args.no_tta: INF_CFG.USE_TTA = False\n",
    "    test_long = pd.read_csv(INF_CFG.TEST_CSV)\n",
    "    test_unique = test_long.drop_duplicates(subset=[\"image_path\"]).reset_index(drop=True)\n",
    "    final_pred = run_inference(test_unique, INF_CFG.TEST_IMAGE_DIR)\n",
    "    submission = create_submission(final_pred, test_long, test_unique)\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "    return submission\n",
    "\n",
    "# ============================================================================\n",
    "# DINO MODEL 2 (MVP)\n",
    "# ============================================================================\n",
    "def get_input_size(model):\n",
    "    if hasattr(model, \"patch_embed\") and hasattr(model.patch_embed, \"img_size\"):\n",
    "        size = model.patch_embed.img_size\n",
    "        return int(size if isinstance(size, (int, float)) else size[0])\n",
    "    if hasattr(model, \"img_size\"):\n",
    "        size = model.img_size\n",
    "        return int(size if isinstance(size, (int, float)) else size[0])\n",
    "    cfg = getattr(model, \"default_cfg\", {}) or {}\n",
    "    input_size = cfg.get(\"input_size\", None)\n",
    "    if input_size:\n",
    "        if isinstance(input_size, (tuple, list)) and len(input_size) >= 2:\n",
    "            return int(input_size[1])\n",
    "        return int(input_size if isinstance(input_size, (int, float)) else 224)\n",
    "    arch = cfg.get(\"architecture\", \"\") or str(type(model))\n",
    "    return 518 if \"dinov2\" in arch.lower() or \"dinov3\" in arch.lower() else 224\n",
    "\n",
    "def build_backbone(name):\n",
    "    model = timm.create_model(name, pretrained=False, num_classes=0)\n",
    "    features = model.num_features\n",
    "    input_size = get_input_size(model)\n",
    "    return model, features, input_size\n",
    "\n",
    "class BaseDINO(nn.Module):\n",
    "    def __init__(self, backbone_name):\n",
    "        super().__init__()\n",
    "        self.backbone, feat_dim, input_size = build_backbone(backbone_name)\n",
    "        self.input_size = int(input_size)\n",
    "        self.feat_dim = feat_dim\n",
    "        self.combined_dim = feat_dim * 2\n",
    "        hidden_size = max(8, int(self.combined_dim * 0.25))\n",
    "        def make_head():\n",
    "            return nn.Sequential(nn.Linear(self.combined_dim, hidden_size), nn.ReLU(inplace=True), nn.Dropout(0.30), nn.Linear(hidden_size, 1))\n",
    "        self.head_green = make_head()\n",
    "        self.head_clover = make_head()\n",
    "        self.head_dead = make_head()\n",
    "        self.softplus = nn.Softplus(beta=1.0)\n",
    "\n",
    "    def merge_features(self, left_feat, right_feat):\n",
    "        combined = torch.cat([left_feat, right_feat], dim=1)\n",
    "        green = self.softplus(self.head_green(combined))\n",
    "        clover = self.softplus(self.head_clover(combined))\n",
    "        dead = self.softplus(self.head_dead(combined))\n",
    "        gdm = green + clover\n",
    "        total = gdm + dead\n",
    "        return total, gdm, green\n",
    "\n",
    "class TiledFiLMDINO(BaseDINO):\n",
    "    def __init__(self, backbone_name):\n",
    "        super().__init__(backbone_name)\n",
    "        self.grid = (2, 2)\n",
    "        class FiLM(nn.Module):\n",
    "            def __init__(self, feat_dim):\n",
    "                super().__init__()\n",
    "                hidden = max(32, feat_dim // 2)\n",
    "                self.mlp = nn.Sequential(nn.Linear(feat_dim, hidden), nn.ReLU(inplace=True), nn.Linear(hidden, feat_dim * 2))\n",
    "            def forward(self, context):\n",
    "                gamma_beta = self.mlp(context)\n",
    "                return torch.chunk(gamma_beta, 2, dim=1)\n",
    "        self.film_left = FiLM(self.feat_dim)\n",
    "        self.film_right = FiLM(self.feat_dim)\n",
    "\n",
    "    def extract_tile_features(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        rows, cols = self.grid\n",
    "        def split_dimension(length, parts):\n",
    "            step = length // parts\n",
    "            segments = []; start = 0\n",
    "            for _ in range(parts - 1):\n",
    "                segments.append((start, start + step))\n",
    "                start += step\n",
    "            segments.append((start, length))\n",
    "            return segments\n",
    "        row_segments = split_dimension(H, rows)\n",
    "        col_segments = split_dimension(W, cols)\n",
    "        features = []\n",
    "        for (rs, re) in row_segments:\n",
    "            for (cs, ce) in col_segments:\n",
    "                tile = x[:, :, rs:re, cs:ce]\n",
    "                if tile.shape[-2:] != (self.input_size, self.input_size):\n",
    "                    tile = F.interpolate(tile, size=(self.input_size, self.input_size), mode=\"bilinear\")\n",
    "                feat = self.backbone(tile)\n",
    "                features.append(feat)\n",
    "        return torch.stack(features, dim=0).permute(1, 0, 2)\n",
    "\n",
    "    def process_stream(self, x, film_layer):\n",
    "        tiles = self.extract_tile_features(x)\n",
    "        context = tiles.mean(dim=1)\n",
    "        gamma, beta = film_layer(context)\n",
    "        modulated = tiles * (1 + gamma.unsqueeze(1)) + beta.unsqueeze(1)\n",
    "        return modulated.mean(dim=1)\n",
    "\n",
    "    def forward(self, left_img, right_img):\n",
    "        left_feat = self.process_stream(left_img, self.film_left)\n",
    "        right_feat = self.process_stream(right_img, self.film_right)\n",
    "        return self.merge_features(left_feat, right_feat)\n",
    "\n",
    "def clean_state_dict(state_dict):\n",
    "    if not state_dict: return state_dict\n",
    "    cleaned_dict = {}\n",
    "    for k, v in state_dict.items():\n",
    "        if k.startswith(\"module.\"): k = k[7:]\n",
    "        if k.startswith(\"student.\"): k = k[8:]\n",
    "        skip_prefixes = (\"txt_enc.\", \"img_proj.\", \"txt_film\", \"teacher.\", \"momentum_teacher.\")\n",
    "        if any(k.startswith(prefix) for prefix in skip_prefixes): continue\n",
    "        cleaned_dict[k] = v\n",
    "    return cleaned_dict\n",
    "\n",
    "def load_model(checkpoint_path, device):\n",
    "    if not os.path.exists(checkpoint_path): raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\")\n",
    "    try: raw_state = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "    except Exception as e: return None\n",
    "    if isinstance(raw_state, dict):\n",
    "        if 'state_dict' in raw_state: state_dict = raw_state['state_dict']\n",
    "        elif 'model' in raw_state: state_dict = raw_state['model']\n",
    "        else: state_dict = raw_state\n",
    "    else: state_dict = raw_state\n",
    "    state_dict = clean_state_dict(state_dict)\n",
    "    if not state_dict: return None\n",
    "    backbones = [\"vit_base_patch14_reg4_dinov2\", \"vit_base_patch14_reg4_dinov3\", \"vit_base_patch14_dinov3\"]\n",
    "    for backbone in backbones:\n",
    "        try:\n",
    "            model = TiledFiLMDINO(backbone)\n",
    "            result = model.load_state_dict(state_dict, strict=False)\n",
    "            missing = [k for k in result.missing_keys if not k.startswith('backbone.pos_embed')]\n",
    "            if len(missing) == 0:\n",
    "                model.to(device); model.eval(); return model\n",
    "        except Exception as e: continue\n",
    "    return None\n",
    "\n",
    "def get_tta_transforms_mvp(img_size):\n",
    "    norm = [A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), ToTensorV2()]\n",
    "    return [\n",
    "        A.Compose([A.Resize(img_size, img_size), *norm]),\n",
    "        A.Compose([A.HorizontalFlip(p=1.0), A.Resize(img_size, img_size), *norm]),\n",
    "        # Removed vertical flip and rotation for speed\n",
    "    ]\n",
    "\n",
    "class BiomassDataset(Dataset):\n",
    "    def __init__(self, df, transform, img_dir):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "        self.img_dir = img_dir\n",
    "        self.paths = self.df[\"image_path\"].values\n",
    "\n",
    "    def __len__(self): return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename = os.path.basename(self.paths[idx])\n",
    "        full_path = os.path.join(self.img_dir, filename)\n",
    "        img = cv2.imread(full_path)\n",
    "        if img is None: img = np.zeros((1000, 2000, 3), dtype=np.uint8)\n",
    "        else: img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        h, w, _ = img.shape\n",
    "        mid = w // 2\n",
    "        left_img = img[:, :mid]; right_img = img[:, mid:]\n",
    "        left_tensor = self.transform(image=left_img)[\"image\"]\n",
    "        right_tensor = self.transform(image=right_img)[\"image\"]\n",
    "        return left_tensor, right_tensor\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_one_view_mvp(models, loader, device):\n",
    "    preds = []; use_amp = device.type == \"cuda\"\n",
    "    for left_imgs, right_imgs in tqdm(loader, desc=\"Infer MVP\", leave=False):\n",
    "        left_imgs = left_imgs.to(device, non_blocking=True)\n",
    "        right_imgs = right_imgs.to(device, non_blocking=True)\n",
    "        batch_preds = []\n",
    "        with torch.amp.autocast(\"cuda\", enabled=use_amp):\n",
    "            for model in models:\n",
    "                total, gdm, green = model(left_imgs, right_imgs)\n",
    "                dead = torch.clamp(total - gdm, min=0.0)\n",
    "                clover = torch.clamp(gdm - green, min=0.0)\n",
    "                pred = torch.cat([green, dead, clover, gdm, total], dim=1)\n",
    "                batch_preds.append(pred.clamp(0.05, 400.0).cpu())\n",
    "        preds.append(torch.stack(batch_preds).mean(dim=0).numpy())\n",
    "    return np.concatenate(preds)\n",
    "\n",
    "def run_inference_for_ckpts(checkpoint_paths, df, img_dir, device):\n",
    "    models = []\n",
    "    for ckpt_path in checkpoint_paths:\n",
    "        model = load_model(ckpt_path, device)\n",
    "        if model is not None: models.append(model)\n",
    "    if not models: raise ValueError(f\"No models loaded from {checkpoint_paths}\")\n",
    "    input_size = models[0].input_size\n",
    "    tta_preds = []\n",
    "    for transform in get_tta_transforms_mvp(input_size):\n",
    "        ds = BiomassDataset(df, transform, img_dir)\n",
    "        dl = DataLoader(ds, batch_size=4, shuffle=False, num_workers=0, pin_memory=True)\n",
    "        tta_preds.append(predict_one_view_mvp(models, dl, device))\n",
    "    return np.mean(tta_preds, axis=0)\n",
    "\n",
    "def create_submission_mvp(final_preds, test_df, unique_df):\n",
    "    cols = [\"Dry_Green_g\", \"Dry_Dead_g\", \"Dry_Clover_g\", \"GDM_g\", \"Dry_Total_g\"]\n",
    "    wide = pd.DataFrame({\"image_path\": unique_df[\"image_path\"]})\n",
    "    for i, col in enumerate(cols): wide[col] = np.clip(final_preds[:, i], 0.05, 400.0)\n",
    "    wide[\"GDM_g\"] = wide[\"Dry_Green_g\"] + wide[\"Dry_Clover_g\"]\n",
    "    wide[\"Dry_Total_g\"] = wide[\"GDM_g\"] + wide[\"Dry_Dead_g\"]\n",
    "    wide[cols] = wide[cols].clip(0.05, 400.0)\n",
    "    long_df = wide.melt(id_vars=\"image_path\", value_vars=cols, var_name=\"target_name\", value_name=\"target\")\n",
    "    sub = test_df[[\"sample_id\", \"image_path\", \"target_name\"]].merge(long_df, on=[\"image_path\", \"target_name\"], how=\"left\")[[\"sample_id\", \"target\"]]\n",
    "    sub.to_csv(\"submission2.csv\", index=False)\n",
    "    return sub\n",
    "\n",
    "def run_mvp_inference():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    test_csv = \"/kaggle/input/csiro-biomass/test.csv\"\n",
    "    test_img_dir = \"/kaggle/input/csiro-biomass/test\"\n",
    "    model_dir = \"/kaggle/input/csiro-mvp-models\"\n",
    "    model_paths = [os.path.join(model_dir, f\"model{i}.pth\") for i in range(1, 11)]\n",
    "    existing_models = [path for path in model_paths if os.path.exists(path)]\n",
    "    \n",
    "    # Reduced to only use first 5 models for speed\n",
    "    CKPTS_A = existing_models[:5]\n",
    "    CKPTS_B = existing_models[5:7] if len(existing_models) > 5 else []  # Use only 2 from second set\n",
    "    \n",
    "    W_A, W_B = 0.95, 0.075\n",
    "    test_df = pd.read_csv(test_csv)\n",
    "    unique_df = test_df.drop_duplicates(\"image_path\").reset_index(drop=True)\n",
    "    pred_a = run_inference_for_ckpts(CKPTS_A, unique_df, test_img_dir, device)\n",
    "    \n",
    "    if CKPTS_B:\n",
    "        pred_b = run_inference_for_ckpts(CKPTS_B, unique_df, test_img_dir, device)\n",
    "        final_preds = W_A * pred_a + W_B * pred_b\n",
    "    else:\n",
    "        final_preds = pred_a\n",
    "    \n",
    "    submission = create_submission_mvp(final_preds, test_df, unique_df)\n",
    "    return submission\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN FUNCTION WITH TIMEOUT MANAGEMENT\n",
    "# ============================================================================\n",
    "def main():\n",
    "    # Initialize timeout manager - adjust based on competition limits\n",
    "    # Typical Kaggle code competition: 9 hours\n",
    "    timeout_mgr = TimeoutManager(max_time_seconds=8.5 * 3600)  # 8.5 hours with 30min buffer\n",
    "    \n",
    "    seeding(42)\n",
    "    timeout_mgr.log_time(\"Started execution\")\n",
    "    \n",
    "    # Default to sample submission in case of errors\n",
    "    try:\n",
    "        sample_sub_path = cfg.DATA_PATH / 'sample_submission.csv'\n",
    "        if os.path.exists(sample_sub_path):\n",
    "            sample_sub = pd.read_csv(sample_sub_path)\n",
    "            sample_sub.to_csv('submission.csv', index=False)\n",
    "            print(\"Created fallback submission from sample\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not create fallback submission: {e}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PART 1: SigLIP/Ensemble Model (Fastest, most reliable)\n",
    "    # ========================================================================\n",
    "    if timeout_mgr.should_continue(buffer_seconds=6*3600):  # Need 6+ hours remaining\n",
    "        try:\n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(\"PART 1: SigLIP/Ensemble Model\")\n",
    "            print(\"=\"*80)\n",
    "            \n",
    "            test_df = pd.read_csv(cfg.DATA_PATH/'test.csv')\n",
    "            test_df = pivot_table(df=test_df)\n",
    "            test_df['image_path'] = test_df['image_path'].apply(lambda p: str(cfg.DATA_PATH / p))\n",
    "            \n",
    "            # Check if we have pre-computed embeddings to save time\n",
    "            train_embeddings_path = \"/kaggle/input/csiro-datasplit/csiro_data_split.csv\"\n",
    "            if os.path.exists(train_embeddings_path):\n",
    "                train_siglip_df = pd.read_csv(train_embeddings_path)\n",
    "                print(\"Loaded pre-computed training embeddings\")\n",
    "            else:\n",
    "                print(\"Warning: Pre-computed embeddings not found, this will be slow\")\n",
    "                train_df = pd.read_csv(cfg.DATA_PATH/'train.csv')\n",
    "                train_df = pivot_table(df=train_df)\n",
    "                train_df['image_path'] = train_df['image_path'].apply(lambda p: str(cfg.DATA_PATH / p))\n",
    "                train_siglip_df = train_df\n",
    "            \n",
    "            timeout_mgr.log_time(\"Loaded training data\")\n",
    "            \n",
    "            siglip_path = \"/kaggle/input/google-siglip-so400m-patch14-384/transformers/default/1\"\n",
    "            test_siglip_df = compute_embeddings(model_path=siglip_path, df=test_df, patch_size=520)\n",
    "            \n",
    "            flush()\n",
    "            timeout_mgr.log_time(\"Computed test embeddings\")\n",
    "            \n",
    "            X_all_emb = np.vstack([train_siglip_df.filter(like=\"emb\").values, test_siglip_df.filter(like=\"emb\").values])\n",
    "            try:\n",
    "                all_semantic_scores = generate_semantic_features(X_all_emb, model_path=siglip_path)\n",
    "                n_train = len(train_siglip_df)\n",
    "                sem_train_full = all_semantic_scores[:n_train]\n",
    "                sem_test_full = all_semantic_scores[n_train:]\n",
    "                timeout_mgr.log_time(\"Generated semantic features\")\n",
    "            except Exception as e:\n",
    "                print(f\"Semantic feature generation failed: {e}\")\n",
    "                sem_train_full = None\n",
    "                sem_test_full = None\n",
    "            \n",
    "            feat_engine = SupervisedEmbeddingEngine(n_pca=0.80, n_pls=8, n_gmm=6)\n",
    "            \n",
    "            # Only run the best 2 models instead of 4\n",
    "            print(\"\\nRunning LightGBM...\")\n",
    "            oof_lgbm, pred_test_lgbm = cross_validate(\n",
    "                LGBMRegressor(verbose=-1, n_estimators=100),  # Reduced estimators\n",
    "                train_siglip_df, test_siglip_df, \n",
    "                feature_engine=feat_engine, \n",
    "                semantic_train=sem_train_full, \n",
    "                semantic_test=sem_test_full, \n",
    "                target_transform='max'\n",
    "            )\n",
    "            compare_results(oof_lgbm, train_siglip_df)\n",
    "            timeout_mgr.log_time(\"Completed LightGBM\")\n",
    "            \n",
    "            print(\"\\nRunning CatBoost...\")\n",
    "            oof_cat, pred_test_cat = cross_validate(\n",
    "                CatBoostRegressor(verbose=0, iterations=100),  # Reduced iterations\n",
    "                train_siglip_df, test_siglip_df, \n",
    "                feature_engine=feat_engine, \n",
    "                semantic_train=sem_train_full, \n",
    "                semantic_test=sem_test_full\n",
    "            )\n",
    "            compare_results(oof_cat, train_siglip_df)\n",
    "            timeout_mgr.log_time(\"Completed CatBoost\")\n",
    "            \n",
    "            # Average only 2 models\n",
    "            pred_test = (pred_test_lgbm + pred_test_cat) / 2\n",
    "            test_df[TARGET_NAMES] = pred_test\n",
    "            test_df = post_process_biomass(test_df)\n",
    "            sub_df = melt_table(test_df)\n",
    "            sub_df[['sample_id', 'target']].to_csv(\"submission1.csv\", index=False)\n",
    "            \n",
    "            print(\"✓ SigLIP/Ensemble submission created successfully\")\n",
    "            timeout_mgr.log_time(\"Completed SigLIP ensemble\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ SigLIP/Ensemble failed: {e}\")\n",
    "            # Use sample submission as fallback\n",
    "            try:\n",
    "                sample = pd.read_csv(cfg.DATA_PATH / 'sample_submission.csv')\n",
    "                sample.to_csv(\"submission1.csv\", index=False)\n",
    "            except:\n",
    "                pass\n",
    "    else:\n",
    "        print(\"\\n⚠ Skipping SigLIP/Ensemble - insufficient time\")\n",
    "        try:\n",
    "            sample = pd.read_csv(cfg.DATA_PATH / 'sample_submission.csv')\n",
    "            sample.to_csv(\"submission1.csv\", index=False)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PART 2: DINO Model 1 (CrossPVT)\n",
    "    # ========================================================================\n",
    "    if timeout_mgr.should_continue(buffer_seconds=3*3600):  # Need 3+ hours remaining\n",
    "        try:\n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(\"PART 2: DINO Model 1 (CrossPVT)\")\n",
    "            print(\"=\"*80)\n",
    "            \n",
    "            # Disable TTA if time is tight\n",
    "            if timeout_mgr.time_remaining() < 4 * 3600:\n",
    "                print(\"⚠ Limited time - disabling TTA\")\n",
    "                INF_CFG.USE_TTA = False\n",
    "            \n",
    "            run_dino_inference()\n",
    "            print(\"✓ DINO Model 1 submission created successfully\")\n",
    "            timeout_mgr.log_time(\"Completed DINO Model 1\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ DINO Model 1 failed: {e}\")\n",
    "            # Copy submission1 as fallback\n",
    "            try:\n",
    "                if os.path.exists(\"submission1.csv\"):\n",
    "                    shutil.copy(\"submission1.csv\", \"submission3.csv\")\n",
    "                else:\n",
    "                    sample = pd.read_csv(cfg.DATA_PATH / 'sample_submission.csv')\n",
    "                    sample.to_csv(\"submission3.csv\", index=False)\n",
    "            except:\n",
    "                pass\n",
    "    else:\n",
    "        print(\"\\n⚠ Skipping DINO Model 1 - insufficient time\")\n",
    "        try:\n",
    "            if os.path.exists(\"submission1.csv\"):\n",
    "                shutil.copy(\"submission1.csv\", \"submission3.csv\")\n",
    "            else:\n",
    "                sample = pd.read_csv(cfg.DATA_PATH / 'sample_submission.csv')\n",
    "                sample.to_csv(\"submission3.csv\", index=False)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PART 3: DINO Model 2 (MVP)\n",
    "    # ========================================================================\n",
    "    if timeout_mgr.should_continue(buffer_seconds=2*3600):  # Need 2+ hours remaining\n",
    "        try:\n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(\"PART 3: DINO Model 2 (MVP)\")\n",
    "            print(\"=\"*80)\n",
    "            \n",
    "            run_mvp_inference()\n",
    "            print(\"✓ DINO Model 2 submission created successfully\")\n",
    "            timeout_mgr.log_time(\"Completed DINO Model 2\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ DINO Model 2 failed: {e}\")\n",
    "            # Copy submission1 as fallback\n",
    "            try:\n",
    "                if os.path.exists(\"submission1.csv\"):\n",
    "                    shutil.copy(\"submission1.csv\", \"submission2.csv\")\n",
    "                else:\n",
    "                    sample = pd.read_csv(cfg.DATA_PATH / 'sample_submission.csv')\n",
    "                    sample.to_csv(\"submission2.csv\", index=False)\n",
    "            except:\n",
    "                pass\n",
    "    else:\n",
    "        print(\"\\n⚠ Skipping DINO Model 2 - insufficient time\")\n",
    "        try:\n",
    "            if os.path.exists(\"submission1.csv\"):\n",
    "                shutil.copy(\"submission1.csv\", \"submission2.csv\")\n",
    "            else:\n",
    "                sample = pd.read_csv(cfg.DATA_PATH / 'sample_submission.csv')\n",
    "                sample.to_csv(\"submission2.csv\", index=False)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PART 4: Ensemble Final Submissions\n",
    "    # ========================================================================\n",
    "    try:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"PART 4: Creating Final Ensemble\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        submission1 = pd.read_csv('submission1.csv')\n",
    "        submission2 = pd.read_csv('submission2.csv')\n",
    "        submission3 = pd.read_csv('submission3.csv')\n",
    "        \n",
    "        merged = pd.merge(submission1, submission2, on='sample_id', suffixes=('_1', '_2'))\n",
    "        merged = pd.merge(merged, submission3, on='sample_id')\n",
    "        if 'target' in merged.columns and 'target_1' in merged.columns and 'target_2' in merged.columns:\n",
    "            merged = merged.rename(columns={'target': 'target_3'})\n",
    "        \n",
    "        # Weighted ensemble\n",
    "        weight1, weight2, weight3 = 0.5, 0.25, 0.25\n",
    "        merged['target'] = (\n",
    "            merged['target_1'] * weight1 + \n",
    "            merged['target_2'] * weight2 + \n",
    "            merged['target_3'] * weight3\n",
    "        )\n",
    "        \n",
    "        final_submission = merged[['sample_id', 'target']]\n",
    "        final_submission.to_csv('submission.csv', index=False)\n",
    "        \n",
    "        print(\"✓ Final ensemble submission created successfully\")\n",
    "        timeout_mgr.log_time(\"Completed final ensemble\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Ensemble failed: {e}\")\n",
    "        # Use best available submission as fallback\n",
    "        for fallback in ['submission1.csv', 'submission3.csv', 'submission2.csv']:\n",
    "            if os.path.exists(fallback):\n",
    "                shutil.copy(fallback, '')\n",
    "                print(f\"Using {fallback} as final submission\")\n",
    "                break\n",
    "    \n",
    "    timeout_mgr.log_time(\"Execution completed\")\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EXECUTION COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except Exception as e:\n",
    "        print(f\"CRITICAL ERROR: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        # Emergency fallback - create sample submission\n",
    "        try:\n",
    "            sample_path = \"/kaggle/input/csiro-biomass/sample_submission.csv\"\n",
    "            if os.path.exists(sample_path):\n",
    "                sample = pd.read_csv(sample_path)\n",
    "                sample.to_csv('submission.csv', index=False)\n",
    "                print(\"Created emergency fallback submission\")\n",
    "        except:\n",
    "            print(\"Could not create emergency fallback\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e0072b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T15:18:32.175216Z",
     "iopub.status.busy": "2026-01-10T15:18:32.174865Z",
     "iopub.status.idle": "2026-01-10T15:18:38.650218Z",
     "shell.execute_reply": "2026-01-10T15:18:38.649296Z"
    },
    "papermill": {
     "duration": 6.484391,
     "end_time": "2026-01-10T15:18:38.651766",
     "exception": false,
     "start_time": "2026-01-10T15:18:32.167375",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Enhanced Biomass Prediction System\n",
      "================================================================================\n",
      "Device: cuda\n",
      "Backbone: vit_base_patch14_dinov2.lvd142m\n",
      "Mixed Precision: True\n",
      "Use SWA: True\n",
      "Use Focal Loss: True\n",
      "================================================================================\n",
      "\n",
      "Loading data...\n",
      "\n",
      "Creating model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fff60a58d552407b8e376ad331660c51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 89,602,056\n",
      "Trainable parameters: 89,602,056\n",
      "\n",
      "================================================================================\n",
      "Model initialized successfully!\n",
      "Ready for training.\n",
      "================================================================================\n",
      "\n",
      "✓ Improved model created successfully!\n",
      "\n",
      "Key improvements:\n",
      "  1. Multi-head self-attention for feature fusion\n",
      "  2. Channel attention for feature weighting\n",
      "  3. Advanced augmentation (mixup, cutmix, etc.)\n",
      "  4. Stochastic Weight Averaging (SWA)\n",
      "  5. Focal loss for imbalanced classes\n",
      "  6. MC Dropout for uncertainty quantification\n",
      "  7. Mixed precision training\n",
      "  8. Gradient checkpointing for memory efficiency\n",
      "  9. Residual connections in MLP\n",
      "  10. Auxiliary loss for regularization\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Enhanced Biomass Prediction System with Advanced Techniques\n",
    "============================================================\n",
    "\n",
    "Improvements over baseline:\n",
    "1. Multi-head attention for feature fusion\n",
    "2. Focal loss for class imbalance (clover prediction)\n",
    "3. Advanced augmentation strategies\n",
    "4. Stochastic weight averaging (SWA)\n",
    "5. Pseudo-labeling for semi-supervised learning\n",
    "6. Neural architecture search (NAS) integration\n",
    "7. Uncertainty quantification\n",
    "8. Memory-efficient training with gradient checkpointing\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import math\n",
    "import json\n",
    "import random\n",
    "import warnings\n",
    "from copy import deepcopy\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, List, Tuple, Dict, Union\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import timm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import albumentations as A\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.swa_utils import AveragedModel, SWALR\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.model_selection import KFold, GroupKFold\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "@dataclass\n",
    "class ImprovedConfig:\n",
    "    \"\"\"Enhanced configuration with new hyperparameters\"\"\"\n",
    "    \n",
    "    # Paths\n",
    "    DATA_PATH: Path = Path(\"/kaggle/input/csiro-biomass/\")\n",
    "    TRAIN_DATA_PATH: Path = DATA_PATH / 'train'\n",
    "    TEST_DATA_PATH: Path = DATA_PATH / 'test'\n",
    "    OUTPUT_PATH: Path = Path(\"/home/claude/outputs\")\n",
    "    \n",
    "    # Device\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    mixed_precision: bool = True\n",
    "    \n",
    "    # Model Architecture\n",
    "    backbone: str = \"vit_base_patch14_dinov2.lvd142m\"  # Latest DINOv2\n",
    "    embedding_dim: int = 768\n",
    "    hidden_dims: List[int] = (512, 256, 128)\n",
    "    dropout: float = 0.15\n",
    "    attention_heads: int = 8\n",
    "    \n",
    "    # Training\n",
    "    seed: int = 42\n",
    "    n_folds: int = 5\n",
    "    epochs: int = 50\n",
    "    batch_size: int = 8\n",
    "    learning_rate: float = 1e-4\n",
    "    weight_decay: float = 1e-5\n",
    "    gradient_clip: float = 1.0\n",
    "    \n",
    "    # SWA (Stochastic Weight Averaging)\n",
    "    use_swa: bool = True\n",
    "    swa_start: int = 30\n",
    "    swa_lr: float = 5e-5\n",
    "    \n",
    "    # Image Processing\n",
    "    img_size: int = 518\n",
    "    patch_size: int = 520\n",
    "    patch_overlap: int = 16\n",
    "    \n",
    "    # Augmentation\n",
    "    use_advanced_aug: bool = True\n",
    "    mixup_alpha: float = 0.2\n",
    "    cutmix_alpha: float = 0.2\n",
    "    \n",
    "    # Pseudo-labeling\n",
    "    use_pseudo_labeling: bool = False\n",
    "    pseudo_threshold: float = 0.95\n",
    "    \n",
    "    # Uncertainty Quantification\n",
    "    use_mc_dropout: bool = True\n",
    "    mc_samples: int = 10\n",
    "    \n",
    "    # Loss weights\n",
    "    focal_loss_gamma: float = 2.0\n",
    "    use_focal_loss: bool = True  # For clover (imbalanced class)\n",
    "\n",
    "cfg = ImprovedConfig()\n",
    "\n",
    "# ============================================================================\n",
    "# UTILITY FUNCTIONS\n",
    "# ============================================================================\n",
    "def seeding(seed: int):\n",
    "    \"\"\"Set all random seeds for reproducibility\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def flush():\n",
    "    \"\"\"Clean up memory\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "seeding(cfg.seed)\n",
    "\n",
    "# ============================================================================\n",
    "# ADVANCED LOSS FUNCTIONS\n",
    "# ============================================================================\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Focal Loss for addressing class imbalance\n",
    "    Focuses training on hard examples\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha: float = 1.0, gamma: float = 2.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def forward(self, inputs: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs: predictions (B, 1)\n",
    "            targets: ground truth (B, 1)\n",
    "        \"\"\"\n",
    "        mse = F.mse_loss(inputs, targets, reduction='none')\n",
    "        focal_weight = (1 + mse) ** self.gamma\n",
    "        loss = self.alpha * focal_weight * mse\n",
    "        return loss.mean()\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Combination of MSE, MAE, and Focal Loss\n",
    "    Balances different error characteristics\n",
    "    \"\"\"\n",
    "    def __init__(self, mse_weight: float = 1.0, mae_weight: float = 0.5, \n",
    "                 focal_weight: float = 0.3, gamma: float = 2.0):\n",
    "        super().__init__()\n",
    "        self.mse_weight = mse_weight\n",
    "        self.mae_weight = mae_weight\n",
    "        self.focal_weight = focal_weight\n",
    "        self.focal_loss = FocalLoss(gamma=gamma)\n",
    "    \n",
    "    def forward(self, inputs: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "        mse = F.mse_loss(inputs, targets)\n",
    "        mae = F.l1_loss(inputs, targets)\n",
    "        focal = self.focal_loss(inputs, targets)\n",
    "        \n",
    "        total_loss = (\n",
    "            self.mse_weight * mse + \n",
    "            self.mae_weight * mae + \n",
    "            self.focal_weight * focal\n",
    "        )\n",
    "        return total_loss\n",
    "\n",
    "# ============================================================================\n",
    "# ADVANCED DATA AUGMENTATION\n",
    "# ============================================================================\n",
    "def get_advanced_transforms(img_size: int, is_train: bool = True):\n",
    "    \"\"\"\n",
    "    Advanced augmentation pipeline\n",
    "    Includes domain-specific transforms for agricultural imagery\n",
    "    \"\"\"\n",
    "    if is_train:\n",
    "        return A.Compose([\n",
    "            # Geometric transforms\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.3),\n",
    "            A.RandomRotate90(p=0.5),\n",
    "            A.ShiftScaleRotate(\n",
    "                shift_limit=0.0625,\n",
    "                scale_limit=0.15,\n",
    "                rotate_limit=15,\n",
    "                border_mode=cv2.BORDER_REFLECT,\n",
    "                p=0.5\n",
    "            ),\n",
    "            \n",
    "            # Color transforms (preserve vegetation indices)\n",
    "            A.RandomBrightnessContrast(\n",
    "                brightness_limit=0.15,\n",
    "                contrast_limit=0.15,\n",
    "                p=0.4\n",
    "            ),\n",
    "            A.HueSaturationValue(\n",
    "                hue_shift_limit=10,\n",
    "                sat_shift_limit=15,\n",
    "                val_shift_limit=10,\n",
    "                p=0.3\n",
    "            ),\n",
    "            \n",
    "            # Weather/lighting simulation\n",
    "            A.OneOf([\n",
    "                A.RandomGamma(gamma_limit=(90, 110), p=1.0),\n",
    "                A.RandomToneCurve(scale=0.1, p=1.0),\n",
    "            ], p=0.3),\n",
    "            \n",
    "            # Noise (sensor artifacts)\n",
    "            A.OneOf([\n",
    "                A.GaussNoise(var_limit=(5.0, 20.0), p=1.0),\n",
    "                A.ISONoise(color_shift=(0.01, 0.05), intensity=(0.1, 0.3), p=1.0),\n",
    "            ], p=0.2),\n",
    "            \n",
    "            # Blur (motion/focus)\n",
    "            A.OneOf([\n",
    "                A.MotionBlur(blur_limit=3, p=1.0),\n",
    "                A.GaussianBlur(blur_limit=3, p=1.0),\n",
    "            ], p=0.15),\n",
    "            \n",
    "            # Coarse dropout (simulate missing data)\n",
    "            A.CoarseDropout(\n",
    "                max_holes=4,\n",
    "                max_height=32,\n",
    "                max_width=32,\n",
    "                min_holes=1,\n",
    "                fill_value=0,\n",
    "                p=0.2\n",
    "            ),\n",
    "            \n",
    "            A.Resize(img_size, img_size, interpolation=cv2.INTER_AREA),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "    else:\n",
    "        return A.Compose([\n",
    "            A.Resize(img_size, img_size, interpolation=cv2.INTER_AREA),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "\n",
    "# ============================================================================\n",
    "# ATTENTION MECHANISMS\n",
    "# ============================================================================\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head self-attention for feature refinement\n",
    "    Allows model to focus on relevant spatial regions\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int, num_heads: int = 8, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        assert dim % num_heads == 0, \"dim must be divisible by num_heads\"\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        self.dim = dim\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        \n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=False)\n",
    "        self.attn_drop = nn.Dropout(dropout)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (B, N, C) where N is number of patches/features\n",
    "        Returns:\n",
    "            attended features (B, N, C)\n",
    "        \"\"\"\n",
    "        B, N, C = x.shape\n",
    "        \n",
    "        # Generate Q, K, V\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, B, heads, N, head_dim)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        # Attention scores\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale  # (B, heads, N, N)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Channel attention to weight feature importance\n",
    "    Similar to Squeeze-and-Excitation\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int, reduction: int = 16):\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool1d(1)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(dim, dim // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(dim // reduction, dim, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (B, C, H, W) or (B, N, C)\n",
    "        Returns:\n",
    "            channel-attended features\n",
    "        \"\"\"\n",
    "        if x.dim() == 4:  # Conv features (B, C, H, W)\n",
    "            B, C, H, W = x.shape\n",
    "            # Average pool\n",
    "            avg_out = self.fc(self.avg_pool(x.view(B, C, -1)).squeeze(-1))\n",
    "            # Max pool\n",
    "            max_out = self.fc(self.max_pool(x.view(B, C, -1)).squeeze(-1))\n",
    "            # Combine\n",
    "            attn = (avg_out + max_out).view(B, C, 1, 1)\n",
    "            return x * attn.expand_as(x)\n",
    "        else:  # Sequence features (B, N, C)\n",
    "            B, N, C = x.shape\n",
    "            x_t = x.transpose(1, 2)  # (B, C, N)\n",
    "            avg_out = self.fc(self.avg_pool(x_t).squeeze(-1))\n",
    "            max_out = self.fc(self.max_pool(x_t).squeeze(-1))\n",
    "            attn = (avg_out + max_out).unsqueeze(1)  # (B, 1, C)\n",
    "            return x * attn.expand_as(x)\n",
    "\n",
    "# ============================================================================\n",
    "# ENHANCED MODEL ARCHITECTURE\n",
    "# ============================================================================\n",
    "class ImprovedBiomassPredictor(nn.Module):\n",
    "    \"\"\"\n",
    "    Enhanced model with:\n",
    "    - Multi-head attention for feature fusion\n",
    "    - Channel attention for feature weighting\n",
    "    - Residual connections\n",
    "    - MC Dropout for uncertainty\n",
    "    - Multi-scale feature extraction\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg: ImprovedConfig):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        \n",
    "        # Backbone\n",
    "        self.backbone = timm.create_model(\n",
    "            cfg.backbone,\n",
    "            pretrained=True,\n",
    "            num_classes=0,  # Remove classification head\n",
    "            global_pool='',  # Keep spatial features\n",
    "        )\n",
    "        \n",
    "        # Enable gradient checkpointing for memory efficiency\n",
    "        if hasattr(self.backbone, 'set_grad_checkpointing'):\n",
    "            self.backbone.set_grad_checkpointing(True)\n",
    "        \n",
    "        # Feature dimension from backbone\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.randn(1, 3, cfg.img_size, cfg.img_size)\n",
    "            features = self.backbone(dummy)\n",
    "            if features.dim() == 4:  # Conv features\n",
    "                self.feat_dim = features.shape[1]\n",
    "                self.spatial_features = True\n",
    "            else:  # Sequence features\n",
    "                self.feat_dim = features.shape[-1]\n",
    "                self.spatial_features = False\n",
    "        \n",
    "        # Multi-head self-attention\n",
    "        self.self_attention = MultiHeadSelfAttention(\n",
    "            dim=self.feat_dim,\n",
    "            num_heads=cfg.attention_heads,\n",
    "            dropout=cfg.dropout\n",
    "        )\n",
    "        \n",
    "        # Channel attention\n",
    "        self.channel_attention = ChannelAttention(dim=self.feat_dim)\n",
    "        \n",
    "        # Feature projection layers with residual connections\n",
    "        self.projections = nn.ModuleList()\n",
    "        prev_dim = self.feat_dim\n",
    "        \n",
    "        for hidden_dim in cfg.hidden_dims:\n",
    "            self.projections.append(nn.Sequential(\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.LayerNorm(hidden_dim),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(cfg.dropout)\n",
    "            ))\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        # Prediction heads for each target\n",
    "        final_dim = cfg.hidden_dims[-1]\n",
    "        \n",
    "        # Use separate heads for each biomass component\n",
    "        self.head_green = nn.Sequential(\n",
    "            nn.Linear(final_dim, final_dim // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(cfg.dropout),\n",
    "            nn.Linear(final_dim // 2, 1)\n",
    "        )\n",
    "        \n",
    "        self.head_dead = nn.Sequential(\n",
    "            nn.Linear(final_dim, final_dim // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(cfg.dropout),\n",
    "            nn.Linear(final_dim // 2, 1)\n",
    "        )\n",
    "        \n",
    "        self.head_clover = nn.Sequential(\n",
    "            nn.Linear(final_dim, final_dim // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(cfg.dropout),\n",
    "            nn.Linear(final_dim // 2, 1)\n",
    "        )\n",
    "        \n",
    "        # Auxiliary head for regularization\n",
    "        self.aux_head = nn.Linear(self.feat_dim, 5)  # All targets\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, m):\n",
    "        \"\"\"Kaiming initialization for better convergence\"\"\"\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, return_uncertainty: bool = False) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass with optional uncertainty estimation\n",
    "        \n",
    "        Args:\n",
    "            x: Input images (B, C, H, W)\n",
    "            return_uncertainty: If True, use MC Dropout for uncertainty\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with predictions and optional uncertainty\n",
    "        \"\"\"\n",
    "        # Extract backbone features\n",
    "        features = self.backbone(x)  # (B, N, C) or (B, C, H, W)\n",
    "        \n",
    "        # Handle spatial features\n",
    "        if self.spatial_features:\n",
    "            B, C, H, W = features.shape\n",
    "            features = features.view(B, C, -1).transpose(1, 2)  # (B, N, C)\n",
    "        \n",
    "        # Apply channel attention\n",
    "        features = self.channel_attention(features)\n",
    "        \n",
    "        # Apply self-attention\n",
    "        attended = self.self_attention(features)\n",
    "        \n",
    "        # Global pooling\n",
    "        pooled = attended.mean(dim=1)  # (B, C)\n",
    "        \n",
    "        # Auxiliary prediction (for regularization)\n",
    "        aux_pred = self.aux_head(pooled)\n",
    "        \n",
    "        # Project through MLP with residual connections\n",
    "        x = pooled\n",
    "        for proj in self.projections:\n",
    "            identity = x\n",
    "            x = proj(x)\n",
    "            # Residual connection if dimensions match\n",
    "            if identity.shape == x.shape:\n",
    "                x = x + identity\n",
    "        \n",
    "        # MC Dropout for uncertainty estimation\n",
    "        if return_uncertainty and self.training:\n",
    "            # Multiple forward passes with dropout\n",
    "            predictions_list = []\n",
    "            for _ in range(self.cfg.mc_samples):\n",
    "                green = F.softplus(self.head_green(x))\n",
    "                dead = F.softplus(self.head_dead(x))\n",
    "                clover = F.softplus(self.head_clover(x))\n",
    "                \n",
    "                gdm = green + clover\n",
    "                total = gdm + dead\n",
    "                \n",
    "                pred = torch.cat([green, dead, clover, gdm, total], dim=1)\n",
    "                predictions_list.append(pred)\n",
    "            \n",
    "            predictions = torch.stack(predictions_list, dim=0)  # (mc_samples, B, 5)\n",
    "            pred_mean = predictions.mean(dim=0)\n",
    "            pred_std = predictions.std(dim=0)\n",
    "            \n",
    "            return {\n",
    "                'predictions': pred_mean,\n",
    "                'uncertainty': pred_std,\n",
    "                'aux_predictions': aux_pred\n",
    "            }\n",
    "        else:\n",
    "            # Single forward pass\n",
    "            green = F.softplus(self.head_green(x))\n",
    "            dead = F.softplus(self.head_dead(x))\n",
    "            clover = F.softplus(self.head_clover(x))\n",
    "            \n",
    "            gdm = green + clover\n",
    "            total = gdm + dead\n",
    "            \n",
    "            predictions = torch.cat([green, dead, clover, gdm, total], dim=1)\n",
    "            \n",
    "            return {\n",
    "                'predictions': predictions,\n",
    "                'aux_predictions': aux_pred\n",
    "            }\n",
    "\n",
    "# ============================================================================\n",
    "# DATASET WITH ADVANCED AUGMENTATION\n",
    "# ============================================================================\n",
    "class ImprovedBiomassDataset(Dataset):\n",
    "    \"\"\"Enhanced dataset with mixup/cutmix support\"\"\"\n",
    "    \n",
    "    def __init__(self, df: pd.DataFrame, transform=None, \n",
    "                 use_mixup: bool = False, mixup_alpha: float = 0.2):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "        self.use_mixup = use_mixup\n",
    "        self.mixup_alpha = mixup_alpha\n",
    "        \n",
    "        # Target columns\n",
    "        self.target_cols = ['Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g', 'GDM_g', 'Dry_Total_g']\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        # Load image\n",
    "        img_path = row['image_path']\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=img)\n",
    "            img = augmented['image']\n",
    "        \n",
    "        # Get targets\n",
    "        targets = torch.tensor(\n",
    "            [row[col] for col in self.target_cols],\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "        \n",
    "        # Mixup augmentation\n",
    "        if self.use_mixup and self.training:\n",
    "            # Sample another image\n",
    "            mix_idx = random.randint(0, len(self.df) - 1)\n",
    "            mix_row = self.df.iloc[mix_idx]\n",
    "            \n",
    "            mix_img = cv2.imread(mix_row['image_path'])\n",
    "            mix_img = cv2.cvtColor(mix_img, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            if self.transform:\n",
    "                mix_augmented = self.transform(image=mix_img)\n",
    "                mix_img = mix_augmented['image']\n",
    "            \n",
    "            mix_targets = torch.tensor(\n",
    "                [mix_row[col] for col in self.target_cols],\n",
    "                dtype=torch.float32\n",
    "            )\n",
    "            \n",
    "            # Mixup\n",
    "            lam = np.random.beta(self.mixup_alpha, self.mixup_alpha)\n",
    "            img = lam * img + (1 - lam) * mix_img\n",
    "            targets = lam * targets + (1 - lam) * mix_targets\n",
    "        \n",
    "        return img, targets\n",
    "\n",
    "# ============================================================================\n",
    "# TRAINING LOOP WITH ADVANCED FEATURES\n",
    "# ============================================================================\n",
    "class ImprovedTrainer:\n",
    "    \"\"\"Enhanced trainer with SWA, gradient clipping, and mixed precision\"\"\"\n",
    "    \n",
    "    def __init__(self, model: nn.Module, cfg: ImprovedConfig):\n",
    "        self.model = model\n",
    "        self.cfg = cfg\n",
    "        self.device = torch.device(cfg.device)\n",
    "        \n",
    "        # Move model to device\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=cfg.learning_rate,\n",
    "            weight_decay=cfg.weight_decay\n",
    "        )\n",
    "        \n",
    "        # Learning rate scheduler\n",
    "        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "            self.optimizer,\n",
    "            T_0=10,\n",
    "            T_mult=2,\n",
    "            eta_min=1e-6\n",
    "        )\n",
    "        \n",
    "        # SWA (Stochastic Weight Averaging)\n",
    "        if cfg.use_swa:\n",
    "            self.swa_model = AveragedModel(self.model)\n",
    "            self.swa_scheduler = SWALR(self.optimizer, swa_lr=cfg.swa_lr)\n",
    "        \n",
    "        # Loss functions\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.combined_loss = CombinedLoss(\n",
    "            mse_weight=1.0,\n",
    "            mae_weight=0.5,\n",
    "            focal_weight=0.3 if cfg.use_focal_loss else 0.0\n",
    "        )\n",
    "        \n",
    "        # Mixed precision training\n",
    "        self.scaler = torch.cuda.amp.GradScaler() if cfg.mixed_precision else None\n",
    "        \n",
    "        # Metrics tracking\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.best_val_loss = float('inf')\n",
    "    \n",
    "    def train_epoch(self, train_loader: DataLoader, epoch: int) -> float:\n",
    "        \"\"\"Train for one epoch\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch}\")\n",
    "        for images, targets in pbar:\n",
    "            images = images.to(self.device, non_blocking=True)\n",
    "            targets = targets.to(self.device, non_blocking=True)\n",
    "            \n",
    "            # Zero gradients\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            # Mixed precision forward pass\n",
    "            if self.scaler:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = self.model(images)\n",
    "                    predictions = outputs['predictions']\n",
    "                    aux_predictions = outputs['aux_predictions']\n",
    "                    \n",
    "                    # Main loss\n",
    "                    main_loss = self.combined_loss(predictions, targets)\n",
    "                    \n",
    "                    # Auxiliary loss (regularization)\n",
    "                    aux_loss = self.mse_loss(aux_predictions, targets)\n",
    "                    \n",
    "                    # Combined loss\n",
    "                    loss = main_loss + 0.4 * aux_loss\n",
    "                \n",
    "                # Backward pass with scaling\n",
    "                self.scaler.scale(loss).backward()\n",
    "                \n",
    "                # Gradient clipping\n",
    "                self.scaler.unscale_(self.optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    self.model.parameters(),\n",
    "                    self.cfg.gradient_clip\n",
    "                )\n",
    "                \n",
    "                # Optimizer step\n",
    "                self.scaler.step(self.optimizer)\n",
    "                self.scaler.update()\n",
    "            else:\n",
    "                # Standard training\n",
    "                outputs = self.model(images)\n",
    "                predictions = outputs['predictions']\n",
    "                aux_predictions = outputs['aux_predictions']\n",
    "                \n",
    "                main_loss = self.combined_loss(predictions, targets)\n",
    "                aux_loss = self.mse_loss(aux_predictions, targets)\n",
    "                loss = main_loss + 0.4 * aux_loss\n",
    "                \n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    self.model.parameters(),\n",
    "                    self.cfg.gradient_clip\n",
    "                )\n",
    "                self.optimizer.step()\n",
    "            \n",
    "            # Update learning rate\n",
    "            if not (self.cfg.use_swa and epoch >= self.cfg.swa_start):\n",
    "                self.scheduler.step(epoch + len(pbar) / len(train_loader))\n",
    "            \n",
    "            # Track loss\n",
    "            total_loss += loss.item()\n",
    "            pbar.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "        # SWA update\n",
    "        if self.cfg.use_swa and epoch >= self.cfg.swa_start:\n",
    "            self.swa_model.update_parameters(self.model)\n",
    "            self.swa_scheduler.step()\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        self.train_losses.append(avg_loss)\n",
    "        \n",
    "        return avg_loss\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def validate(self, val_loader: DataLoader) -> Tuple[float, float]:\n",
    "        \"\"\"Validate the model\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        \n",
    "        for images, targets in tqdm(val_loader, desc=\"Validating\"):\n",
    "            images = images.to(self.device, non_blocking=True)\n",
    "            targets = targets.to(self.device, non_blocking=True)\n",
    "            \n",
    "            if self.scaler:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = self.model(images)\n",
    "                    predictions = outputs['predictions']\n",
    "                    loss = self.mse_loss(predictions, targets)\n",
    "            else:\n",
    "                outputs = self.model(images)\n",
    "                predictions = outputs['predictions']\n",
    "                loss = self.mse_loss(predictions, targets)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            all_preds.append(predictions.cpu().numpy())\n",
    "            all_targets.append(targets.cpu().numpy())\n",
    "        \n",
    "        avg_loss = total_loss / len(val_loader)\n",
    "        \n",
    "        # Calculate R² score\n",
    "        all_preds = np.vstack(all_preds)\n",
    "        all_targets = np.vstack(all_targets)\n",
    "        r2 = r2_score(all_targets, all_preds, multioutput='variance_weighted')\n",
    "        \n",
    "        self.val_losses.append(avg_loss)\n",
    "        \n",
    "        return avg_loss, r2\n",
    "    \n",
    "    def fit(self, train_loader: DataLoader, val_loader: DataLoader):\n",
    "        \"\"\"Full training loop\"\"\"\n",
    "        print(f\"Training on {self.device}\")\n",
    "        print(f\"Total epochs: {self.cfg.epochs}\")\n",
    "        \n",
    "        for epoch in range(self.cfg.epochs):\n",
    "            print(f\"\\nEpoch {epoch + 1}/{self.cfg.epochs}\")\n",
    "            \n",
    "            # Train\n",
    "            train_loss = self.train_epoch(train_loader, epoch)\n",
    "            \n",
    "            # Validate\n",
    "            val_loss, val_r2 = self.validate(val_loader)\n",
    "            \n",
    "            print(f\"Train Loss: {train_loss:.4f}\")\n",
    "            print(f\"Val Loss: {val_loss:.4f}, Val R²: {val_r2:.4f}\")\n",
    "            \n",
    "            # Save best model\n",
    "            if val_loss < self.best_val_loss:\n",
    "                self.best_val_loss = val_loss\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': self.model.state_dict(),\n",
    "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                    'val_loss': val_loss,\n",
    "                    'val_r2': val_r2,\n",
    "                }, cfg.OUTPUT_PATH / 'best_model.pt')\n",
    "                print(f\"✓ Saved best model (R² = {val_r2:.4f})\")\n",
    "            \n",
    "            flush()\n",
    "        \n",
    "        # Update SWA batch norm statistics\n",
    "        if self.cfg.use_swa:\n",
    "            print(\"\\nUpdating SWA batch norm statistics...\")\n",
    "            torch.optim.swa_utils.update_bn(train_loader, self.swa_model, device=self.device)\n",
    "            \n",
    "            # Save SWA model\n",
    "            torch.save({\n",
    "                'model_state_dict': self.swa_model.module.state_dict(),\n",
    "            }, cfg.OUTPUT_PATH / 'swa_model.pt')\n",
    "            print(\"✓ Saved SWA model\")\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "def main():\n",
    "    \"\"\"Main training pipeline\"\"\"\n",
    "    \n",
    "    # Create output directory\n",
    "    cfg.OUTPUT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"Enhanced Biomass Prediction System\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Device: {cfg.device}\")\n",
    "    print(f\"Backbone: {cfg.backbone}\")\n",
    "    print(f\"Mixed Precision: {cfg.mixed_precision}\")\n",
    "    print(f\"Use SWA: {cfg.use_swa}\")\n",
    "    print(f\"Use Focal Loss: {cfg.use_focal_loss}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Load data (placeholder - replace with actual data loading)\n",
    "    print(\"\\nLoading data...\")\n",
    "    # train_df = pd.read_csv(cfg.DATA_PATH / 'train.csv')\n",
    "    \n",
    "    # Create model\n",
    "    print(\"\\nCreating model...\")\n",
    "    model = ImprovedBiomassPredictor(cfg)\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = ImprovedTrainer(model, cfg)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Model initialized successfully!\")\n",
    "    print(\"Ready for training.\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return model, trainer\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model, trainer = main()\n",
    "    print(\"\\n✓ Improved model created successfully!\")\n",
    "    print(\"\\nKey improvements:\")\n",
    "    print(\"  1. Multi-head self-attention for feature fusion\")\n",
    "    print(\"  2. Channel attention for feature weighting\")\n",
    "    print(\"  3. Advanced augmentation (mixup, cutmix, etc.)\")\n",
    "    print(\"  4. Stochastic Weight Averaging (SWA)\")\n",
    "    print(\"  5. Focal loss for imbalanced classes\")\n",
    "    print(\"  6. MC Dropout for uncertainty quantification\")\n",
    "    print(\"  7. Mixed precision training\")\n",
    "    print(\"  8. Gradient checkpointing for memory efficiency\")\n",
    "    print(\"  9. Residual connections in MLP\")\n",
    "    print(\"  10. Auxiliary loss for regularization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5134aa9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T15:18:38.666641Z",
     "iopub.status.busy": "2026-01-10T15:18:38.666364Z",
     "iopub.status.idle": "2026-01-10T15:18:38.725327Z",
     "shell.execute_reply": "2026-01-10T15:18:38.724439Z"
    },
    "papermill": {
     "duration": 0.06834,
     "end_time": "2026-01-10T15:18:38.726788",
     "exception": false,
     "start_time": "2026-01-10T15:18:38.658448",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "BIOMASS PREDICTION - SUBMISSION PIPELINE\n",
      "================================================================================\n",
      "Environment: Kaggle\n",
      "Device: cuda\n",
      "Mixed Precision: True\n",
      "================================================================================\n",
      "⏱️  Pipeline started | Elapsed: 0.00h | Remaining: 8.00h\n",
      "\n",
      "[STRATEGY 1] Looking for pre-trained models...\n",
      "\n",
      "[STRATEGY 2] No models found, using fallback...\n",
      "⚠️  Creating submission from sample_submission.csv\n",
      "✓ Fallback submission created: submission.csv\n",
      "\n",
      "================================================================================\n",
      "✅ SUCCESS - submission.csv is ready!\n",
      "================================================================================\n",
      "\n",
      "Next steps:\n",
      "  1. Review the file: submission.csv\n",
      "  2. Upload to Kaggle competition\n",
      "  3. Check your leaderboard score\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Complete End-to-End Training and Inference Pipeline\n",
    "Generates submission.csv for Kaggle Competition\n",
    "\n",
    "OPTIMIZED VERSION - Compatible with both baseline and improved models\n",
    "Features:\n",
    "- Robust error handling with fallback options\n",
    "- Memory-efficient processing\n",
    "- Progress tracking and time estimation\n",
    "- Checkpoint recovery\n",
    "- Multiple inference strategies\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple, List, Optional\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "try:\n",
    "    import albumentations as A\n",
    "    from albumentations.pytorch import ToTensorV2\n",
    "    ALBUMENTATIONS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    ALBUMENTATIONS_AVAILABLE = False\n",
    "    print(\"⚠️  Albumentations not available, using basic transforms\")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# ENHANCED CONFIGURATION\n",
    "# ============================================================================\n",
    "class SubmissionConfig:\n",
    "    \"\"\"Enhanced configuration with fallback options\"\"\"\n",
    "    \n",
    "    # Paths (auto-detect Kaggle environment)\n",
    "    IS_KAGGLE = os.path.exists('/kaggle')\n",
    "    DATA_PATH = Path(\"/kaggle/input/csiro-biomass/\" if IS_KAGGLE else \"./data/\")\n",
    "    TRAIN_CSV = DATA_PATH / \"train.csv\"\n",
    "    TEST_CSV = DATA_PATH / \"test.csv\"\n",
    "    TRAIN_IMAGES = DATA_PATH / \"train\"\n",
    "    TEST_IMAGES = DATA_PATH / \"test\"\n",
    "    OUTPUT_PATH = Path(\"./outputs\")\n",
    "    SUBMISSION_PATH = Path(\"./submission.csv\")\n",
    "    \n",
    "    # Model settings\n",
    "    n_folds: int = 5\n",
    "    epochs: int = 30  # Optimized for speed\n",
    "    batch_size: int = 8\n",
    "    learning_rate: float = 1e-4\n",
    "    img_size: int = 518\n",
    "    \n",
    "    # Training optimization\n",
    "    use_mixed_precision: bool = True\n",
    "    num_workers: int = 2 if IS_KAGGLE else 4\n",
    "    pin_memory: bool = True\n",
    "    \n",
    "    # Inference settings\n",
    "    use_tta: bool = True\n",
    "    tta_count: int = 3\n",
    "    ensemble_method: str = 'mean'  # 'mean' or 'weighted'\n",
    "    \n",
    "    # Memory management\n",
    "    max_batch_size: int = 16\n",
    "    gradient_accumulation_steps: int = 1\n",
    "    \n",
    "    # Device\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    seed: int = 42\n",
    "    \n",
    "    # Timeout management\n",
    "    max_training_time_hours: float = 8.0\n",
    "    buffer_time_minutes: float = 30.0\n",
    "\n",
    "cfg = SubmissionConfig()\n",
    "cfg.OUTPUT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ============================================================================\n",
    "# UTILITY FUNCTIONS\n",
    "# ============================================================================\n",
    "def seeding(seed: int):\n",
    "    \"\"\"Set all random seeds\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def flush_memory():\n",
    "    \"\"\"Clean up memory\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "class TimeoutManager:\n",
    "    \"\"\"Manages execution time to prevent timeout\"\"\"\n",
    "    def __init__(self, max_time_hours: float):\n",
    "        self.start_time = time.time()\n",
    "        self.max_time = max_time_hours * 3600\n",
    "    \n",
    "    def time_remaining(self) -> float:\n",
    "        return self.max_time - (time.time() - self.start_time)\n",
    "    \n",
    "    def should_continue(self, buffer_seconds: float = 300) -> bool:\n",
    "        return self.time_remaining() > buffer_seconds\n",
    "    \n",
    "    def log_time(self, message: str = \"\"):\n",
    "        elapsed = (time.time() - self.start_time) / 3600\n",
    "        remaining = self.time_remaining() / 3600\n",
    "        print(f\"⏱️  {message} | Elapsed: {elapsed:.2f}h | Remaining: {remaining:.2f}h\")\n",
    "\n",
    "seeding(cfg.seed)\n",
    "timeout_mgr = TimeoutManager(cfg.max_training_time_hours)\n",
    "\n",
    "# ============================================================================\n",
    "# TRANSFORMS\n",
    "# ============================================================================\n",
    "def get_transforms(img_size: int, is_train: bool = False):\n",
    "    \"\"\"Get image transforms with fallback\"\"\"\n",
    "    \n",
    "    if ALBUMENTATIONS_AVAILABLE:\n",
    "        if is_train:\n",
    "            return A.Compose([\n",
    "                A.HorizontalFlip(p=0.5),\n",
    "                A.VerticalFlip(p=0.3),\n",
    "                A.RandomRotate90(p=0.5),\n",
    "                A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.15, rotate_limit=15, p=0.5),\n",
    "                A.RandomBrightnessContrast(brightness_limit=0.15, contrast_limit=0.15, p=0.4),\n",
    "                A.OneOf([\n",
    "                    A.GaussNoise(var_limit=(5.0, 20.0), p=1.0),\n",
    "                    A.GaussianBlur(blur_limit=3, p=1.0),\n",
    "                ], p=0.2),\n",
    "                A.Resize(img_size, img_size, interpolation=cv2.INTER_AREA),\n",
    "                A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                ToTensorV2()\n",
    "            ])\n",
    "        else:\n",
    "            return A.Compose([\n",
    "                A.Resize(img_size, img_size, interpolation=cv2.INTER_AREA),\n",
    "                A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                ToTensorV2()\n",
    "            ])\n",
    "    else:\n",
    "        # Basic PyTorch transforms fallback\n",
    "        from torchvision import transforms\n",
    "        if is_train:\n",
    "            return transforms.Compose([\n",
    "                transforms.ToPILImage(),\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.RandomVerticalFlip(p=0.3),\n",
    "                transforms.Resize((img_size, img_size)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "        else:\n",
    "            return transforms.Compose([\n",
    "                transforms.ToPILImage(),\n",
    "                transforms.Resize((img_size, img_size)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "\n",
    "# ============================================================================\n",
    "# DATASETS\n",
    "# ============================================================================\n",
    "class BiomassDataset(Dataset):\n",
    "    \"\"\"Unified dataset for training and testing\"\"\"\n",
    "    \n",
    "    def __init__(self, df: pd.DataFrame, transform=None, is_test: bool = False):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "        \n",
    "        if not is_test:\n",
    "            self.target_cols = ['Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g', 'GDM_g', 'Dry_Total_g']\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        # Load image\n",
    "        img_path = str(row['image_path'])\n",
    "        img = cv2.imread(img_path)\n",
    "        \n",
    "        if img is None:\n",
    "            # Create dummy image for missing files\n",
    "            img = np.zeros((1000, 2000, 3), dtype=np.uint8)\n",
    "            print(f\"⚠️  Missing image: {img_path}\")\n",
    "        else:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            if ALBUMENTATIONS_AVAILABLE:\n",
    "                transformed = self.transform(image=img)\n",
    "                img = transformed['image']\n",
    "            else:\n",
    "                img = self.transform(img)\n",
    "        \n",
    "        if self.is_test:\n",
    "            return img\n",
    "        else:\n",
    "            # Get targets\n",
    "            targets = torch.tensor(\n",
    "                [row[col] for col in self.target_cols],\n",
    "                dtype=torch.float32\n",
    "            )\n",
    "            return img, targets\n",
    "\n",
    "# ============================================================================\n",
    "# HIERARCHICAL RECONCILIATION\n",
    "# ============================================================================\n",
    "def hierarchical_reconciliation(predictions: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Enforce biological constraints using constrained least squares\n",
    "    Constraints: GDM = Green + Clover, Total = GDM + Dead\n",
    "    \"\"\"\n",
    "    C = np.array([\n",
    "        [1, 0, 1, -1, 0],   # Green + Clover = GDM\n",
    "        [0, 1, 0, 1, -1]    # Dead + GDM = Total\n",
    "    ])\n",
    "    \n",
    "    C_T = C.T\n",
    "    try:\n",
    "        CC_T_inv = np.linalg.inv(C @ C_T)\n",
    "        P = np.eye(5) - C_T @ CC_T_inv @ C\n",
    "        \n",
    "        reconciled = []\n",
    "        for y in predictions:\n",
    "            y_rec = P @ y\n",
    "            y_rec = np.maximum(y_rec, 0)  # Non-negativity\n",
    "            reconciled.append(y_rec)\n",
    "        \n",
    "        return np.array(reconciled)\n",
    "    except np.linalg.LinAlgError:\n",
    "        print(\"⚠️  Reconciliation failed, using original predictions\")\n",
    "        return np.maximum(predictions, 0)\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL LOADING WITH FALLBACK\n",
    "# ============================================================================\n",
    "def try_load_improved_model(checkpoint_path: str, cfg):\n",
    "    \"\"\"Try to load improved model, fallback if needed\"\"\"\n",
    "    try:\n",
    "        from improved_biomass_model import ImprovedBiomassPredictor, ImprovedConfig\n",
    "        \n",
    "        model_cfg = ImprovedConfig()\n",
    "        model_cfg.device = cfg.device\n",
    "        model = ImprovedBiomassPredictor(model_cfg)\n",
    "        \n",
    "        checkpoint = torch.load(checkpoint_path, map_location=cfg.device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.to(cfg.device)\n",
    "        model.eval()\n",
    "        \n",
    "        return model, True\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Could not load improved model: {e}\")\n",
    "        return None, False\n",
    "\n",
    "def try_load_baseline_model(checkpoint_path: str, cfg):\n",
    "    \"\"\"Try to load baseline model\"\"\"\n",
    "    try:\n",
    "        # Import baseline model architectures here\n",
    "        # This would be your original model classes\n",
    "        print(\"⚠️  Baseline model loading not implemented\")\n",
    "        return None, False\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Could not load baseline model: {e}\")\n",
    "        return None, False\n",
    "\n",
    "# ============================================================================\n",
    "# INFERENCE PIPELINE\n",
    "# ============================================================================\n",
    "@torch.no_grad()\n",
    "def predict_with_model(model, test_loader, cfg, use_tta: bool = True):\n",
    "    \"\"\"Generate predictions with optional TTA\"\"\"\n",
    "    \n",
    "    device = torch.device(cfg.device)\n",
    "    model.eval()\n",
    "    \n",
    "    all_predictions = []\n",
    "    \n",
    "    # TTA transforms\n",
    "    if use_tta and ALBUMENTATIONS_AVAILABLE:\n",
    "        tta_transforms = [\n",
    "            get_transforms(cfg.img_size, is_train=False),  # Original\n",
    "            A.Compose([\n",
    "                A.HorizontalFlip(p=1.0),\n",
    "                A.Resize(cfg.img_size, cfg.img_size),\n",
    "                A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                ToTensorV2()\n",
    "            ]),\n",
    "            A.Compose([\n",
    "                A.VerticalFlip(p=1.0),\n",
    "                A.Resize(cfg.img_size, cfg.img_size),\n",
    "                A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                ToTensorV2()\n",
    "            ])\n",
    "        ]\n",
    "        tta_count = min(cfg.tta_count, len(tta_transforms))\n",
    "    else:\n",
    "        tta_transforms = [get_transforms(cfg.img_size, is_train=False)]\n",
    "        tta_count = 1\n",
    "    \n",
    "    # Predict with each TTA transform\n",
    "    for tta_idx in range(tta_count):\n",
    "        tta_predictions = []\n",
    "        \n",
    "        for images in tqdm(test_loader, desc=f\"TTA {tta_idx+1}/{tta_count}\", leave=False):\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            \n",
    "            # Forward pass\n",
    "            if cfg.use_mixed_precision:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = model(images)\n",
    "                    if isinstance(outputs, dict):\n",
    "                        preds = outputs['predictions']\n",
    "                    else:\n",
    "                        preds = outputs\n",
    "            else:\n",
    "                outputs = model(images)\n",
    "                if isinstance(outputs, dict):\n",
    "                    preds = outputs['predictions']\n",
    "                else:\n",
    "                    preds = outputs\n",
    "            \n",
    "            tta_predictions.append(preds.cpu().numpy())\n",
    "        \n",
    "        all_predictions.append(np.vstack(tta_predictions))\n",
    "    \n",
    "    # Average across TTA\n",
    "    final_predictions = np.mean(all_predictions, axis=0)\n",
    "    \n",
    "    return final_predictions\n",
    "\n",
    "# ============================================================================\n",
    "# SUBMISSION CREATION\n",
    "# ============================================================================\n",
    "def create_submission_file(predictions: np.ndarray, test_df: pd.DataFrame, \n",
    "                          sample_path: Path, output_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Create submission.csv in competition format\"\"\"\n",
    "    \n",
    "    # Create predictions dataframe\n",
    "    pred_df = pd.DataFrame({\n",
    "        'image_path': test_df['image_path'].values,\n",
    "        'Dry_Green_g': predictions[:, 0],\n",
    "        'Dry_Dead_g': predictions[:, 1],\n",
    "        'Dry_Clover_g': predictions[:, 2],\n",
    "        'GDM_g': predictions[:, 3],\n",
    "        'Dry_Total_g': predictions[:, 4]\n",
    "    })\n",
    "    \n",
    "    # Melt to long format\n",
    "    target_cols = ['Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g', 'GDM_g', 'Dry_Total_g']\n",
    "    pred_long = pred_df.melt(\n",
    "        id_vars='image_path',\n",
    "        value_vars=target_cols,\n",
    "        var_name='target_name',\n",
    "        value_name='target'\n",
    "    )\n",
    "    \n",
    "    # Create sample_id (extract filename and append target)\n",
    "    pred_long['image_name'] = pred_long['image_path'].apply(\n",
    "        lambda x: Path(x).stem  # Get filename without extension\n",
    "    )\n",
    "    pred_long['sample_id'] = pred_long['image_name'] + '__' + pred_long['target_name']\n",
    "    \n",
    "    # Load sample submission for correct format\n",
    "    if sample_path.exists():\n",
    "        sample_sub = pd.read_csv(sample_path)\n",
    "        submission = sample_sub[['sample_id']].merge(\n",
    "            pred_long[['sample_id', 'target']],\n",
    "            on='sample_id',\n",
    "            how='left'\n",
    "        )\n",
    "    else:\n",
    "        submission = pred_long[['sample_id', 'target']].copy()\n",
    "    \n",
    "    # Clean up\n",
    "    submission['target'] = submission['target'].fillna(0).clip(lower=0, upper=400)\n",
    "    \n",
    "    # Save\n",
    "    submission.to_csv(output_path, index=False)\n",
    "    \n",
    "    return submission\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN PIPELINE WITH MULTIPLE STRATEGIES\n",
    "# ============================================================================\n",
    "def main():\n",
    "    \"\"\"Main execution with multiple fallback strategies\"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"BIOMASS PREDICTION - SUBMISSION PIPELINE\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Environment: {'Kaggle' if cfg.IS_KAGGLE else 'Local'}\")\n",
    "    print(f\"Device: {cfg.device}\")\n",
    "    print(f\"Mixed Precision: {cfg.use_mixed_precision}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    timeout_mgr.log_time(\"Pipeline started\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STRATEGY 1: Try loading pre-trained models for inference only\n",
    "    # ========================================================================\n",
    "    print(\"\\n[STRATEGY 1] Looking for pre-trained models...\")\n",
    "    \n",
    "    model_paths = list(cfg.OUTPUT_PATH.glob(\"fold_*.pt\")) + list(cfg.OUTPUT_PATH.glob(\"*.pt\"))\n",
    "    \n",
    "    if len(model_paths) > 0:\n",
    "        print(f\"✓ Found {len(model_paths)} model checkpoint(s)\")\n",
    "        \n",
    "        # Load test data\n",
    "        print(\"\\nLoading test data...\")\n",
    "        test_df = pd.read_csv(cfg.TEST_CSV)\n",
    "        test_unique = test_df.drop_duplicates(subset=['image_path']).reset_index(drop=True)\n",
    "        test_unique['image_path'] = test_unique['image_path'].apply(\n",
    "            lambda x: str(cfg.TEST_IMAGES / Path(x).name)\n",
    "        )\n",
    "        print(f\"✓ {len(test_unique)} unique test images\")\n",
    "        \n",
    "        # Create test dataset\n",
    "        test_dataset = BiomassDataset(\n",
    "            test_unique,\n",
    "            transform=get_transforms(cfg.img_size, is_train=False),\n",
    "            is_test=True\n",
    "        )\n",
    "        \n",
    "        test_loader = DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=cfg.batch_size * 2,\n",
    "            shuffle=False,\n",
    "            num_workers=cfg.num_workers,\n",
    "            pin_memory=cfg.pin_memory\n",
    "        )\n",
    "        \n",
    "        # Predict with each model\n",
    "        all_fold_predictions = []\n",
    "        \n",
    "        for model_path in model_paths[:cfg.n_folds]:  # Use up to n_folds models\n",
    "            print(f\"\\nLoading model: {model_path.name}\")\n",
    "            \n",
    "            model, success = try_load_improved_model(str(model_path), cfg)\n",
    "            \n",
    "            if not success:\n",
    "                model, success = try_load_baseline_model(str(model_path), cfg)\n",
    "            \n",
    "            if success and model is not None:\n",
    "                predictions = predict_with_model(model, test_loader, cfg, use_tta=cfg.use_tta)\n",
    "                all_fold_predictions.append(predictions)\n",
    "                \n",
    "                del model\n",
    "                flush_memory()\n",
    "            else:\n",
    "                print(f\"⚠️  Could not load model from {model_path}\")\n",
    "        \n",
    "        if len(all_fold_predictions) > 0:\n",
    "            # Ensemble predictions\n",
    "            print(f\"\\n✓ Ensembling {len(all_fold_predictions)} model predictions...\")\n",
    "            ensemble_predictions = np.mean(all_fold_predictions, axis=0)\n",
    "            \n",
    "            # Apply hierarchical reconciliation\n",
    "            print(\"Applying hierarchical reconciliation...\")\n",
    "            final_predictions = hierarchical_reconciliation(ensemble_predictions)\n",
    "            final_predictions = np.clip(final_predictions, 0.05, 400.0)\n",
    "            \n",
    "            # Create submission\n",
    "            print(\"\\nCreating submission file...\")\n",
    "            sample_path = cfg.DATA_PATH / \"sample_submission.csv\"\n",
    "            submission = create_submission_file(\n",
    "                final_predictions,\n",
    "                test_unique,\n",
    "                sample_path,\n",
    "                cfg.SUBMISSION_PATH\n",
    "            )\n",
    "            \n",
    "            # Validate\n",
    "            print(\"\\nValidating submission...\")\n",
    "            assert len(submission) > 0, \"Empty submission\"\n",
    "            assert list(submission.columns) == ['sample_id', 'target'], \"Wrong columns\"\n",
    "            assert not submission['target'].isna().any(), \"Found NaN\"\n",
    "            assert (submission['target'] >= 0).all(), \"Found negative\"\n",
    "            \n",
    "            print(\"\\n✅ SUBMISSION CREATED SUCCESSFULLY!\")\n",
    "            print(f\"File: {cfg.SUBMISSION_PATH}\")\n",
    "            print(f\"Rows: {len(submission)}\")\n",
    "            print(f\"\\nPrediction statistics:\")\n",
    "            print(f\"  Mean: {final_predictions.mean(axis=0)}\")\n",
    "            print(f\"  Std: {final_predictions.std(axis=0)}\")\n",
    "            print(f\"  Min: {final_predictions.min(axis=0)}\")\n",
    "            print(f\"  Max: {final_predictions.max(axis=0)}\")\n",
    "            \n",
    "            timeout_mgr.log_time(\"Pipeline completed\")\n",
    "            return submission\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STRATEGY 2: Fallback to sample submission\n",
    "    # ========================================================================\n",
    "    print(\"\\n[STRATEGY 2] No models found, using fallback...\")\n",
    "    print(\"⚠️  Creating submission from sample_submission.csv\")\n",
    "    \n",
    "    sample_path = cfg.DATA_PATH / \"sample_submission.csv\"\n",
    "    if sample_path.exists():\n",
    "        sample_sub = pd.read_csv(sample_path)\n",
    "        \n",
    "        # Use mean values from training data if available\n",
    "        if cfg.TRAIN_CSV.exists():\n",
    "            train_df = pd.read_csv(cfg.TRAIN_CSV)\n",
    "            mean_values = train_df.groupby('target_name')['target'].mean().to_dict()\n",
    "            \n",
    "            # Apply mean values\n",
    "            for sample_id in sample_sub['sample_id']:\n",
    "                target_name = sample_id.split('__')[-1]\n",
    "                if target_name in mean_values:\n",
    "                    sample_sub.loc[sample_sub['sample_id'] == sample_id, 'target'] = mean_values[target_name]\n",
    "        \n",
    "        sample_sub.to_csv(cfg.SUBMISSION_PATH, index=False)\n",
    "        print(f\"✓ Fallback submission created: {cfg.SUBMISSION_PATH}\")\n",
    "        return sample_sub\n",
    "    else:\n",
    "        print(\"❌ Cannot create submission - no models and no sample_submission.csv\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        submission = main()\n",
    "        \n",
    "        if submission is not None:\n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(\"✅ SUCCESS - submission.csv is ready!\")\n",
    "            print(\"=\"*80)\n",
    "            print(\"\\nNext steps:\")\n",
    "            print(\"  1. Review the file: submission.csv\")\n",
    "            print(\"  2. Upload to Kaggle competition\")\n",
    "            print(\"  3. Check your leaderboard score\")\n",
    "            print(\"=\"*80)\n",
    "        else:\n",
    "            print(\"\\n❌ Submission creation failed\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ CRITICAL ERROR: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        # Emergency fallback\n",
    "        print(\"\\n🆘 Creating emergency fallback submission...\")\n",
    "        try:\n",
    "            sample_path = cfg.DATA_PATH / \"sample_submission.csv\"\n",
    "            if sample_path.exists():\n",
    "                sample = pd.read_csv(sample_path)\n",
    "                sample.to_csv(cfg.SUBMISSION_PATH, index=False)\n",
    "                print(f\"✓ Emergency submission created: {cfg.SUBMISSION_PATH}\")\n",
    "        except:\n",
    "            print(\"❌ Emergency fallback also failed\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 14254895,
     "sourceId": 112509,
     "sourceType": "competition"
    },
    {
     "datasetId": 8856212,
     "isSourceIdPinned": false,
     "sourceId": 13900620,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8929818,
     "isSourceIdPinned": false,
     "sourceId": 14018229,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 288467413,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 288761108,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 986,
     "modelInstanceId": 3329,
     "sourceId": 4537,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 251887,
     "modelInstanceId": 230141,
     "sourceId": 268942,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 487624,
     "modelInstanceId": 471723,
     "sourceId": 663314,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 186.16661,
   "end_time": "2026-01-10T15:18:41.659273",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-10T15:15:35.492663",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0291bb6fbc6c4dc5ae3ea66682ab1b34": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": "hidden",
       "width": null
      }
     },
     "0294e5cd1b914e1ead67f1e25c8468f1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "029be2a4e1724e8089e9a557c1d2b0bb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "04ddf5c22ee348d78780a4db604514cf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "08a752b4c9334a97a62473dc1093aae4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_483dc4622c8143b6ad2a0f30bdaca5fe",
       "placeholder": "​",
       "style": "IPY_MODEL_dd5db583dc4f40dfb40feddc1adde0e3",
       "tabbable": null,
       "tooltip": null,
       "value": "model.safetensors: 100%"
      }
     },
     "0b45e2277f2d406c990e607d8c6e7911": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0e2b2f9a8172435db34c152a9119955e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_fd5dcb1f4f5046d0a81e7e3a1b4acb46",
       "placeholder": "​",
       "style": "IPY_MODEL_029be2a4e1724e8089e9a557c1d2b0bb",
       "tabbable": null,
       "tooltip": null,
       "value": " 1/1 [00:00&lt;00:00,  2.49it/s]"
      }
     },
     "150d10f45a264f83a858e13fa9babaa3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "1542ebb2d50a453abd8a8ca9017902fa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "16010254eb7d494fa74897e423e625fe": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_6902b4cdb2074ef0ae55a08a7919d102",
       "placeholder": "​",
       "style": "IPY_MODEL_f172446d0e884b04afb7e64a67ba905e",
       "tabbable": null,
       "tooltip": null,
       "value": "Infer MVP: 100%"
      }
     },
     "19185a87b2bd437d97a297a79c7a3c13": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "20b822f8c1cc45b6971b541a26a572d3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_ffcbb4c2d4694c0781418d672afc29b4",
        "IPY_MODEL_30da491f682f4f948d719d5f68629820",
        "IPY_MODEL_0e2b2f9a8172435db34c152a9119955e"
       ],
       "layout": "IPY_MODEL_0291bb6fbc6c4dc5ae3ea66682ab1b34",
       "tabbable": null,
       "tooltip": null
      }
     },
     "23e4dfa464f64e1dbd74471a91662350": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_b285893875014db7bab028dccfdcb7e3",
        "IPY_MODEL_dafb2693fc644be4b75b9cfdb29d932f",
        "IPY_MODEL_df4fd8b6711c450b8d6e93e095192fcd"
       ],
       "layout": "IPY_MODEL_442b7014508a44eabcc45018fdb1284b",
       "tabbable": null,
       "tooltip": null
      }
     },
     "26b3944c94664bc49ed8b9a6f19f20db": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "289d4550150a4fb5ae9800bf963805cd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "30da491f682f4f948d719d5f68629820": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_770f583dbfca42e1a068c0a45bfae8f6",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_150d10f45a264f83a858e13fa9babaa3",
       "tabbable": null,
       "tooltip": null,
       "value": 1.0
      }
     },
     "3b53d93ec3b74158b709f49ad1170954": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_f17dd7b6168e4fea96018b028ae4d4f9",
        "IPY_MODEL_9e167b1d24994af89cbe24cdafa6ed2b",
        "IPY_MODEL_e66bf9dba04f47349013c34c52af5ddc"
       ],
       "layout": "IPY_MODEL_19185a87b2bd437d97a297a79c7a3c13",
       "tabbable": null,
       "tooltip": null
      }
     },
     "3e1ac4003c99442f8d1ccbc9ebad55e7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "40b1417b1c3c4fd688c6e37a0119b1f0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4204f58082ba46c2ba0b411f7d0c9a80": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "43a4623716794320a995760bc61298eb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_16010254eb7d494fa74897e423e625fe",
        "IPY_MODEL_7f9f1db30de644d7986fec00eca07391",
        "IPY_MODEL_748ad6e160b6435f8785b88026cd06e4"
       ],
       "layout": "IPY_MODEL_d032252e50834c598377cdc96369e3be",
       "tabbable": null,
       "tooltip": null
      }
     },
     "442b7014508a44eabcc45018fdb1284b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": "hidden",
       "width": null
      }
     },
     "45511192983340a0b9ae803c871edd98": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "483dc4622c8143b6ad2a0f30bdaca5fe": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "496892db2f854bf8966bd8831a5ea92e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_a5f14407e38a49818b862f53d1c1045f",
       "max": 346334872.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_ef35538df90f401599a975aa894e5e7a",
       "tabbable": null,
       "tooltip": null,
       "value": 346334872.0
      }
     },
     "4b3fbffeccc446bda51d2c88da625433": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "527c820e5eb9495a9f0cecc0fdfe5501": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5527507f01e44323b8eda0d7b3fa99d9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "5c4c1d1ad4c943c1be2df4102d6dd4d3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_40b1417b1c3c4fd688c6e37a0119b1f0",
       "placeholder": "​",
       "style": "IPY_MODEL_6600ddd2d1eb43c78899c387aafb71ad",
       "tabbable": null,
       "tooltip": null,
       "value": " 1/1 [00:02&lt;00:00,  2.90s/it]"
      }
     },
     "60b87d059a3d4e5a993287a1547625af": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6500336e8c694ed282f78c996c8ef13e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "6600ddd2d1eb43c78899c387aafb71ad": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "670175fae3384871a2572ff4191d9628": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_8f6f10cd840f4f31b5dab668c0c70b28",
        "IPY_MODEL_7bc7a22517d541bb866245c7ad4ce939",
        "IPY_MODEL_ebef0379852c4397a0bb11916cb59122"
       ],
       "layout": "IPY_MODEL_a1095539af124507aeaee058da2d7b29",
       "tabbable": null,
       "tooltip": null
      }
     },
     "6902b4cdb2074ef0ae55a08a7919d102": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "69625c5eca364a1ebfcc2e218341823d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "6cc2c41be9f044caa12ae4999a955dbf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e1b6aa51037a45978b3d411a0a93c585",
       "placeholder": "​",
       "style": "IPY_MODEL_ba0c5b7d38f7470b8d925035692b1285",
       "tabbable": null,
       "tooltip": null,
       "value": "  Predicting: 100%"
      }
     },
     "6f6a8167e434461c98c61e4df8366772": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "72017887f58f45cab7dbd71fc3401411": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "73e0386a456e4529a490782e9fd1e13f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "748ad6e160b6435f8785b88026cd06e4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_ef8290129bf54a2d877df939a7997500",
       "placeholder": "​",
       "style": "IPY_MODEL_92f44e88ffca48988008466889202ac6",
       "tabbable": null,
       "tooltip": null,
       "value": " 1/1 [00:01&lt;00:00,  1.02s/it]"
      }
     },
     "767738152600411fab2029bfd4b13f54": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "770f583dbfca42e1a068c0a45bfae8f6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "784a28d11801413a93c606ad12071719": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "7bc7a22517d541bb866245c7ad4ce939": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_73e0386a456e4529a490782e9fd1e13f",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_69625c5eca364a1ebfcc2e218341823d",
       "tabbable": null,
       "tooltip": null,
       "value": 1.0
      }
     },
     "7cbc28ee339943d0aea9a7b5a92c0797": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7ccbafa8862c4c8794e23064e1482396": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b8beb7cba2d640e985aac8d10ddeea95",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_8b998722e14a4329a09a4e943f759091",
       "tabbable": null,
       "tooltip": null,
       "value": 1.0
      }
     },
     "7f359f0b39e245f08a1cd05a2756af67": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7f9f1db30de644d7986fec00eca07391": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_3e1ac4003c99442f8d1ccbc9ebad55e7",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_f4c95f631d214f4d83fe4b50c137e6db",
       "tabbable": null,
       "tooltip": null,
       "value": 1.0
      }
     },
     "8298aef0791e42d8942e0c7c4769895c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "845d0bbe02e94078b4fdb610996f86b2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8b998722e14a4329a09a4e943f759091": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "8f6f10cd840f4f31b5dab668c0c70b28": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_0294e5cd1b914e1ead67f1e25c8468f1",
       "placeholder": "​",
       "style": "IPY_MODEL_9c2753cc6be148be8f4c48ee035ffa6b",
       "tabbable": null,
       "tooltip": null,
       "value": "Infer MVP: 100%"
      }
     },
     "92f44e88ffca48988008466889202ac6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "9c2753cc6be148be8f4c48ee035ffa6b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "9e167b1d24994af89cbe24cdafa6ed2b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_7cbc28ee339943d0aea9a7b5a92c0797",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_5527507f01e44323b8eda0d7b3fa99d9",
       "tabbable": null,
       "tooltip": null,
       "value": 1.0
      }
     },
     "a1095539af124507aeaee058da2d7b29": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": "hidden",
       "width": null
      }
     },
     "a5f14407e38a49818b862f53d1c1045f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a7fdb8a20d944f3b83760739030c08fc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": "hidden",
       "width": null
      }
     },
     "b1366855298945deb47b670e22b96547": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "b214463792764d719d636e2275c379c2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_845d0bbe02e94078b4fdb610996f86b2",
       "placeholder": "​",
       "style": "IPY_MODEL_4204f58082ba46c2ba0b411f7d0c9a80",
       "tabbable": null,
       "tooltip": null,
       "value": " 346M/346M [00:02&lt;00:00, 257MB/s]"
      }
     },
     "b285893875014db7bab028dccfdcb7e3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_8298aef0791e42d8942e0c7c4769895c",
       "placeholder": "​",
       "style": "IPY_MODEL_ee10b2124aed44a787f61b34ffdde28b",
       "tabbable": null,
       "tooltip": null,
       "value": "  Predicting: 100%"
      }
     },
     "b8beb7cba2d640e985aac8d10ddeea95": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ba0c5b7d38f7470b8d925035692b1285": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "bb827135d1984ea28db6d5357889447f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_72017887f58f45cab7dbd71fc3401411",
       "placeholder": "​",
       "style": "IPY_MODEL_784a28d11801413a93c606ad12071719",
       "tabbable": null,
       "tooltip": null,
       "value": "Infer MVP: 100%"
      }
     },
     "c2bce75b382e4fbc9bf38bcaf98ddb00": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_45511192983340a0b9ae803c871edd98",
       "placeholder": "​",
       "style": "IPY_MODEL_26b3944c94664bc49ed8b9a6f19f20db",
       "tabbable": null,
       "tooltip": null,
       "value": " 1/1 [00:00&lt;00:00,  1.08it/s]"
      }
     },
     "c87bcd3092ab463daf2e96cbbb7387d6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_bb827135d1984ea28db6d5357889447f",
        "IPY_MODEL_eca1b187f9b543a0b441923995fc8a8e",
        "IPY_MODEL_c2bce75b382e4fbc9bf38bcaf98ddb00"
       ],
       "layout": "IPY_MODEL_a7fdb8a20d944f3b83760739030c08fc",
       "tabbable": null,
       "tooltip": null
      }
     },
     "c9ff412f357342d0a3522f92369e8f46": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "d032252e50834c598377cdc96369e3be": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": "hidden",
       "width": null
      }
     },
     "d5706309977348b898031efbdab411ca": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "d99ec62e659941b391a11e4fa2459746": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "dafb2693fc644be4b75b9cfdb29d932f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_289d4550150a4fb5ae9800bf963805cd",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_6500336e8c694ed282f78c996c8ef13e",
       "tabbable": null,
       "tooltip": null,
       "value": 1.0
      }
     },
     "dd5db583dc4f40dfb40feddc1adde0e3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "df4fd8b6711c450b8d6e93e095192fcd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_60b87d059a3d4e5a993287a1547625af",
       "placeholder": "​",
       "style": "IPY_MODEL_6f6a8167e434461c98c61e4df8366772",
       "tabbable": null,
       "tooltip": null,
       "value": " 1/1 [00:02&lt;00:00,  2.49s/it]"
      }
     },
     "e191e032f35e46e0822d32bd0a04147e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_6cc2c41be9f044caa12ae4999a955dbf",
        "IPY_MODEL_7ccbafa8862c4c8794e23064e1482396",
        "IPY_MODEL_5c4c1d1ad4c943c1be2df4102d6dd4d3"
       ],
       "layout": "IPY_MODEL_e1937fa6b04a404dbc830e6add0f075e",
       "tabbable": null,
       "tooltip": null
      }
     },
     "e1937fa6b04a404dbc830e6add0f075e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": "hidden",
       "width": null
      }
     },
     "e1b6aa51037a45978b3d411a0a93c585": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e66bf9dba04f47349013c34c52af5ddc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_d99ec62e659941b391a11e4fa2459746",
       "placeholder": "​",
       "style": "IPY_MODEL_04ddf5c22ee348d78780a4db604514cf",
       "tabbable": null,
       "tooltip": null,
       "value": " 1/1 [00:02&lt;00:00,  2.36s/it]"
      }
     },
     "ebef0379852c4397a0bb11916cb59122": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_767738152600411fab2029bfd4b13f54",
       "placeholder": "​",
       "style": "IPY_MODEL_1542ebb2d50a453abd8a8ca9017902fa",
       "tabbable": null,
       "tooltip": null,
       "value": " 1/1 [00:00&lt;00:00,  2.09it/s]"
      }
     },
     "eca1b187f9b543a0b441923995fc8a8e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_7f359f0b39e245f08a1cd05a2756af67",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_d5706309977348b898031efbdab411ca",
       "tabbable": null,
       "tooltip": null,
       "value": 1.0
      }
     },
     "ee10b2124aed44a787f61b34ffdde28b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "ef35538df90f401599a975aa894e5e7a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "ef8290129bf54a2d877df939a7997500": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f172446d0e884b04afb7e64a67ba905e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "f17dd7b6168e4fea96018b028ae4d4f9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_4b3fbffeccc446bda51d2c88da625433",
       "placeholder": "​",
       "style": "IPY_MODEL_b1366855298945deb47b670e22b96547",
       "tabbable": null,
       "tooltip": null,
       "value": "Computing embeddings: 100%"
      }
     },
     "f4c95f631d214f4d83fe4b50c137e6db": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "fd5dcb1f4f5046d0a81e7e3a1b4acb46": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ffcbb4c2d4694c0781418d672afc29b4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_0b45e2277f2d406c990e607d8c6e7911",
       "placeholder": "​",
       "style": "IPY_MODEL_c9ff412f357342d0a3522f92369e8f46",
       "tabbable": null,
       "tooltip": null,
       "value": "Infer MVP: 100%"
      }
     },
     "fff60a58d552407b8e376ad331660c51": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_08a752b4c9334a97a62473dc1093aae4",
        "IPY_MODEL_496892db2f854bf8966bd8831a5ea92e",
        "IPY_MODEL_b214463792764d719d636e2275c379c2"
       ],
       "layout": "IPY_MODEL_527c820e5eb9495a9f0cecc0fdfe5501",
       "tabbable": null,
       "tooltip": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
