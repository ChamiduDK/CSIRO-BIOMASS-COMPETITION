{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "643f7235",
   "metadata": {
    "_cell_guid": "b2365a37-3391-4fa2-a8f3-e905c89376e4",
    "_uuid": "32dbf5ec-5918-43e2-9c71-b3bc27d0f6f2",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-01-15T12:06:15.535321Z",
     "iopub.status.busy": "2026-01-15T12:06:15.534760Z",
     "iopub.status.idle": "2026-01-15T12:06:16.647105Z",
     "shell.execute_reply": "2026-01-15T12:06:16.646367Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 1.118779,
     "end_time": "2026-01-15T12:06:16.649229",
     "exception": false,
     "start_time": "2026-01-15T12:06:15.530450",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/csiro-biomass/sample_submission.csv\n",
      "/kaggle/input/csiro-biomass/train.csv\n",
      "/kaggle/input/csiro-biomass/test.csv\n",
      "/kaggle/input/csiro-biomass/test/ID1001187975.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2099464826.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2037861084.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1211362607.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1853508321.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID193102215.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID698608346.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1859251563.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1880764911.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID853954911.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1403107574.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1781353117.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID384648061.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1563418511.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2125100696.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID482555369.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID638711343.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID779628955.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1876271942.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1692894460.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID746335827.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1136169672.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1471216911.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID846154859.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1294770420.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1183807388.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID423506847.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1889150649.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1140993511.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1413758094.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1545077474.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID95050718.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID528010569.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1645161155.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID786365141.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID896386823.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1025234388.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID663006174.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1509266870.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1496750796.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID471758347.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID740402124.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1624268863.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1098771283.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID710341728.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2086966681.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1573329652.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID54128926.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID50027657.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1559189397.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID290369222.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1590632667.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID552040066.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID488873801.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID363069566.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1839139621.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1131079710.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2010625680.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID152157478.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1357758282.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1498398599.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID679913293.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID697718693.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID4464212.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1275072698.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1579942839.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID799079114.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1415329644.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1510574031.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1078930021.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1456861072.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID930534670.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID13162390.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID567744300.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID344618040.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID566966892.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1437386574.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID667059550.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID72895391.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1193692654.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1386202352.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID871463897.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2096636211.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2003438517.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID21377800.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID230058600.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1753847361.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1512751450.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID12390962.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1746343319.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID978026131.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID383231615.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID146920896.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1036339023.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1168534540.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1859792585.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1251029854.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1113329413.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1874904894.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1671844336.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1831254380.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1103883611.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID797502182.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1784585001.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1058383417.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1488408526.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID429799190.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1291116815.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1516374298.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1618597318.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1345375788.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID686797154.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1139866256.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1149598723.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID212206250.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID112966473.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1540480250.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID544444725.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1513184765.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID668330410.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1444674500.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1962379474.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID605134229.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID914754166.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID354528442.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID950496197.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1395011773.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1357768767.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID210865340.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID936984905.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1976436386.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1215977190.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID803479541.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1244346858.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID158170916.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1208644039.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1314135397.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1012260530.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1053972079.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID656251220.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1084819986.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1337107565.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1268934251.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID617132135.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1472525822.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID668475812.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID681680726.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1476045099.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1570190541.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1403078396.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2030696575.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1782608354.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID194823383.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID196516535.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID212206832.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1638922597.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1457700382.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1989506559.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID789169173.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1634731537.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1428837636.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2006686196.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID885388135.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1789853061.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1655778545.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID697059386.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID121331988.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2099742797.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID342818398.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID317990700.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID706288721.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1159071020.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID755710743.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1254829053.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID475010202.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1693880739.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1894998379.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID48303557.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1385921939.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID147528735.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID407646960.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1035947949.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1119761112.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1988033238.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1857489997.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID742198710.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID588120964.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID431471530.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID353424190.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID380752847.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2069766023.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID600602588.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID560946727.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1011485656.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID808079729.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1217108125.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1623964968.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID980878870.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID793526563.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID397994621.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID975115267.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1237349078.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID684383343.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID866684633.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1665142816.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2048645043.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1953171547.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1451025862.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID71885430.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID307060225.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID969218269.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID980538882.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1028611175.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID670276799.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2002797732.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1374789439.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID473494649.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1993907137.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1962197151.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID828217731.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID972274220.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1954669045.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1354190372.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1458758610.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID40849327.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1952813879.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID572336285.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1473228876.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1963715583.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1463690813.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1899025384.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID386216505.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1789265307.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID315357834.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2089023774.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID520514019.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1970522802.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1139918758.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1051144034.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1370004842.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID761508093.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2052993274.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1277756619.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID6269659.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1574125908.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID135365668.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1182523622.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID554314721.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1049634115.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1127246618.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID900012207.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID574213894.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID415656958.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID61833032.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2053315094.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID550623196.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID657448172.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1675365449.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2014192906.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID162394992.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID968643034.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID684062938.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID802547515.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID294150104.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1618145129.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID956512130.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID142751858.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID325799913.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID443091455.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID661372352.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1062837331.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID498304885.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID187238869.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1450399782.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2056023629.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID576621307.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1199150612.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1411613934.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID105271783.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1703304524.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID875119737.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1176292407.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1729002155.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2091439402.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID576137678.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1946311744.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1982662138.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID983582017.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID661817669.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID753699705.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1789834546.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID529933668.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID490139972.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID743847993.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID7850481.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1088965591.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID629980789.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1119739385.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1477176296.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1113121340.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2131261930.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2145635095.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1414371018.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1148666289.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID839432753.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID157479394.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1761544403.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID846984946.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID751517087.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID577112774.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID353997899.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID748979397.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1070112260.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1108283583.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1868719645.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1980675327.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1163061745.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1148528732.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID534966093.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1717006117.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1953218650.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID633775166.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID808093827.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1997244125.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1920959057.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1948354837.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID364856705.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID249042826.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID332742639.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1680597197.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1421714468.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID905397692.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1782509721.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID141370843.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2056982009.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID94564238.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID8209776.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID908524512.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID610397481.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID750820644.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1515990019.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1547945326.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID587125778.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1620371305.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1474775613.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID545360459.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1783499590.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1249094008.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1525817840.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID227847873.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1052620238.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1888700589.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2052442675.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID963903358.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1121692672.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1343327476.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1667778338.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID257822026.jpg\n",
      "/kaggle/input/dinov2/pytorch/giant/1/config.json\n",
      "/kaggle/input/dinov2/pytorch/giant/1/preprocessor_config.json\n",
      "/kaggle/input/dinov2/pytorch/giant/1/README.md\n",
      "/kaggle/input/dinov2/pytorch/giant/1/pytorch_model.bin\n",
      "/kaggle/input/dinov2/pytorch/giant/1/.gitattributes\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_0/metrics.csv\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_0/swanlab_info.json\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_0/checkpoints/best_loss.pt\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_0/checkpoints/best_wr2.pt\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_0/checkpoints/last.pt\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_4/metrics.csv\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_4/swanlab_info.json\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_4/checkpoints/best_loss.pt\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_4/checkpoints/best_wr2.pt\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_4/checkpoints/last.pt\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_1/metrics.csv\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_1/swanlab_info.json\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_1/checkpoints/best_loss.pt\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_1/checkpoints/best_wr2.pt\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_1/checkpoints/last.pt\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_3/metrics.csv\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_3/swanlab_info.json\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_3/checkpoints/best_loss.pt\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_3/checkpoints/best_wr2.pt\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_3/checkpoints/last.pt\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_2/metrics.csv\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_2/swanlab_info.json\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_2/checkpoints/best_loss.pt\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_2/checkpoints/best_wr2.pt\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_2/checkpoints/last.pt\n",
      "/kaggle/input/csiro-mvp-models/model1.pth\n",
      "/kaggle/input/csiro-mvp-models/model2.pth\n",
      "/kaggle/input/csiro-mvp-models/model10.pth\n",
      "/kaggle/input/csiro-mvp-models/model7.pth\n",
      "/kaggle/input/csiro-mvp-models/model6.pth\n",
      "/kaggle/input/csiro-mvp-models/model4.pth\n",
      "/kaggle/input/csiro-mvp-models/model5.pth\n",
      "/kaggle/input/csiro-mvp-models/model9.pth\n",
      "/kaggle/input/csiro-mvp-models/model3.pth\n",
      "/kaggle/input/csiro-mvp-models/model8.pth\n",
      "/kaggle/input/csiro-image-embeddings/train_siglip_embeddings.csv\n",
      "/kaggle/input/csiro-image-embeddings/train_dino_embeddings.csv\n",
      "/kaggle/input/csiro-biomass-competition-comprehensive-eda/__results__.html\n",
      "/kaggle/input/csiro-biomass-competition-comprehensive-eda/__notebook__.ipynb\n",
      "/kaggle/input/csiro-biomass-competition-comprehensive-eda/__output__.json\n",
      "/kaggle/input/csiro-biomass-competition-comprehensive-eda/custom.css\n",
      "/kaggle/input/csiro-biomass-competition-comprehensive-eda/__results___files/__results___15_0.png\n",
      "/kaggle/input/csiro-biomass-competition-comprehensive-eda/__results___files/__results___25_0.png\n",
      "/kaggle/input/csiro-biomass-competition-comprehensive-eda/__results___files/__results___19_0.png\n",
      "/kaggle/input/csiro-biomass-competition-comprehensive-eda/__results___files/__results___31_0.png\n",
      "/kaggle/input/csiro-biomass-competition-comprehensive-eda/__results___files/__results___22_0.png\n",
      "/kaggle/input/csiro-biomass-competition-comprehensive-eda/__results___files/__results___28_0.png\n",
      "/kaggle/input/csiro-biomass-solution-out-put/submission2.csv\n",
      "/kaggle/input/csiro-biomass-solution-out-put/professional_1_distributions.png\n",
      "/kaggle/input/csiro-biomass-solution-out-put/submission4.csv\n",
      "/kaggle/input/csiro-biomass-solution-out-put/submission3.csv\n",
      "/kaggle/input/csiro-biomass-solution-out-put/professional_6_hierarchical.png\n",
      "/kaggle/input/csiro-biomass-solution-out-put/submission1.csv\n",
      "/kaggle/input/csiro-biomass-solution-out-put/submission5.csv\n",
      "/kaggle/input/csiro-biomass-solution-out-put/professional_4_scatter_matrix.png\n",
      "/kaggle/input/csiro-biomass-solution-out-put/submission.csv\n",
      "/kaggle/input/csiro-biomass-solution-out-put/professional_5_sample_images.png\n",
      "/kaggle/input/csiro-biomass-solution-out-put/professional_3_boxplots.png\n",
      "/kaggle/input/csiro-biomass-solution-out-put/professional_2_correlation_matrix.png\n",
      "/kaggle/input/csiro-biomass-solution-out-put/catboost_info/learn_error.tsv\n",
      "/kaggle/input/csiro-biomass-solution-out-put/catboost_info/catboost_training.json\n",
      "/kaggle/input/csiro-biomass-solution-out-put/catboost_info/time_left.tsv\n",
      "/kaggle/input/csiro-biomass-solution-out-put/catboost_info/learn/events.out.tfevents\n",
      "/kaggle/input/google-siglip-so400m-patch14-384/transformers/default/1/config.json\n",
      "/kaggle/input/google-siglip-so400m-patch14-384/transformers/default/1/preprocessor_config.json\n",
      "/kaggle/input/google-siglip-so400m-patch14-384/transformers/default/1/spiece.model\n",
      "/kaggle/input/google-siglip-so400m-patch14-384/transformers/default/1/README.md\n",
      "/kaggle/input/google-siglip-so400m-patch14-384/transformers/default/1/tokenizer.json\n",
      "/kaggle/input/google-siglip-so400m-patch14-384/transformers/default/1/tokenizer_config.json\n",
      "/kaggle/input/google-siglip-so400m-patch14-384/transformers/default/1/gitattributes\n",
      "/kaggle/input/google-siglip-so400m-patch14-384/transformers/default/1/model.safetensors\n",
      "/kaggle/input/google-siglip-so400m-patch14-384/transformers/default/1/special_tokens_map.json\n",
      "/kaggle/input/csiro-datasplit/__results__.html\n",
      "/kaggle/input/csiro-datasplit/csiro_data_split.csv\n",
      "/kaggle/input/csiro-datasplit/__notebook__.ipynb\n",
      "/kaggle/input/csiro-datasplit/__output__.json\n",
      "/kaggle/input/csiro-datasplit/custom.css\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81f597ec",
   "metadata": {
    "_cell_guid": "990a6c22-7764-40a4-8606-aa6afce5916e",
    "_uuid": "bca5df7c-fe9c-4431-897c-e0e8c354e0b4",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-01-15T12:06:16.658289Z",
     "iopub.status.busy": "2026-01-15T12:06:16.657914Z",
     "iopub.status.idle": "2026-01-15T12:06:42.008812Z",
     "shell.execute_reply": "2026-01-15T12:06:42.007944Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 25.357088,
     "end_time": "2026-01-15T12:06:42.010449",
     "exception": false,
     "start_time": "2026-01-15T12:06:16.653361",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CSIRO BIOMASS COMPETITION - PROFESSIONAL EDA\n",
      "================================================================================\n",
      "\n",
      "Loading data...\n",
      "Train shape: (1785, 9)\n",
      "Test shape: (5, 3)\n",
      "Pivoted train shape: (357, 6)\n",
      "\n",
      "================================================================================\n",
      "Creating Visualization 1: Target Distributions with Normal Curves\n",
      "================================================================================\n",
      "✓ Saved: professional_1_distributions.png\n",
      "\n",
      "================================================================================\n",
      "Creating Visualization 2: Complete Correlation Matrix\n",
      "================================================================================\n",
      "✓ Saved: professional_2_correlation_matrix.png\n",
      "\n",
      "================================================================================\n",
      "Creating Visualization 3: Detailed Box Plots\n",
      "================================================================================\n",
      "✓ Saved: professional_3_boxplots.png\n",
      "\n",
      "================================================================================\n",
      "Creating Visualization 4: Scatter Plot Matrix\n",
      "================================================================================\n",
      "✓ Saved: professional_4_scatter_matrix.png\n",
      "\n",
      "================================================================================\n",
      "Creating Visualization 5: Detailed Sample Image Analysis\n",
      "================================================================================\n",
      "✓ Saved: professional_5_sample_images.png\n",
      "\n",
      "================================================================================\n",
      "Creating Visualization 6: Hierarchical Relationships\n",
      "================================================================================\n",
      "✓ Saved: professional_6_hierarchical.png\n",
      "\n",
      "================================================================================\n",
      "EDA COMPLETE - ALL VISUALIZATIONS SAVED\n",
      "================================================================================\n",
      "\n",
      "Generated Professional Visualizations:\n",
      "  1. professional_1_distributions.png - Target distributions with bell curves\n",
      "  2. professional_2_correlation_matrix.png - Complete correlation analysis\n",
      "  3. professional_3_boxplots.png - Detailed box plots with statistics\n",
      "  4. professional_4_scatter_matrix.png - Scatter plot matrix with regression\n",
      "  5. professional_5_sample_images.png - Detailed image analysis\n",
      "  6. professional_6_hierarchical.png - Hierarchical relationship validation\n",
      "\n",
      "All visualizations use consistent professional color scheme\n",
      "Statistical details and annotations included on all plots\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# CSIRO BIOMASS COMPETITION - PROFESSIONAL EDA\n",
    "# ================================================================\n",
    "# Professional visualizations with consistent color scheme\n",
    "# Complete statistical analysis with bell curves and annotations\n",
    "# ================================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import skew, kurtosis, normaltest, pearsonr, spearmanr, norm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# ================================================================\n",
    "# PROFESSIONAL STYLING CONFIGURATION\n",
    "# ================================================================\n",
    "\n",
    "# Professional color palette (consistent across all plots)\n",
    "COLORS = {\n",
    "    'primary': '#2E4057',      # Dark blue-gray (main)\n",
    "    'secondary': '#048A81',     # Teal (accent)\n",
    "    'tertiary': '#54C6EB',      # Light blue\n",
    "    'highlight': '#D95D39',     # Coral red (emphasis)\n",
    "    'neutral': '#8B8B8B',       # Gray\n",
    "    'background': '#F5F5F5',    # Light gray background\n",
    "    'grid': '#E0E0E0',          # Grid color\n",
    "    'text': '#2C3E50'           # Dark text\n",
    "}\n",
    "\n",
    "# Set professional matplotlib style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams.update({\n",
    "    'figure.figsize': (12, 7),\n",
    "    'font.size': 11,\n",
    "    'axes.labelsize': 12,\n",
    "    'axes.titlesize': 14,\n",
    "    'xtick.labelsize': 10,\n",
    "    'ytick.labelsize': 10,\n",
    "    'legend.fontsize': 10,\n",
    "    'figure.titlesize': 16,\n",
    "    'axes.facecolor': COLORS['background'],\n",
    "    'figure.facecolor': 'white',\n",
    "    'axes.edgecolor': COLORS['neutral'],\n",
    "    'grid.color': COLORS['grid'],\n",
    "    'grid.alpha': 0.3,\n",
    "    'text.color': COLORS['text'],\n",
    "    'axes.labelcolor': COLORS['text'],\n",
    "    'xtick.color': COLORS['text'],\n",
    "    'ytick.color': COLORS['text']\n",
    "})\n",
    "\n",
    "# ================================================================\n",
    "# CONFIGURATION\n",
    "# ================================================================\n",
    "\n",
    "class Config:\n",
    "    BASE_PATH = Path(\"/kaggle/input/csiro-biomass/\")\n",
    "    TRAIN_PATH = BASE_PATH / \"train\"\n",
    "    TEST_PATH = BASE_PATH / \"test\"\n",
    "    OUTPUT_PATH = Path(\"/kaggle/working/\")\n",
    "    \n",
    "    TARGET_COLS = ['Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g', 'GDM_g', 'Dry_Total_g']\n",
    "    TARGET_NAMES = {\n",
    "        'Dry_Green_g': 'Green Biomass',\n",
    "        'Dry_Dead_g': 'Dead Biomass',\n",
    "        'Dry_Clover_g': 'Clover Biomass',\n",
    "        'GDM_g': 'Green Dry Matter',\n",
    "        'Dry_Total_g': 'Total Biomass'\n",
    "    }\n",
    "    \n",
    "    TARGET_WEIGHTS = {\n",
    "        'Dry_Green_g': 0.1,\n",
    "        'Dry_Dead_g': 0.1,\n",
    "        'Dry_Clover_g': 0.1,\n",
    "        'GDM_g': 0.2,\n",
    "        'Dry_Total_g': 0.5,\n",
    "    }\n",
    "\n",
    "cfg = Config()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CSIRO BIOMASS COMPETITION - PROFESSIONAL EDA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ================================================================\n",
    "# LOAD DATA\n",
    "# ================================================================\n",
    "\n",
    "print(\"\\nLoading data...\")\n",
    "train_df = pd.read_csv(cfg.BASE_PATH / \"train.csv\")\n",
    "test_df = pd.read_csv(cfg.BASE_PATH / \"test.csv\")\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")\n",
    "\n",
    "# Pivot to get one row per image\n",
    "train_pivot = train_df.pivot_table(\n",
    "    values='target',\n",
    "    index='image_path',\n",
    "    columns='target_name',\n",
    "    aggfunc='mean'\n",
    ").reset_index()\n",
    "\n",
    "print(f\"Pivoted train shape: {train_pivot.shape}\")\n",
    "\n",
    "# ================================================================\n",
    "# 1. COMPREHENSIVE TARGET DISTRIBUTION WITH BELL CURVES\n",
    "# ================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Creating Visualization 1: Target Distributions with Normal Curves\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fig = plt.figure(figsize=(20, 12))\n",
    "gs = fig.add_gridspec(3, 2, hspace=0.3, wspace=0.25)\n",
    "\n",
    "for idx, col in enumerate(cfg.TARGET_COLS):\n",
    "    row = idx // 2\n",
    "    col_idx = idx % 2\n",
    "    ax = fig.add_subplot(gs[row, col_idx])\n",
    "    \n",
    "    values = train_pivot[col].dropna().values\n",
    "    \n",
    "    # Histogram\n",
    "    n, bins, patches = ax.hist(values, bins=40, alpha=0.6, color=COLORS['primary'], \n",
    "                                edgecolor='white', linewidth=1.5, density=True,\n",
    "                                label='Observed Distribution')\n",
    "    \n",
    "    # Fit normal distribution\n",
    "    mu, sigma = values.mean(), values.std()\n",
    "    x = np.linspace(values.min(), values.max(), 100)\n",
    "    normal_curve = norm.pdf(x, mu, sigma)\n",
    "    \n",
    "    # Plot normal curve\n",
    "    ax.plot(x, normal_curve, color=COLORS['highlight'], linewidth=3, \n",
    "            label=f'Normal Fit (μ={mu:.1f}, σ={sigma:.1f})', linestyle='--')\n",
    "    \n",
    "    # Statistics lines\n",
    "    ax.axvline(mu, color=COLORS['secondary'], linestyle='-', linewidth=2.5, \n",
    "               label=f'Mean: {mu:.2f}g', alpha=0.8)\n",
    "    ax.axvline(np.median(values), color=COLORS['tertiary'], linestyle='-', linewidth=2.5,\n",
    "               label=f'Median: {np.median(values):.2f}g', alpha=0.8)\n",
    "    \n",
    "    # Annotations\n",
    "    ax.text(0.98, 0.97, f'n = {len(values):,}', transform=ax.transAxes,\n",
    "            verticalalignment='top', horizontalalignment='right',\n",
    "            bbox=dict(boxstyle='round', facecolor='white', alpha=0.9, edgecolor=COLORS['neutral']),\n",
    "            fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # Statistical properties\n",
    "    skewness = skew(values)\n",
    "    kurt = kurtosis(values)\n",
    "    stats_text = f'Skewness: {skewness:.2f}\\nKurtosis: {kurt:.2f}\\nMin: {values.min():.2f}g\\nMax: {values.max():.2f}g'\n",
    "    ax.text(0.02, 0.97, stats_text, transform=ax.transAxes,\n",
    "            verticalalignment='top', horizontalalignment='left',\n",
    "            bbox=dict(boxstyle='round', facecolor='white', alpha=0.9, edgecolor=COLORS['neutral']),\n",
    "            fontsize=9)\n",
    "    \n",
    "    # Labels and title\n",
    "    ax.set_xlabel(f'{cfg.TARGET_NAMES[col]} (grams)', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Probability Density', fontsize=12, fontweight='bold')\n",
    "    ax.set_title(f'{cfg.TARGET_NAMES[col]} Distribution', \n",
    "                 fontsize=14, fontweight='bold', pad=15, color=COLORS['text'])\n",
    "    ax.legend(loc='upper right', framealpha=0.95, edgecolor=COLORS['neutral'])\n",
    "    ax.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "# Add overall title\n",
    "fig.suptitle('Target Variable Distributions with Statistical Analysis', \n",
    "             fontsize=18, fontweight='bold', y=0.995, color=COLORS['text'])\n",
    "\n",
    "plt.savefig(cfg.OUTPUT_PATH / 'professional_1_distributions.png', dpi=300, bbox_inches='tight', \n",
    "            facecolor='white', edgecolor='none')\n",
    "print(\"✓ Saved: professional_1_distributions.png\")\n",
    "plt.close()\n",
    "\n",
    "# ================================================================\n",
    "# 2. COMPREHENSIVE CORRELATION MATRIX  \n",
    "# ================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Creating Visualization 2: Complete Correlation Matrix\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate correlations\n",
    "correlation_matrix = train_pivot[cfg.TARGET_COLS].corr()\n",
    "\n",
    "# Create simple figure\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "# Professional RED-WHITE-GREEN diverging colormap\n",
    "# Red (negative) → White/Yellow (zero) → Green (positive)\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "colors_cmap = [\n",
    "    '#D73027',  # Dark red (strong negative)\n",
    "    '#FC8D59',  # Medium red\n",
    "    '#FEE090',  # Light red/orange\n",
    "    '#FFFFBF',  # White/Yellow (zero)\n",
    "    '#E0F3DB',  # Very light green\n",
    "    '#A8DDB5',  # Light green\n",
    "    '#66C2A4',  # Medium light green\n",
    "    '#2CA25F',  # Medium green\n",
    "    '#006D2C'   # Dark green (strong positive)\n",
    "]\n",
    "n_bins = 100\n",
    "cmap_diverging = LinearSegmentedColormap.from_list('red_white_green', colors_cmap, N=n_bins)\n",
    "\n",
    "# Create heatmap with full range -1 to 1\n",
    "im = ax.imshow(correlation_matrix, cmap=cmap_diverging, aspect='auto', vmin=-1, vmax=1)\n",
    "\n",
    "# Set ticks\n",
    "ax.set_xticks(np.arange(len(cfg.TARGET_COLS)))\n",
    "ax.set_yticks(np.arange(len(cfg.TARGET_COLS)))\n",
    "ax.set_xticklabels([cfg.TARGET_NAMES[col] for col in cfg.TARGET_COLS], \n",
    "                    rotation=45, ha='right', fontsize=11, fontweight='bold')\n",
    "ax.set_yticklabels([cfg.TARGET_NAMES[col] for col in cfg.TARGET_COLS], \n",
    "                    fontsize=11, fontweight='bold')\n",
    "\n",
    "# Add correlation values and significance\n",
    "for i in range(len(cfg.TARGET_COLS)):\n",
    "    for j in range(len(cfg.TARGET_COLS)):\n",
    "        corr_val = correlation_matrix.iloc[i, j]\n",
    "        \n",
    "        # Calculate p-value\n",
    "        if i != j:\n",
    "            _, p_val = pearsonr(train_pivot[cfg.TARGET_COLS[i]], \n",
    "                               train_pivot[cfg.TARGET_COLS[j]])\n",
    "            sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else ''\n",
    "        else:\n",
    "            sig = ''\n",
    "        \n",
    "        # Choose text color based on correlation value\n",
    "        # Dark text for light colors (near zero), white text for dark colors (strong correlations)\n",
    "        if abs(corr_val) > 0.6:\n",
    "            text_color = 'white'\n",
    "        else:\n",
    "            text_color = 'black'\n",
    "        \n",
    "        # Main correlation value\n",
    "        ax.text(j, i, f'{corr_val:.3f}', ha='center', va='center',\n",
    "                color=text_color, fontsize=12, fontweight='bold')\n",
    "        \n",
    "        # Significance stars\n",
    "        if sig:\n",
    "            ax.text(j, i + 0.35, sig, ha='center', va='center',\n",
    "                    color=text_color, fontsize=10, fontweight='bold')\n",
    "\n",
    "# Colorbar with professional labels\n",
    "cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "cbar.set_label('Pearson Correlation Coefficient (r)', rotation=270, labelpad=25, \n",
    "               fontsize=12, fontweight='bold')\n",
    "cbar.ax.tick_params(labelsize=10)\n",
    "\n",
    "# Add interpretive labels on colorbar\n",
    "cbar.ax.text(3.5, -0.9, 'Strong Negative', transform=cbar.ax.transData,\n",
    "            fontsize=9, fontweight='bold', color='#D73027', rotation=0)\n",
    "cbar.ax.text(3.5, 0.0, 'No Correlation', transform=cbar.ax.transData,\n",
    "            fontsize=9, fontweight='bold', color='#666666', rotation=0)\n",
    "cbar.ax.text(3.5, 0.9, 'Strong Positive', transform=cbar.ax.transData,\n",
    "            fontsize=9, fontweight='bold', color='#006D2C', rotation=0)\n",
    "\n",
    "# Title with significance legend\n",
    "title_text = 'Complete Correlation Matrix: All Target Variables\\n'\n",
    "title_text += 'Red = Negative | Yellow = Zero | Green = Positive  |  '\n",
    "title_text += '*** p<0.001, ** p<0.01, * p<0.05'\n",
    "ax.set_title(title_text, fontsize=13, fontweight='bold', pad=20, color=COLORS['text'])\n",
    "\n",
    "# Grid\n",
    "ax.set_xticks(np.arange(len(cfg.TARGET_COLS)) - 0.5, minor=True)\n",
    "ax.set_yticks(np.arange(len(cfg.TARGET_COLS)) - 0.5, minor=True)\n",
    "ax.grid(which='minor', color='white', linestyle='-', linewidth=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(cfg.OUTPUT_PATH / 'professional_2_correlation_matrix.png', dpi=300, bbox_inches='tight',\n",
    "            facecolor='white', edgecolor='none')\n",
    "print(\"✓ Saved: professional_2_correlation_matrix.png\")\n",
    "plt.close()\n",
    "\n",
    "# ================================================================\n",
    "# 3. DETAILED BOX PLOTS WITH STATISTICS\n",
    "# ================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Creating Visualization 3: Detailed Box Plots\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "# Prepare data\n",
    "data_for_box = [train_pivot[col].dropna().values for col in cfg.TARGET_COLS]\n",
    "labels = [cfg.TARGET_NAMES[col] for col in cfg.TARGET_COLS]\n",
    "\n",
    "# Create box plot\n",
    "bp = ax.boxplot(data_for_box, labels=labels, patch_artist=True,\n",
    "                widths=0.6, showmeans=True,\n",
    "                meanprops=dict(marker='D', markerfacecolor=COLORS['highlight'], \n",
    "                              markeredgecolor='white', markersize=10),\n",
    "                medianprops=dict(color=COLORS['secondary'], linewidth=2.5),\n",
    "                boxprops=dict(facecolor=COLORS['primary'], alpha=0.7, \n",
    "                             edgecolor=COLORS['text'], linewidth=1.5),\n",
    "                whiskerprops=dict(color=COLORS['text'], linewidth=1.5),\n",
    "                capprops=dict(color=COLORS['text'], linewidth=1.5),\n",
    "                flierprops=dict(marker='o', markerfacecolor=COLORS['highlight'], \n",
    "                               markersize=5, alpha=0.5, markeredgecolor='none'))\n",
    "\n",
    "# Add detailed statistics\n",
    "for idx, (col, data) in enumerate(zip(cfg.TARGET_COLS, data_for_box)):\n",
    "    # Calculate statistics\n",
    "    q1 = np.percentile(data, 25)\n",
    "    median = np.median(data)\n",
    "    q3 = np.percentile(data, 75)\n",
    "    mean = np.mean(data)\n",
    "    iqr = q3 - q1\n",
    "    lower_whisker = q1 - 1.5 * iqr\n",
    "    upper_whisker = q3 + 1.5 * iqr\n",
    "    outliers = len(data[(data < lower_whisker) | (data > upper_whisker)])\n",
    "    \n",
    "    # Add text annotation\n",
    "    stats_text = f'Mean: {mean:.1f}g\\nMedian: {median:.1f}g\\nIQR: {iqr:.1f}g\\nOutliers: {outliers}'\n",
    "    ax.text(idx + 1, ax.get_ylim()[1] * 0.95, stats_text,\n",
    "            ha='center', va='top', fontsize=9,\n",
    "            bbox=dict(boxstyle='round', facecolor='white', alpha=0.9, \n",
    "                     edgecolor=COLORS['neutral']))\n",
    "\n",
    "# Labels and formatting\n",
    "ax.set_ylabel('Biomass (grams)', fontsize=13, fontweight='bold')\n",
    "ax.set_xlabel('Target Variables', fontsize=13, fontweight='bold')\n",
    "ax.set_title('Box Plot Analysis: Target Variables with Statistical Details', \n",
    "             fontsize=15, fontweight='bold', pad=20, color=COLORS['text'])\n",
    "\n",
    "# Legend\n",
    "legend_elements = [\n",
    "    plt.Line2D([0], [0], marker='D', color='w', label='Mean',\n",
    "              markerfacecolor=COLORS['highlight'], markersize=10),\n",
    "    plt.Line2D([0], [0], color=COLORS['secondary'], linewidth=2.5, label='Median'),\n",
    "    plt.Line2D([0], [0], marker='o', color='w', label='Outliers',\n",
    "              markerfacecolor=COLORS['highlight'], markersize=7, alpha=0.5)\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc='upper right', framealpha=0.95, \n",
    "         edgecolor=COLORS['neutral'], fontsize=11)\n",
    "\n",
    "ax.grid(True, alpha=0.3, axis='y', linestyle='--')\n",
    "ax.set_axisbelow(True)\n",
    "plt.xticks(rotation=15, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(cfg.OUTPUT_PATH / 'professional_3_boxplots.png', dpi=300, bbox_inches='tight',\n",
    "            facecolor='white', edgecolor='none')\n",
    "print(\"✓ Saved: professional_3_boxplots.png\")\n",
    "plt.close()\n",
    "\n",
    "# ================================================================\n",
    "# 4. SCATTER PLOT MATRIX WITH REGRESSION LINES\n",
    "# ================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Creating Visualization 4: Scatter Plot Matrix\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Select subset of targets for clarity\n",
    "main_targets = ['Dry_Green_g', 'Dry_Dead_g', 'GDM_g', 'Dry_Total_g']\n",
    "n_targets = len(main_targets)\n",
    "\n",
    "fig, axes = plt.subplots(n_targets, n_targets, figsize=(18, 18))\n",
    "\n",
    "for i, target1 in enumerate(main_targets):\n",
    "    for j, target2 in enumerate(main_targets):\n",
    "        ax = axes[i, j]\n",
    "        \n",
    "        if i == j:\n",
    "            # Diagonal: histogram with KDE\n",
    "            data = train_pivot[target1].dropna().values\n",
    "            ax.hist(data, bins=30, alpha=0.6, color=COLORS['primary'], \n",
    "                   edgecolor='white', density=True)\n",
    "            \n",
    "            # KDE\n",
    "            from scipy.stats import gaussian_kde\n",
    "            kde = gaussian_kde(data)\n",
    "            x_range = np.linspace(data.min(), data.max(), 100)\n",
    "            ax.plot(x_range, kde(x_range), color=COLORS['highlight'], \n",
    "                   linewidth=2.5, label='KDE')\n",
    "            \n",
    "            ax.set_ylabel('Density', fontsize=9)\n",
    "            if i == n_targets - 1:\n",
    "                ax.set_xlabel(cfg.TARGET_NAMES[target1], fontsize=10, fontweight='bold')\n",
    "        else:\n",
    "            # Off-diagonal: scatter plot\n",
    "            x_data = train_pivot[target2].dropna()\n",
    "            y_data = train_pivot[target1].dropna()\n",
    "            \n",
    "            # Align data\n",
    "            common_idx = x_data.index.intersection(y_data.index)\n",
    "            x_vals = x_data.loc[common_idx].values\n",
    "            y_vals = y_data.loc[common_idx].values\n",
    "            \n",
    "            # Scatter\n",
    "            ax.scatter(x_vals, y_vals, alpha=0.4, s=20, color=COLORS['primary'], \n",
    "                      edgecolors='none')\n",
    "            \n",
    "            # Regression line\n",
    "            if len(x_vals) > 1:\n",
    "                z = np.polyfit(x_vals, y_vals, 1)\n",
    "                p = np.poly1d(z)\n",
    "                x_line = np.linspace(x_vals.min(), x_vals.max(), 100)\n",
    "                ax.plot(x_line, p(x_line), color=COLORS['highlight'], \n",
    "                       linewidth=2.5, linestyle='--')\n",
    "                \n",
    "                # Correlation\n",
    "                r, p_val = pearsonr(x_vals, y_vals)\n",
    "                ax.text(0.05, 0.95, f'r={r:.3f}', transform=ax.transAxes,\n",
    "                       verticalalignment='top', fontsize=9, fontweight='bold',\n",
    "                       bbox=dict(boxstyle='round', facecolor='white', alpha=0.9))\n",
    "        \n",
    "        # Labels\n",
    "        if j == 0:\n",
    "            ax.set_ylabel(cfg.TARGET_NAMES[target1], fontsize=10, fontweight='bold')\n",
    "        if i == n_targets - 1:\n",
    "            ax.set_xlabel(cfg.TARGET_NAMES[target2], fontsize=10, fontweight='bold')\n",
    "        \n",
    "        ax.grid(True, alpha=0.2, linestyle='--')\n",
    "        ax.tick_params(labelsize=8)\n",
    "\n",
    "fig.suptitle('Scatter Plot Matrix: Pairwise Relationships Between Targets', \n",
    "             fontsize=16, fontweight='bold', y=0.995)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(cfg.OUTPUT_PATH / 'professional_4_scatter_matrix.png', dpi=300, bbox_inches='tight',\n",
    "            facecolor='white', edgecolor='none')\n",
    "print(\"✓ Saved: professional_4_scatter_matrix.png\")\n",
    "plt.close()\n",
    "\n",
    "# ================================================================\n",
    "# 5. DETAILED SAMPLE IMAGE ANALYSIS\n",
    "# ================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Creating Visualization 5: Detailed Sample Image Analysis\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Select 6 sample images\n",
    "sample_images = train_pivot['image_path'].sample(min(6, len(train_pivot)), random_state=42).tolist()\n",
    "\n",
    "fig = plt.figure(figsize=(20, 16))\n",
    "gs = fig.add_gridspec(6, 4, hspace=0.35, wspace=0.3)  # 6 rows, 4 columns\n",
    "\n",
    "for idx, img_path in enumerate(sample_images):\n",
    "    img_name = os.path.basename(img_path)\n",
    "    full_path = cfg.TRAIN_PATH / img_name\n",
    "    \n",
    "    img = cv2.imread(str(full_path))\n",
    "    if img is None:\n",
    "        continue\n",
    "    \n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img_hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    # Main image (left column)\n",
    "    ax_img = fig.add_subplot(gs[idx, 0])\n",
    "    ax_img.imshow(img_rgb)\n",
    "    ax_img.set_title(f'Sample {idx+1}', fontsize=12, fontweight='bold')\n",
    "    ax_img.axis('off')\n",
    "    \n",
    "    # Add image properties\n",
    "    h, w, _ = img_rgb.shape\n",
    "    green_ratio = img_rgb[:,:,1] / (img_rgb.sum(axis=2) + 1)\n",
    "    brightness = img_rgb.mean()\n",
    "    \n",
    "    props_text = f'Size: {w}×{h}\\nBrightness: {brightness:.1f}\\nGreen Ratio: {green_ratio.mean():.3f}'\n",
    "    ax_img.text(0.02, 0.98, props_text, transform=ax_img.transAxes,\n",
    "               verticalalignment='top', fontsize=9,\n",
    "               bbox=dict(boxstyle='round', facecolor='white', alpha=0.9))\n",
    "    \n",
    "    # RGB histogram\n",
    "    ax_hist = fig.add_subplot(gs[idx, 1])\n",
    "    colors_rgb = ['#E74C3C', '#27AE60', '#3498DB']  # Red, Green, Blue\n",
    "    labels_rgb = ['Red', 'Green', 'Blue']\n",
    "    \n",
    "    for c, color, label in zip(range(3), colors_rgb, labels_rgb):\n",
    "        hist = cv2.calcHist([img_rgb], [c], None, [256], [0, 256])\n",
    "        ax_hist.plot(hist, color=color, linewidth=2, label=label, alpha=0.7)\n",
    "    \n",
    "    ax_hist.set_xlabel('Pixel Value', fontsize=9)\n",
    "    ax_hist.set_ylabel('Frequency', fontsize=9)\n",
    "    ax_hist.set_title('RGB Distribution', fontsize=10, fontweight='bold')\n",
    "    ax_hist.legend(fontsize=8)\n",
    "    ax_hist.grid(True, alpha=0.3)\n",
    "    ax_hist.set_xlim([0, 256])\n",
    "    \n",
    "    # Green channel analysis\n",
    "    ax_green = fig.add_subplot(gs[idx, 2])\n",
    "    green_channel = img_rgb[:,:,1]\n",
    "    ax_green.imshow(green_channel, cmap='Greens')\n",
    "    ax_green.set_title('Green Channel', fontsize=10, fontweight='bold')\n",
    "    ax_green.axis('off')\n",
    "    \n",
    "    # Vegetation index\n",
    "    ax_veg = fig.add_subplot(gs[idx, 3])\n",
    "    ax_veg.imshow(green_ratio, cmap='RdYlGn', vmin=0, vmax=0.5)\n",
    "    ax_veg.set_title('Vegetation Index', fontsize=10, fontweight='bold')\n",
    "    ax_veg.axis('off')\n",
    "\n",
    "fig.suptitle('Detailed Sample Image Analysis: RGB, Green Channel, and Vegetation Index', \n",
    "             fontsize=16, fontweight='bold', y=0.995)\n",
    "\n",
    "plt.savefig(cfg.OUTPUT_PATH / 'professional_5_sample_images.png', dpi=300, bbox_inches='tight',\n",
    "            facecolor='white', edgecolor='none')\n",
    "print(\"✓ Saved: professional_5_sample_images.png\")\n",
    "plt.close()\n",
    "\n",
    "# ================================================================\n",
    "# 6. HIERARCHICAL RELATIONSHIP VALIDATION\n",
    "# ================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Creating Visualization 6: Hierarchical Relationships\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
    "\n",
    "# Green + Clover = GDM\n",
    "ax = axes[0]\n",
    "calculated = train_pivot['Dry_Green_g'] + train_pivot['Dry_Clover_g']\n",
    "actual = train_pivot['GDM_g']\n",
    "\n",
    "ax.scatter(calculated, actual, alpha=0.5, s=40, color=COLORS['primary'], \n",
    "          edgecolors='white', linewidth=0.5)\n",
    "\n",
    "# Perfect match line\n",
    "max_val = max(calculated.max(), actual.max())\n",
    "ax.plot([0, max_val], [0, max_val], color=COLORS['highlight'], \n",
    "       linewidth=3, linestyle='--', label='Perfect Match (y=x)')\n",
    "\n",
    "# Regression line\n",
    "z = np.polyfit(calculated, actual, 1)\n",
    "p = np.poly1d(z)\n",
    "x_line = np.linspace(calculated.min(), calculated.max(), 100)\n",
    "ax.plot(x_line, p(x_line), color=COLORS['secondary'], \n",
    "       linewidth=2.5, label=f'Regression: y={z[0]:.3f}x+{z[1]:.2f}')\n",
    "\n",
    "# Statistics\n",
    "r, p_val = pearsonr(calculated, actual)\n",
    "rmse = np.sqrt(((calculated - actual) ** 2).mean())\n",
    "\n",
    "stats_text = f'Pearson r: {r:.4f}\\np-value: {p_val:.2e}\\nRMSE: {rmse:.2f}g\\nn: {len(calculated):,}'\n",
    "ax.text(0.05, 0.95, stats_text, transform=ax.transAxes,\n",
    "       verticalalignment='top', fontsize=11, fontweight='bold',\n",
    "       bbox=dict(boxstyle='round', facecolor='white', alpha=0.95, \n",
    "                edgecolor=COLORS['neutral'], linewidth=2))\n",
    "\n",
    "ax.set_xlabel('Green + Clover (calculated, grams)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('GDM (actual, grams)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Hierarchical Validation: Green + Clover = GDM', \n",
    "            fontsize=13, fontweight='bold', pad=15)\n",
    "ax.legend(loc='lower right', fontsize=11, framealpha=0.95)\n",
    "ax.grid(True, alpha=0.3, linestyle='--')\n",
    "\n",
    "# GDM + Dead = Total\n",
    "ax = axes[1]\n",
    "calculated = train_pivot['GDM_g'] + train_pivot['Dry_Dead_g']\n",
    "actual = train_pivot['Dry_Total_g']\n",
    "\n",
    "ax.scatter(calculated, actual, alpha=0.5, s=40, color=COLORS['primary'], \n",
    "          edgecolors='white', linewidth=0.5)\n",
    "\n",
    "max_val = max(calculated.max(), actual.max())\n",
    "ax.plot([0, max_val], [0, max_val], color=COLORS['highlight'], \n",
    "       linewidth=3, linestyle='--', label='Perfect Match (y=x)')\n",
    "\n",
    "z = np.polyfit(calculated, actual, 1)\n",
    "p = np.poly1d(z)\n",
    "x_line = np.linspace(calculated.min(), calculated.max(), 100)\n",
    "ax.plot(x_line, p(x_line), color=COLORS['secondary'], \n",
    "       linewidth=2.5, label=f'Regression: y={z[0]:.3f}x+{z[1]:.2f}')\n",
    "\n",
    "r, p_val = pearsonr(calculated, actual)\n",
    "rmse = np.sqrt(((calculated - actual) ** 2).mean())\n",
    "\n",
    "stats_text = f'Pearson r: {r:.4f}\\np-value: {p_val:.2e}\\nRMSE: {rmse:.2f}g\\nn: {len(calculated):,}'\n",
    "ax.text(0.05, 0.95, stats_text, transform=ax.transAxes,\n",
    "       verticalalignment='top', fontsize=11, fontweight='bold',\n",
    "       bbox=dict(boxstyle='round', facecolor='white', alpha=0.95, \n",
    "                edgecolor=COLORS['neutral'], linewidth=2))\n",
    "\n",
    "ax.set_xlabel('GDM + Dead (calculated, grams)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Total Biomass (actual, grams)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Hierarchical Validation: GDM + Dead = Total', \n",
    "            fontsize=13, fontweight='bold', pad=15)\n",
    "ax.legend(loc='lower right', fontsize=11, framealpha=0.95)\n",
    "ax.grid(True, alpha=0.3, linestyle='--')\n",
    "\n",
    "fig.suptitle('Hierarchical Relationship Validation with Statistical Analysis', \n",
    "             fontsize=15, fontweight='bold', y=1.00)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(cfg.OUTPUT_PATH / 'professional_6_hierarchical.png', dpi=300, bbox_inches='tight',\n",
    "            facecolor='white', edgecolor='none')\n",
    "print(\"✓ Saved: professional_6_hierarchical.png\")\n",
    "plt.close()\n",
    "\n",
    "# ================================================================\n",
    "# SUMMARY\n",
    "# ================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EDA COMPLETE - ALL VISUALIZATIONS SAVED\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nGenerated Professional Visualizations:\")\n",
    "print(\"  1. professional_1_distributions.png - Target distributions with bell curves\")\n",
    "print(\"  2. professional_2_correlation_matrix.png - Complete correlation analysis\")\n",
    "print(\"  3. professional_3_boxplots.png - Detailed box plots with statistics\")\n",
    "print(\"  4. professional_4_scatter_matrix.png - Scatter plot matrix with regression\")\n",
    "print(\"  5. professional_5_sample_images.png - Detailed image analysis\")\n",
    "print(\"  6. professional_6_hierarchical.png - Hierarchical relationship validation\")\n",
    "print(\"\\nAll visualizations use consistent professional color scheme\")\n",
    "print(\"Statistical details and annotations included on all plots\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eda30c5d",
   "metadata": {
    "_cell_guid": "d8b0f9ea-68bd-4f17-91d0-be9802ee00d4",
    "_uuid": "3cc1b924-f2f6-4225-b4a3-4e6527dc1e9d",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-01-15T12:06:42.019163Z",
     "iopub.status.busy": "2026-01-15T12:06:42.018883Z",
     "iopub.status.idle": "2026-01-15T12:09:06.015615Z",
     "shell.execute_reply": "2026-01-15T12:09:06.014628Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 144.003184,
     "end_time": "2026-01-15T12:09:06.017116",
     "exception": false,
     "start_time": "2026-01-15T12:06:42.013932",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-15 12:07:36.814935: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1768478856.984625      24 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1768478857.032896      24 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1768478857.421827      24 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768478857.421859      24 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768478857.421862      24 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768478857.421865      24 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TIME] Started execution\n",
      "       Elapsed: 0.00h | Remaining: 8.50h\n",
      "Created fallback submission from sample\n",
      "\n",
      "================================================================================\n",
      "PART 1: SigLIP/Ensemble Model (submission1.csv)\n",
      "================================================================================\n",
      "Loaded pre-computed training embeddings\n",
      "[TIME] Loaded training data\n",
      "       Elapsed: 0.00h | Remaining: 8.50h\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dca39c8164044799f7419d3e1f3370f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing embeddings:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TIME] Computed test embeddings\n",
      "       Elapsed: 0.01h | Remaining: 8.49h\n",
      "[TIME] Generated semantic features\n",
      "       Elapsed: 0.01h | Remaining: 8.49h\n",
      "\n",
      "Running LightGBM...\n",
      "Full CV Score: 0.669802\n",
      "Raw CV Score: 0.669802\n",
      "Processed CV Score: 0.679134\n",
      "Improvement: -0.009332\n",
      "\n",
      "Running CatBoost...\n",
      "Full CV Score: 0.654876\n",
      "Raw CV Score: 0.654876\n",
      "Processed CV Score: 0.669611\n",
      "Improvement: -0.014735\n",
      "✓ submission1.csv created successfully\n",
      "[TIME] Completed submission1\n",
      "       Elapsed: 0.01h | Remaining: 8.49h\n",
      "\n",
      "================================================================================\n",
      "PART 2: XGBoost Variant (submission4.csv)\n",
      "================================================================================\n",
      "\n",
      "Running XGBoost...\n",
      "Full CV Score: 0.656580\n",
      "Raw CV Score: 0.656580\n",
      "Processed CV Score: 0.674476\n",
      "Improvement: -0.017895\n",
      "\n",
      "Running Ridge...\n",
      "Full CV Score: 0.671340\n",
      "Raw CV Score: 0.671340\n",
      "Processed CV Score: 0.681143\n",
      "Improvement: -0.009803\n",
      "✓ submission4.csv created successfully\n",
      "[TIME] Completed submission4\n",
      "       Elapsed: 0.02h | Remaining: 8.48h\n",
      "\n",
      "================================================================================\n",
      "PART 3: Gradient Boosting Ensemble (submission5.csv)\n",
      "================================================================================\n",
      "\n",
      "Running HistGradientBoosting...\n",
      "Full CV Score: 0.630357\n",
      "Raw CV Score: 0.630357\n",
      "Processed CV Score: 0.645159\n",
      "Improvement: -0.014802\n",
      "\n",
      "Running ExtraTrees...\n",
      "Full CV Score: 0.680313\n",
      "Raw CV Score: 0.680313\n",
      "Processed CV Score: 0.684796\n",
      "Improvement: -0.004482\n",
      "✓ submission5.csv created successfully\n",
      "[TIME] Completed submission5\n",
      "       Elapsed: 0.02h | Remaining: 8.48h\n",
      "\n",
      "================================================================================\n",
      "Creating submission2 and submission3 placeholders\n",
      "================================================================================\n",
      "✓ submission2.csv created as weighted ensemble\n",
      "✓ submission3.csv created as weighted ensemble\n",
      "\n",
      "================================================================================\n",
      "Creating Final Ensemble (submission.csv)\n",
      "================================================================================\n",
      "✓ Loaded submission1.csv\n",
      "✓ Loaded submission2.csv\n",
      "✓ Loaded submission3.csv\n",
      "✓ Loaded submission4.csv\n",
      "✓ Loaded submission5.csv\n",
      "✓ Final ensemble submission created successfully\n",
      "[TIME] Completed final ensemble\n",
      "       Elapsed: 0.02h | Remaining: 8.48h\n",
      "[TIME] Execution completed\n",
      "       Elapsed: 0.02h | Remaining: 8.48h\n",
      "\n",
      "================================================================================\n",
      "EXECUTION COMPLETE - 5 Submissions Created\n",
      "================================================================================\n",
      "Summary:\n",
      "  ✓ submission1.csv\n",
      "  ✓ submission2.csv\n",
      "  ✓ submission3.csv\n",
      "  ✓ submission4.csv\n",
      "  ✓ submission5.csv\n",
      "  ✓ submission.csv (final ensemble)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import math\n",
    "import json\n",
    "import random\n",
    "import argparse\n",
    "import shutil\n",
    "import time\n",
    "from copy import deepcopy\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, List, Tuple, Dict\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import timm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import albumentations as A\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, BayesianRidge\n",
    "from sklearn.svm import SVR, LinearSVR\n",
    "from sklearn.ensemble import GradientBoostingRegressor, HistGradientBoostingRegressor, ExtraTreesRegressor\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "from transformers import AutoProcessor, AutoImageProcessor, AutoModel, AutoTokenizer\n",
    "\n",
    "# ============================================================================\n",
    "# TIMEOUT MANAGER - ENHANCED\n",
    "# ============================================================================\n",
    "class TimeoutManager:\n",
    "    \"\"\"Manages execution time to prevent timeout\"\"\"\n",
    "    def __init__(self, max_time_seconds):\n",
    "        self.start_time = time.time()\n",
    "        self.max_time = max_time_seconds\n",
    "        self.checkpoints = {}\n",
    "        \n",
    "    def time_elapsed(self):\n",
    "        return time.time() - self.start_time\n",
    "        \n",
    "    def time_remaining(self):\n",
    "        return self.max_time - self.time_elapsed()\n",
    "    \n",
    "    def should_continue(self, buffer_seconds=300):\n",
    "        \"\"\"Check if we have enough time to continue (with buffer)\"\"\"\n",
    "        return self.time_remaining() > buffer_seconds\n",
    "    \n",
    "    def log_time(self, message=\"\"):\n",
    "        elapsed = self.time_elapsed()\n",
    "        remaining = self.time_remaining()\n",
    "        print(f\"[TIME] {message}\")\n",
    "        print(f\"       Elapsed: {elapsed/3600:.2f}h | Remaining: {remaining/3600:.2f}h\")\n",
    "        self.checkpoints[message] = elapsed\n",
    "    \n",
    "    def get_checkpoint_duration(self, message):\n",
    "        \"\"\"Get duration of a specific checkpoint\"\"\"\n",
    "        return self.checkpoints.get(message, 0)\n",
    "\n",
    "# ============================================================================\n",
    "# UTILITY FUNCTIONS\n",
    "# ============================================================================\n",
    "def seeding(SEED):\n",
    "    np.random.seed(SEED)\n",
    "    random.seed(SEED)\n",
    "    os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available(): \n",
    "        torch.cuda.manual_seed(SEED)\n",
    "        torch.cuda.manual_seed_all(SEED)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def flush():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    DATA_PATH: Path = Path(\"/kaggle/input/csiro-biomass/\")\n",
    "    TRAIN_DATA_PATH: Path = DATA_PATH/'train'\n",
    "    TEST_DATA_PATH: Path = DATA_PATH/'test'\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    seed = 42\n",
    "\n",
    "cfg = Config()\n",
    "seeding(cfg.seed)\n",
    "\n",
    "TARGET_NAMES = ['Dry_Clover_g', 'Dry_Dead_g', 'Dry_Green_g', 'Dry_Total_g', 'GDM_g']\n",
    "weights = {\n",
    "    'Dry_Green_g': 0.1,\n",
    "    'Dry_Dead_g': 0.1,\n",
    "    'Dry_Clover_g': 0.1,\n",
    "    'GDM_g': 0.2,\n",
    "    'Dry_Total_g': 0.5,\n",
    "}\n",
    "TARGET_MAX = {\n",
    "    \"Dry_Clover_g\": 71.7865,\n",
    "    \"Dry_Dead_g\": 83.8407,\n",
    "    \"Dry_Green_g\": 157.9836,\n",
    "    \"Dry_Total_g\": 185.70,\n",
    "    \"GDM_g\": 157.9836,\n",
    "}\n",
    "\n",
    "def competition_metric(y_true, y_pred) -> float:\n",
    "    y_weighted = 0\n",
    "    for l, label in enumerate(TARGET_NAMES):\n",
    "        y_weighted = y_weighted + y_true[:, l].mean() * weights[label]\n",
    "    ss_res = 0\n",
    "    ss_tot = 0\n",
    "    for l, label in enumerate(TARGET_NAMES):\n",
    "        ss_res = ss_res + ((y_true[:, l] - y_pred[:, l])**2).mean() * weights[label]\n",
    "        ss_tot = ss_tot + ((y_true[:, l] - y_weighted)**2).mean() * weights[label]\n",
    "    return 1 - ss_res / ss_tot\n",
    "\n",
    "def pivot_table(df: pd.DataFrame)->pd.DataFrame:\n",
    "    if 'target' in df.columns.tolist():\n",
    "        df_pt = pd.pivot_table(\n",
    "            df, \n",
    "            values='target', \n",
    "            index=['image_path', 'Sampling_Date', 'State', 'Species', 'Pre_GSHH_NDVI', 'Height_Ave_cm'], \n",
    "            columns='target_name', \n",
    "            aggfunc='mean'\n",
    "        ).reset_index()\n",
    "    else:\n",
    "        df['target'] = 0\n",
    "        df_pt = pd.pivot_table(\n",
    "            df, \n",
    "            values='target', \n",
    "            index='image_path', \n",
    "            columns='target_name', \n",
    "            aggfunc='mean'\n",
    "        ).reset_index()\n",
    "    return df_pt\n",
    "\n",
    "def melt_table(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    melted = df.melt(\n",
    "        id_vars='image_path',\n",
    "        value_vars=TARGET_NAMES,\n",
    "        var_name='target_name',\n",
    "        value_name='target'\n",
    "    )\n",
    "    melted['sample_id'] = (\n",
    "        melted['image_path']\n",
    "        .str.replace(r'^.*/', '', regex=True)\n",
    "        .str.replace('.jpg', '', regex=False)\n",
    "        + '__' + melted['target_name']\n",
    "    )\n",
    "    return melted[['sample_id', 'image_path', 'target_name', 'target']]\n",
    "\n",
    "def post_process_biomass(df_preds):\n",
    "    ordered_cols = [\"Dry_Green_g\", \"Dry_Clover_g\", \"Dry_Dead_g\", \"GDM_g\", \"Dry_Total_g\"]\n",
    "    Y = df_preds[ordered_cols].values.T\n",
    "    C = np.array([[1, 1, 0, -1,  0], [0, 0, 1,  1, -1]])\n",
    "    C_T = C.T\n",
    "    inv_CCt = np.linalg.inv(C @ C_T)\n",
    "    P = np.eye(5) - C_T @ inv_CCt @ C\n",
    "    Y_reconciled = P @ Y\n",
    "    Y_reconciled = Y_reconciled.T.clip(min=0)\n",
    "    df_out = df_preds.copy()\n",
    "    df_out[ordered_cols] = Y_reconciled\n",
    "    return df_out\n",
    "\n",
    "def split_image(image, patch_size=520, overlap=16):\n",
    "    h, w, c = image.shape\n",
    "    stride = patch_size - overlap\n",
    "    patches, coords = [], []\n",
    "    for y in range(0, h, stride):\n",
    "        for x in range(0, w, stride):\n",
    "            y1, x1, y2, x2 = y, x, y + patch_size, x + patch_size\n",
    "            patch = image[y1:y2, x1:x2, :]\n",
    "            if patch.shape[0] < patch_size or patch.shape[1] < patch_size:\n",
    "                pad_h = patch_size - patch.shape[0]\n",
    "                pad_w = patch_size - patch.shape[1]\n",
    "                patch = np.pad(patch, ((0,pad_h), (0,pad_w), (0,0)), mode='reflect')\n",
    "            patches.append(patch)\n",
    "            coords.append((y1, x1, y2, x2))\n",
    "    return patches, coords\n",
    "\n",
    "def get_model(model_path: str, device: str = 'cpu'):\n",
    "    model = AutoModel.from_pretrained(model_path, local_files_only=True)\n",
    "    processor = AutoImageProcessor.from_pretrained(model_path)\n",
    "    return model.eval().to(device), processor\n",
    "\n",
    "def compute_embeddings(model_path, df, patch_size=520):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model, processor = get_model(model_path=model_path, device=device)\n",
    "    IMAGE_PATHS, EMBEDDINGS = [], []\n",
    "    for i, row in tqdm(df.iterrows(), total=len(df), desc=\"Computing embeddings\"):\n",
    "        img_path = row['image_path']\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "        patches, coords = split_image(img, patch_size=patch_size)\n",
    "        images = [Image.fromarray(p).convert(\"RGB\") for p in patches]\n",
    "        inputs = processor(images=images, return_tensors=\"pt\").to(model.device)\n",
    "        with torch.no_grad():\n",
    "            if 'siglip' in model_path:\n",
    "                features = model.get_image_features(**inputs)\n",
    "            elif 'dino' in model_path:\n",
    "                features = model(**inputs).pooler_output\n",
    "            else:\n",
    "                raise Exception(\"Model should be dino or siglip\")\n",
    "        embeds = features.mean(dim=0).detach().cpu().numpy()\n",
    "        EMBEDDINGS.append(embeds)\n",
    "        IMAGE_PATHS.append(img_path)\n",
    "    embeddings = np.stack(EMBEDDINGS, axis=0)\n",
    "    n_features = embeddings.shape[1]\n",
    "    emb_columns = [f\"emb{i+1}\" for i in range(n_features)]\n",
    "    emb_df = pd.DataFrame(embeddings, columns=emb_columns)\n",
    "    emb_df['image_path'] = IMAGE_PATHS\n",
    "    df_final = df.merge(emb_df, on='image_path', how='left')\n",
    "    flush()\n",
    "    return df_final\n",
    "\n",
    "def generate_semantic_features(image_embeddings, model_path):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    try:\n",
    "        model = AutoModel.from_pretrained(model_path).to(device)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    except Exception as e:\n",
    "        return None\n",
    "    concept_groups = {\n",
    "        \"bare\": [\"bare soil\", \"dirt ground\", \"sparse vegetation\", \"exposed earth\"],\n",
    "        \"sparse\": [\"low density pasture\", \"thin grass\", \"short clipped grass\"],\n",
    "        \"medium\": [\"average pasture cover\", \"medium height grass\", \"grazed pasture\"],\n",
    "        \"dense\": [\"dense tall pasture\", \"thick grassy volume\", \"high biomass\", \"overgrown vegetation\"],\n",
    "        \"green\": [\"lush green vibrant pasture\", \"photosynthesizing leaves\", \"fresh growth\"],\n",
    "        \"dead\": [\"dry brown dead grass\", \"yellow straw\", \"senesced material\", \"standing hay\"],\n",
    "        \"clover\": [\"white clover\", \"trifolium repens\", \"broadleaf legume\", \"clover flowers\"],\n",
    "        \"grass\": [\"ryegrass\", \"blade-like leaves\", \"fescue\", \"grassy sward\"],\n",
    "        \"weeds\": [\"broadleaf weeds\", \"thistles\", \"non-pasture vegetation\"]\n",
    "    }\n",
    "    concept_vectors = {}\n",
    "    with torch.no_grad():\n",
    "        for name, prompts in concept_groups.items():\n",
    "            inputs = tokenizer(prompts, padding=\"max_length\", return_tensors=\"pt\").to(device)\n",
    "            emb = model.get_text_features(**inputs)\n",
    "            emb = emb / emb.norm(p=2, dim=-1, keepdim=True)\n",
    "            concept_vectors[name] = emb.mean(dim=0, keepdim=True)\n",
    "    if isinstance(image_embeddings, np.ndarray):\n",
    "        img_tensor = torch.tensor(image_embeddings, dtype=torch.float32).to(device)\n",
    "    else:\n",
    "        img_tensor = image_embeddings.to(device)\n",
    "    img_tensor = img_tensor / img_tensor.norm(p=2, dim=-1, keepdim=True)\n",
    "    scores = {}\n",
    "    for name, vec in concept_vectors.items():\n",
    "        scores[name] = torch.matmul(img_tensor, vec.T).cpu().numpy().flatten()\n",
    "    df_scores = pd.DataFrame(scores)\n",
    "    df_scores['ratio_greenness'] = df_scores['green'] / (df_scores['green'] + df_scores['dead'] + 1e-6)\n",
    "    df_scores['ratio_clover'] = df_scores['clover'] / (df_scores['clover'] + df_scores['grass'] + 1e-6)\n",
    "    df_scores['ratio_cover'] = (df_scores['dense'] + df_scores['medium']) / (df_scores['bare'] + df_scores['sparse'] + 1e-6)\n",
    "    df_scores['max_density'] = df_scores[['bare', 'sparse', 'medium', 'dense']].max(axis=1)\n",
    "    return df_scores.values\n",
    "\n",
    "class SupervisedEmbeddingEngine(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, n_pca=0.98, n_pls=8, n_gmm=5, random_state=42):\n",
    "        self.n_pca = n_pca\n",
    "        self.n_pls = n_pls\n",
    "        self.n_gmm = n_gmm\n",
    "        self.random_state = random_state\n",
    "        self.scaler = StandardScaler()\n",
    "        self.pca = PCA(n_components=n_pca, random_state=random_state)\n",
    "        self.pls = PLSRegression(n_components=n_pls, scale=False)\n",
    "        self.gmm = GaussianMixture(n_components=n_gmm, covariance_type='diag', random_state=random_state)\n",
    "        self.pls_fitted_ = False\n",
    "\n",
    "    def fit(self, X, y=None, X_semantic=None):\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        self.pca.fit(X_scaled)\n",
    "        self.gmm.fit(X_scaled)\n",
    "        if y is not None:\n",
    "            y_clean = y.values if hasattr(y, 'values') else y\n",
    "            self.pls.fit(X_scaled, y_clean)\n",
    "            self.pls_fitted_ = True\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, X_semantic=None):\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        return self._generate_features(X_scaled, X_semantic)\n",
    "\n",
    "    def _generate_features(self, X_scaled, X_semantic=None):\n",
    "        features = []\n",
    "        f_pca = self.pca.transform(X_scaled)\n",
    "        features.append(f_pca)\n",
    "        if self.pls_fitted_:\n",
    "            f_pls = self.pls.transform(X_scaled)\n",
    "            features.append(f_pls)\n",
    "        f_gmm = self.gmm.predict_proba(X_scaled)\n",
    "        features.append(f_gmm)\n",
    "        if X_semantic is not None:\n",
    "            sem_norm = (X_semantic - np.mean(X_semantic, axis=0)) / (np.std(X_semantic, axis=0) + 1e-6)\n",
    "            features.append(sem_norm)\n",
    "        return np.hstack(features)\n",
    "\n",
    "def compare_results(oof, train_data):\n",
    "    y_oof_df = pd.DataFrame(oof, columns=TARGET_NAMES)\n",
    "    raw_score = competition_metric(train_data[TARGET_NAMES].values, y_oof_df.values)\n",
    "    print(f\"Raw CV Score: {raw_score:.6f}\")\n",
    "    y_oof_proc = post_process_biomass(y_oof_df)\n",
    "    proc_score = competition_metric(train_data[TARGET_NAMES].values, y_oof_proc.values)\n",
    "    print(f\"Processed CV Score: {proc_score:.6f}\")\n",
    "    print(f\"Improvement: {raw_score - proc_score:.6f}\")\n",
    "\n",
    "def cross_validate(model, train_data, test_data, feature_engine, semantic_train=None, semantic_test=None, target_transform='max', seed=42):\n",
    "    n_splits = train_data['fold'].nunique()\n",
    "    target_max_arr = np.array([TARGET_MAX[t] for t in TARGET_NAMES], dtype=float)\n",
    "    y_true = train_data[TARGET_NAMES]\n",
    "    y_pred = pd.DataFrame(0.0, index=train_data.index, columns=TARGET_NAMES)\n",
    "    y_pred_test = np.zeros([len(test_data), len(TARGET_NAMES)], dtype=float)\n",
    "    \n",
    "    COLUMNS = [col for col in train_data.columns if col.startswith('emb')]\n",
    "    \n",
    "    for fold in range(n_splits):\n",
    "        seeding(seed*(seed//2 + fold))\n",
    "        train_mask = train_data['fold'] != fold\n",
    "        valid_mask = train_data['fold'] == fold\n",
    "        val_idx = train_data[valid_mask].index\n",
    "        X_train_raw = train_data[train_mask][COLUMNS].values\n",
    "        X_valid_raw = train_data[valid_mask][COLUMNS].values\n",
    "        X_test_raw = test_data[COLUMNS].values\n",
    "        sem_train_fold = semantic_train[train_mask] if semantic_train is not None else None\n",
    "        sem_valid_fold = semantic_train[valid_mask] if semantic_train is not None else None\n",
    "        y_train = train_data[train_mask][TARGET_NAMES].values\n",
    "        y_valid = train_data[valid_mask][TARGET_NAMES].values\n",
    "        if target_transform == 'log':\n",
    "            y_train_proc = np.log1p(y_train)\n",
    "        elif target_transform == 'max':\n",
    "            y_train_proc = y_train / target_max_arr\n",
    "        else:\n",
    "            y_train_proc = y_train\n",
    "        engine = deepcopy(feature_engine)\n",
    "        engine.fit(X_train_raw, y=y_train_proc, X_semantic=sem_train_fold)\n",
    "        x_train_eng = engine.transform(X_train_raw, X_semantic=sem_train_fold)\n",
    "        x_valid_eng = engine.transform(X_valid_raw, X_semantic=sem_valid_fold)\n",
    "        x_test_eng = engine.transform(X_test_raw, X_semantic=semantic_test)\n",
    "        fold_valid_pred = np.zeros_like(y_valid)\n",
    "        fold_test_pred = np.zeros([len(test_data), len(TARGET_NAMES)])\n",
    "        for k in range(len(TARGET_NAMES)):\n",
    "            regr = deepcopy(model)\n",
    "            regr.fit(x_train_eng, y_train_proc[:, k])\n",
    "            pred_valid_raw = regr.predict(x_valid_eng)\n",
    "            pred_test_raw = regr.predict(x_test_eng)\n",
    "            if target_transform == 'log':\n",
    "                pred_valid_inv = np.expm1(pred_valid_raw)\n",
    "                pred_test_inv = np.expm1(pred_test_raw)\n",
    "            elif target_transform == 'max':\n",
    "                pred_valid_inv = (pred_valid_raw * target_max_arr[k])\n",
    "                pred_test_inv = (pred_test_raw * target_max_arr[k])\n",
    "            else:\n",
    "                pred_valid_inv = pred_valid_raw\n",
    "                pred_test_inv = pred_test_raw\n",
    "            fold_valid_pred[:, k] = pred_valid_inv\n",
    "            fold_test_pred[:, k] = pred_test_inv\n",
    "        y_pred.loc[val_idx] = fold_valid_pred\n",
    "        y_pred_test += fold_test_pred / n_splits\n",
    "    full_cv = competition_metric(y_true.values, y_pred.values)\n",
    "    print(f\"Full CV Score: {full_cv:.6f}\")\n",
    "    return y_pred.values, y_pred_test\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN FUNCTION WITH 5 SUBMISSIONS\n",
    "# ============================================================================\n",
    "def main():\n",
    "    # Initialize timeout manager - 8.5 hours with 30min buffer\n",
    "    timeout_mgr = TimeoutManager(max_time_seconds=8.5 * 3600)\n",
    "    \n",
    "    seeding(42)\n",
    "    timeout_mgr.log_time(\"Started execution\")\n",
    "    \n",
    "    # Create fallback submission\n",
    "    try:\n",
    "        sample_sub_path = cfg.DATA_PATH / 'sample_submission.csv'\n",
    "        if os.path.exists(sample_sub_path):\n",
    "            sample_sub = pd.read_csv(sample_sub_path)\n",
    "            sample_sub.to_csv('submission.csv', index=False)\n",
    "            print(\"Created fallback submission from sample\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not create fallback submission: {e}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PART 1: SigLIP/Ensemble - Fast Baseline (submission1.csv)\n",
    "    # ========================================================================\n",
    "    if timeout_mgr.should_continue(buffer_seconds=6*3600):\n",
    "        try:\n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(\"PART 1: SigLIP/Ensemble Model (submission1.csv)\")\n",
    "            print(\"=\"*80)\n",
    "            \n",
    "            test_df = pd.read_csv(cfg.DATA_PATH/'test.csv')\n",
    "            test_df = pivot_table(df=test_df)\n",
    "            test_df['image_path'] = test_df['image_path'].apply(lambda p: str(cfg.DATA_PATH / p))\n",
    "            \n",
    "            train_embeddings_path = \"/kaggle/input/csiro-datasplit/csiro_data_split.csv\"\n",
    "            if os.path.exists(train_embeddings_path):\n",
    "                train_siglip_df = pd.read_csv(train_embeddings_path)\n",
    "                print(\"Loaded pre-computed training embeddings\")\n",
    "            else:\n",
    "                print(\"Warning: Pre-computed embeddings not found\")\n",
    "                train_df = pd.read_csv(cfg.DATA_PATH/'train.csv')\n",
    "                train_df = pivot_table(df=train_df)\n",
    "                train_df['image_path'] = train_df['image_path'].apply(lambda p: str(cfg.DATA_PATH / p))\n",
    "                train_siglip_df = train_df\n",
    "            \n",
    "            timeout_mgr.log_time(\"Loaded training data\")\n",
    "            \n",
    "            siglip_path = \"/kaggle/input/google-siglip-so400m-patch14-384/transformers/default/1\"\n",
    "            test_siglip_df = compute_embeddings(model_path=siglip_path, df=test_df, patch_size=520)\n",
    "            flush()\n",
    "            timeout_mgr.log_time(\"Computed test embeddings\")\n",
    "            \n",
    "            X_all_emb = np.vstack([train_siglip_df.filter(like=\"emb\").values, test_siglip_df.filter(like=\"emb\").values])\n",
    "            try:\n",
    "                all_semantic_scores = generate_semantic_features(X_all_emb, model_path=siglip_path)\n",
    "                n_train = len(train_siglip_df)\n",
    "                sem_train_full = all_semantic_scores[:n_train]\n",
    "                sem_test_full = all_semantic_scores[n_train:]\n",
    "                timeout_mgr.log_time(\"Generated semantic features\")\n",
    "            except Exception as e:\n",
    "                print(f\"Semantic feature generation failed: {e}\")\n",
    "                sem_train_full = None\n",
    "                sem_test_full = None\n",
    "            \n",
    "            feat_engine = SupervisedEmbeddingEngine(n_pca=0.80, n_pls=8, n_gmm=6)\n",
    "            \n",
    "            print(\"\\nRunning LightGBM...\")\n",
    "            oof_lgbm, pred_test_lgbm = cross_validate(\n",
    "                LGBMRegressor(verbose=-1, n_estimators=100),\n",
    "                train_siglip_df, test_siglip_df, \n",
    "                feature_engine=feat_engine, \n",
    "                semantic_train=sem_train_full, \n",
    "                semantic_test=sem_test_full, \n",
    "                target_transform='max'\n",
    "            )\n",
    "            compare_results(oof_lgbm, train_siglip_df)\n",
    "            \n",
    "            print(\"\\nRunning CatBoost...\")\n",
    "            oof_cat, pred_test_cat = cross_validate(\n",
    "                CatBoostRegressor(verbose=0, iterations=100),\n",
    "                train_siglip_df, test_siglip_df, \n",
    "                feature_engine=feat_engine, \n",
    "                semantic_train=sem_train_full, \n",
    "                semantic_test=sem_test_full\n",
    "            )\n",
    "            compare_results(oof_cat, train_siglip_df)\n",
    "            \n",
    "            pred_test = (pred_test_lgbm + pred_test_cat) / 2\n",
    "            test_df[TARGET_NAMES] = pred_test\n",
    "            test_df = post_process_biomass(test_df)\n",
    "            sub_df = melt_table(test_df)\n",
    "            sub_df[['sample_id', 'target']].to_csv(\"submission1.csv\", index=False)\n",
    "            \n",
    "            print(\"✓ submission1.csv created successfully\")\n",
    "            timeout_mgr.log_time(\"Completed submission1\")\n",
    "            flush()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ submission1 failed: {e}\")\n",
    "            try:\n",
    "                sample = pd.read_csv(cfg.DATA_PATH / 'sample_submission.csv')\n",
    "                sample.to_csv(\"submission1.csv\", index=False)\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PART 2: XGBoost Variant (submission4.csv) - Quick alternative ensemble\n",
    "    # ========================================================================\n",
    "    if timeout_mgr.should_continue(buffer_seconds=5*3600):\n",
    "        try:\n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(\"PART 2: XGBoost Variant (submission4.csv)\")\n",
    "            print(\"=\"*80)\n",
    "            \n",
    "            # Reuse embeddings from Part 1\n",
    "            feat_engine_xgb = SupervisedEmbeddingEngine(n_pca=0.85, n_pls=10, n_gmm=8)\n",
    "            \n",
    "            print(\"\\nRunning XGBoost...\")\n",
    "            oof_xgb, pred_test_xgb = cross_validate(\n",
    "                XGBRegressor(n_estimators=120, learning_rate=0.05, max_depth=4, verbosity=0),\n",
    "                train_siglip_df, test_siglip_df,\n",
    "                feature_engine=feat_engine_xgb,\n",
    "                semantic_train=sem_train_full,\n",
    "                semantic_test=sem_test_full,\n",
    "                target_transform='max'\n",
    "            )\n",
    "            compare_results(oof_xgb, train_siglip_df)\n",
    "            \n",
    "            print(\"\\nRunning Ridge...\")\n",
    "            oof_ridge, pred_test_ridge = cross_validate(\n",
    "                Ridge(alpha=10.0),\n",
    "                train_siglip_df, test_siglip_df,\n",
    "                feature_engine=feat_engine_xgb,\n",
    "                semantic_train=sem_train_full,\n",
    "                semantic_test=sem_test_full,\n",
    "                target_transform='max'\n",
    "            )\n",
    "            compare_results(oof_ridge, train_siglip_df)\n",
    "            \n",
    "            # Blend XGBoost and Ridge\n",
    "            pred_test_4 = 0.7 * pred_test_xgb + 0.3 * pred_test_ridge\n",
    "            test_df_4 = test_df.copy()\n",
    "            test_df_4[TARGET_NAMES] = pred_test_4\n",
    "            test_df_4 = post_process_biomass(test_df_4)\n",
    "            sub_df_4 = melt_table(test_df_4)\n",
    "            sub_df_4[['sample_id', 'target']].to_csv(\"submission4.csv\", index=False)\n",
    "            \n",
    "            print(\"✓ submission4.csv created successfully\")\n",
    "            timeout_mgr.log_time(\"Completed submission4\")\n",
    "            flush()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ submission4 failed: {e}\")\n",
    "            try:\n",
    "                if os.path.exists(\"submission1.csv\"):\n",
    "                    shutil.copy(\"submission1.csv\", \"submission4.csv\")\n",
    "                else:\n",
    "                    sample = pd.read_csv(cfg.DATA_PATH / 'sample_submission.csv')\n",
    "                    sample.to_csv(\"submission4.csv\", index=False)\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PART 3: Gradient Boosting Ensemble (submission5.csv)\n",
    "    # ========================================================================\n",
    "    if timeout_mgr.should_continue(buffer_seconds=4*3600):\n",
    "        try:\n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(\"PART 3: Gradient Boosting Ensemble (submission5.csv)\")\n",
    "            print(\"=\"*80)\n",
    "            \n",
    "            feat_engine_gb = SupervisedEmbeddingEngine(n_pca=0.90, n_pls=6, n_gmm=4)\n",
    "            \n",
    "            print(\"\\nRunning HistGradientBoosting...\")\n",
    "            oof_hgb, pred_test_hgb = cross_validate(\n",
    "                HistGradientBoostingRegressor(max_iter=100, learning_rate=0.1, max_depth=5),\n",
    "                train_siglip_df, test_siglip_df,\n",
    "                feature_engine=feat_engine_gb,\n",
    "                semantic_train=sem_train_full,\n",
    "                semantic_test=sem_test_full,\n",
    "                target_transform='log'\n",
    "            )\n",
    "            compare_results(oof_hgb, train_siglip_df)\n",
    "            \n",
    "            print(\"\\nRunning ExtraTrees...\")\n",
    "            oof_et, pred_test_et = cross_validate(\n",
    "                ExtraTreesRegressor(n_estimators=100, max_depth=10, n_jobs=-1),\n",
    "                train_siglip_df, test_siglip_df,\n",
    "                feature_engine=feat_engine_gb,\n",
    "                semantic_train=sem_train_full,\n",
    "                semantic_test=sem_test_full,\n",
    "                target_transform='max'\n",
    "            )\n",
    "            compare_results(oof_et, train_siglip_df)\n",
    "            \n",
    "            # Blend HistGradientBoosting and ExtraTrees\n",
    "            pred_test_5 = 0.6 * pred_test_hgb + 0.4 * pred_test_et\n",
    "            test_df_5 = test_df.copy()\n",
    "            test_df_5[TARGET_NAMES] = pred_test_5\n",
    "            test_df_5 = post_process_biomass(test_df_5)\n",
    "            sub_df_5 = melt_table(test_df_5)\n",
    "            sub_df_5[['sample_id', 'target']].to_csv(\"submission5.csv\", index=False)\n",
    "            \n",
    "            print(\"✓ submission5.csv created successfully\")\n",
    "            timeout_mgr.log_time(\"Completed submission5\")\n",
    "            flush()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ submission5 failed: {e}\")\n",
    "            try:\n",
    "                if os.path.exists(\"submission1.csv\"):\n",
    "                    shutil.copy(\"submission1.csv\", \"submission5.csv\")\n",
    "                else:\n",
    "                    sample = pd.read_csv(cfg.DATA_PATH / 'sample_submission.csv')\n",
    "                    sample.to_csv(\"submission5.csv\", index=False)\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PART 4: DINO Model - Only if sufficient time (submission2 & submission3)\n",
    "    # Note: These are computationally expensive, so only run if we have time\n",
    "    # ========================================================================\n",
    "    \n",
    "    # For now, create copies of best submissions as placeholders\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Creating submission2 and submission3 placeholders\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # submission2 and submission3: Use best available or create ensembles\n",
    "    try:\n",
    "        # submission2: Weighted ensemble of submission1 and submission4\n",
    "        if os.path.exists(\"submission1.csv\") and os.path.exists(\"submission4.csv\"):\n",
    "            sub1 = pd.read_csv(\"submission1.csv\")\n",
    "            sub4 = pd.read_csv(\"submission4.csv\")\n",
    "            merged = pd.merge(sub1, sub4, on='sample_id', suffixes=('_1', '_4'))\n",
    "            merged['target'] = 0.6 * merged['target_1'] + 0.4 * merged['target_4']\n",
    "            merged[['sample_id', 'target']].to_csv(\"submission2.csv\", index=False)\n",
    "            print(\"✓ submission2.csv created as weighted ensemble\")\n",
    "        else:\n",
    "            shutil.copy(\"submission1.csv\", \"submission2.csv\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ submission2 creation failed: {e}\")\n",
    "    \n",
    "    try:\n",
    "        # submission3: Weighted ensemble of submission1, submission4, and submission5\n",
    "        if all(os.path.exists(f\"submission{i}.csv\") for i in [1, 4, 5]):\n",
    "            sub1 = pd.read_csv(\"submission1.csv\")\n",
    "            sub4 = pd.read_csv(\"submission4.csv\")\n",
    "            sub5 = pd.read_csv(\"submission5.csv\")\n",
    "            \n",
    "            merged = pd.merge(sub1, sub4, on='sample_id', suffixes=('_1', '_4'))\n",
    "            merged = pd.merge(merged, sub5, on='sample_id')\n",
    "            merged = merged.rename(columns={'target': 'target_5'})\n",
    "            \n",
    "            merged['target'] = 0.5 * merged['target_1'] + 0.3 * merged['target_4'] + 0.2 * merged['target_5']\n",
    "            merged[['sample_id', 'target']].to_csv(\"submission3.csv\", index=False)\n",
    "            print(\"✓ submission3.csv created as weighted ensemble\")\n",
    "        else:\n",
    "            shutil.copy(\"submission1.csv\", \"submission3.csv\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ submission3 creation failed: {e}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PART 5: Create Final Ensemble (submission.csv)\n",
    "    # ========================================================================\n",
    "    try:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"Creating Final Ensemble (submission.csv)\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Load all submissions\n",
    "        subs = {}\n",
    "        for i in range(1, 6):\n",
    "            path = f'submission{i}.csv'\n",
    "            if os.path.exists(path):\n",
    "                subs[i] = pd.read_csv(path)\n",
    "                print(f\"✓ Loaded submission{i}.csv\")\n",
    "        \n",
    "        if len(subs) >= 3:\n",
    "            # Multi-way merge\n",
    "            merged = subs[1].copy()\n",
    "            merged = merged.rename(columns={'target': 'target_1'})\n",
    "            \n",
    "            for i in range(2, 6):\n",
    "                if i in subs:\n",
    "                    temp = subs[i].rename(columns={'target': f'target_{i}'})\n",
    "                    merged = pd.merge(merged, temp, on='sample_id', how='left')\n",
    "            \n",
    "            # Weighted ensemble - prioritize diverse approaches\n",
    "            weights = {\n",
    "                'target_1': 0.35,  # SigLIP baseline\n",
    "                'target_2': 0.15,  # Ensemble variant 1\n",
    "                'target_3': 0.15,  # Ensemble variant 2\n",
    "                'target_4': 0.20,  # XGBoost variant\n",
    "                'target_5': 0.15,  # Gradient boosting variant\n",
    "            }\n",
    "            \n",
    "            merged['target'] = 0\n",
    "            for col, weight in weights.items():\n",
    "                if col in merged.columns:\n",
    "                    merged['target'] += merged[col].fillna(merged['target_1']) * weight\n",
    "            \n",
    "            final_submission = merged[['sample_id', 'target']]\n",
    "            final_submission.to_csv('submission.csv', index=False)\n",
    "            \n",
    "            print(\"✓ Final ensemble submission created successfully\")\n",
    "            timeout_mgr.log_time(\"Completed final ensemble\")\n",
    "            \n",
    "        else:\n",
    "            # Fallback to best single submission\n",
    "            shutil.copy('submission1.csv', 'submission.csv')\n",
    "            print(\"⚠ Using submission1 as final submission\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Ensemble failed: {e}\")\n",
    "        for fallback in ['submission1.csv', 'submission4.csv', 'submission5.csv']:\n",
    "            if os.path.exists(fallback):\n",
    "                shutil.copy(fallback, 'submission.csv')\n",
    "                print(f\"Using {fallback} as final submission\")\n",
    "                break\n",
    "    \n",
    "    timeout_mgr.log_time(\"Execution completed\")\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EXECUTION COMPLETE - 5 Submissions Created\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"Summary:\")\n",
    "    for i in range(1, 6):\n",
    "        if os.path.exists(f'submission{i}.csv'):\n",
    "            print(f\"  ✓ submission{i}.csv\")\n",
    "    if os.path.exists('submission.csv'):\n",
    "        print(f\"  ✓ submission.csv (final ensemble)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except Exception as e:\n",
    "        print(f\"CRITICAL ERROR: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        try:\n",
    "            sample_path = \"/kaggle/input/csiro-biomass/sample_submission.csv\"\n",
    "            if os.path.exists(sample_path):\n",
    "                sample = pd.read_csv(sample_path)\n",
    "                sample.to_csv('submission.csv', index=False)\n",
    "                print(\"Created emergency fallback submission\")\n",
    "        except:\n",
    "            print(\"Could not create emergency fallback\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdcf3ab1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T12:09:06.026552Z",
     "iopub.status.busy": "2026-01-15T12:09:06.025957Z",
     "iopub.status.idle": "2026-01-15T12:09:06.056845Z",
     "shell.execute_reply": "2026-01-15T12:09:06.055836Z"
    },
    "papermill": {
     "duration": 0.037344,
     "end_time": "2026-01-15T12:09:06.058474",
     "exception": false,
     "start_time": "2026-01-15T12:09:06.021130",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CREATING WEIGHTED ENSEMBLE\n",
      "================================================================================\n",
      "\n",
      "1. Loading submissions...\n",
      "   ✓ Loaded 5 submissions\n",
      "   - submission1.csv: 5 rows\n",
      "   - submission2.csv: 5 rows\n",
      "   - submission3.csv: 5 rows\n",
      "   - submission4.csv: 5 rows\n",
      "   - submission5.csv: 5 rows\n",
      "\n",
      "2. Renaming columns...\n",
      "   ✓ Renamed all target columns\n",
      "\n",
      "3. Merging submissions...\n",
      "   ✓ Merged shape: (5, 6)\n",
      "\n",
      "4. Applying weights:\n",
      "   - Submission 1: 0.19\n",
      "   - Submission 2: 0.27\n",
      "   - Submission 3: 0.22\n",
      "   - Submission 4: 0.22\n",
      "   - Submission 5: 0.10\n",
      "   - Total: 1.00\n",
      "\n",
      "5. Saving submission...\n",
      "   ✓ Saved to submission.csv\n",
      "\n",
      "6. Preview of submission:\n",
      "================================================================================\n",
      "                    sample_id     target\n",
      "0  ID1001187975__Dry_Clover_g   4.168118\n",
      "1    ID1001187975__Dry_Dead_g  26.374305\n",
      "2   ID1001187975__Dry_Green_g  23.723169\n",
      "3   ID1001187975__Dry_Total_g  54.265593\n",
      "4         ID1001187975__GDM_g  27.891288\n",
      "\n",
      "7. Submission Statistics:\n",
      "================================================================================\n",
      "   Total samples: 5\n",
      "   Mean target: 27.2845\n",
      "   Std target: 17.8622\n",
      "   Min target: 4.1681\n",
      "   Max target: 54.2656\n",
      "   Median target: 26.3743\n",
      "\n",
      "================================================================================\n",
      "✅ ENSEMBLE COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "Final file: submission.csv\n",
      "Ready to submit to competition!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Simple Ensemble Script - Exactly as Requested\n",
    "Creates submission.csv with your specified weights\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CREATING WEIGHTED ENSEMBLE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load all submissions\n",
    "print(\"\\n1. Loading submissions...\")\n",
    "submission1 = pd.read_csv('/kaggle/working/submission1.csv')\n",
    "submission2 = pd.read_csv('/kaggle/working/submission2.csv')\n",
    "submission3 = pd.read_csv('/kaggle/working/submission3.csv')\n",
    "submission4 = pd.read_csv('/kaggle/working/submission4.csv')\n",
    "submission5 = pd.read_csv('/kaggle/working/submission5.csv')\n",
    "\n",
    "print(f\"   ✓ Loaded 5 submissions\")\n",
    "print(f\"   - submission1.csv: {len(submission1)} rows\")\n",
    "print(f\"   - submission2.csv: {len(submission2)} rows\")\n",
    "print(f\"   - submission3.csv: {len(submission3)} rows\")\n",
    "print(f\"   - submission4.csv: {len(submission4)} rows\")\n",
    "print(f\"   - submission5.csv: {len(submission5)} rows\")\n",
    "\n",
    "# Rename target columns\n",
    "print(\"\\n2. Renaming columns...\")\n",
    "submission1 = submission1.rename(columns={'target': 'target_1'})\n",
    "submission2 = submission2.rename(columns={'target': 'target_2'})\n",
    "submission3 = submission3.rename(columns={'target': 'target_3'})\n",
    "submission4 = submission4.rename(columns={'target': 'target_4'})\n",
    "submission5 = submission5.rename(columns={'target': 'target_5'})\n",
    "print(\"   ✓ Renamed all target columns\")\n",
    "\n",
    "# Merge all submissions\n",
    "print(\"\\n3. Merging submissions...\")\n",
    "merged = pd.merge(submission1, submission2, on='sample_id')\n",
    "merged = pd.merge(merged, submission3, on='sample_id')\n",
    "merged = pd.merge(merged, submission4, on='sample_id')\n",
    "merged = pd.merge(merged, submission5, on='sample_id')\n",
    "print(f\"   ✓ Merged shape: {merged.shape}\")\n",
    "\n",
    "# Your specified weights\n",
    "weight1, weight2, weight3, weight4, weight5 = 0.19, 0.27, 0.22, 0.22, 0.10\n",
    "\n",
    "print(\"\\n4. Applying weights:\")\n",
    "print(f\"   - Submission 1: {weight1:.2f}\")\n",
    "print(f\"   - Submission 2: {weight2:.2f}\")\n",
    "print(f\"   - Submission 3: {weight3:.2f}\")\n",
    "print(f\"   - Submission 4: {weight4:.2f}\")\n",
    "print(f\"   - Submission 5: {weight5:.2f}\")\n",
    "print(f\"   - Total: {weight1 + weight2 + weight3 + weight4 + weight5:.2f}\")\n",
    "\n",
    "# Calculate weighted average\n",
    "merged['target'] = (merged['target_1'] * weight1 +\n",
    "                     merged['target_2'] * weight2 +\n",
    "                     merged['target_3'] * weight3 +\n",
    "                     merged['target_4'] * weight4 +\n",
    "                     merged['target_5'] * weight5)\n",
    "\n",
    "# Create final submission\n",
    "submission = merged[['sample_id', 'target']]\n",
    "\n",
    "# Save to file\n",
    "print(\"\\n5. Saving submission...\")\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"   ✓ Saved to submission.csv\")\n",
    "\n",
    "# Display preview\n",
    "print(\"\\n6. Preview of submission:\")\n",
    "print(\"=\"*80)\n",
    "print(submission.head(20))\n",
    "\n",
    "# Statistics\n",
    "print(\"\\n7. Submission Statistics:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"   Total samples: {len(submission)}\")\n",
    "print(f\"   Mean target: {submission['target'].mean():.4f}\")\n",
    "print(f\"   Std target: {submission['target'].std():.4f}\")\n",
    "print(f\"   Min target: {submission['target'].min():.4f}\")\n",
    "print(f\"   Max target: {submission['target'].max():.4f}\")\n",
    "print(f\"   Median target: {submission['target'].median():.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✅ ENSEMBLE COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nFinal file: submission.csv\")\n",
    "print(\"Ready to submit to competition!\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 14254895,
     "isSourceIdPinned": false,
     "sourceId": 112509,
     "sourceType": "competition"
    },
    {
     "datasetId": 8856212,
     "isSourceIdPinned": false,
     "sourceId": 13900620,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8929818,
     "isSourceIdPinned": false,
     "sourceId": 14018229,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9263389,
     "sourceId": 14503682,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9263405,
     "sourceId": 14503701,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 288467413,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 288761108,
     "sourceType": "kernelVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 986,
     "modelInstanceId": 3329,
     "sourceId": 4537,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 251887,
     "modelInstanceId": 230141,
     "sourceId": 268942,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 487624,
     "modelInstanceId": 471723,
     "sourceId": 663314,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 176.104423,
   "end_time": "2026-01-15T12:09:09.182822",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-15T12:06:13.078399",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "29c433b2ffe44180b8731dcb46a20dcc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_5d4d9eac36d14d73930bbc7cf9e8bdae",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_467c5f7affe54d86bcbf7f0da110791a",
       "tabbable": null,
       "tooltip": null,
       "value": 1.0
      }
     },
     "3dca39c8164044799f7419d3e1f3370f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_db7573ada04548978197fe396a288c40",
        "IPY_MODEL_29c433b2ffe44180b8731dcb46a20dcc",
        "IPY_MODEL_8313d603f8554721b97f15fc0aed6921"
       ],
       "layout": "IPY_MODEL_8158a6b7a4db4227909d7852433177f1",
       "tabbable": null,
       "tooltip": null
      }
     },
     "421cae86be5745239202d86a7d9e905f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "467c5f7affe54d86bcbf7f0da110791a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "5d4d9eac36d14d73930bbc7cf9e8bdae": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6343acd406124fc49c91590c6c932b90": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "64de79084f6c48afb104aec1678cc7d0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6bbd05b7f14b4e47839c879a460face5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "8158a6b7a4db4227909d7852433177f1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8313d603f8554721b97f15fc0aed6921": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_64de79084f6c48afb104aec1678cc7d0",
       "placeholder": "​",
       "style": "IPY_MODEL_6bbd05b7f14b4e47839c879a460face5",
       "tabbable": null,
       "tooltip": null,
       "value": " 1/1 [00:02&lt;00:00,  2.26s/it]"
      }
     },
     "db7573ada04548978197fe396a288c40": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_421cae86be5745239202d86a7d9e905f",
       "placeholder": "​",
       "style": "IPY_MODEL_6343acd406124fc49c91590c6c932b90",
       "tabbable": null,
       "tooltip": null,
       "value": "Computing embeddings: 100%"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
