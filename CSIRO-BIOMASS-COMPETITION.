{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c4b92d4",
   "metadata": {
    "_cell_guid": "f53f0eae-4d26-4bb8-89a6-f7450ede2dea",
    "_uuid": "c0ddd839-4e54-4dc6-823b-d7db8db1f672",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-01-11T14:13:12.061313Z",
     "iopub.status.busy": "2026-01-11T14:13:12.061030Z",
     "iopub.status.idle": "2026-01-11T14:13:13.160016Z",
     "shell.execute_reply": "2026-01-11T14:13:13.158984Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 1.105619,
     "end_time": "2026-01-11T14:13:13.162524",
     "exception": false,
     "start_time": "2026-01-11T14:13:12.056905",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/csiro-mvp-models/model1.pth\n",
      "/kaggle/input/csiro-mvp-models/model2.pth\n",
      "/kaggle/input/csiro-mvp-models/model10.pth\n",
      "/kaggle/input/csiro-mvp-models/model7.pth\n",
      "/kaggle/input/csiro-mvp-models/model6.pth\n",
      "/kaggle/input/csiro-mvp-models/model4.pth\n",
      "/kaggle/input/csiro-mvp-models/model5.pth\n",
      "/kaggle/input/csiro-mvp-models/model9.pth\n",
      "/kaggle/input/csiro-mvp-models/model3.pth\n",
      "/kaggle/input/csiro-mvp-models/model8.pth\n",
      "/kaggle/input/csiro-biomass-competition-comprehensive-eda/__results__.html\n",
      "/kaggle/input/csiro-biomass-competition-comprehensive-eda/__notebook__.ipynb\n",
      "/kaggle/input/csiro-biomass-competition-comprehensive-eda/__output__.json\n",
      "/kaggle/input/csiro-biomass-competition-comprehensive-eda/custom.css\n",
      "/kaggle/input/csiro-biomass-competition-comprehensive-eda/__results___files/__results___15_0.png\n",
      "/kaggle/input/csiro-biomass-competition-comprehensive-eda/__results___files/__results___25_0.png\n",
      "/kaggle/input/csiro-biomass-competition-comprehensive-eda/__results___files/__results___19_0.png\n",
      "/kaggle/input/csiro-biomass-competition-comprehensive-eda/__results___files/__results___31_0.png\n",
      "/kaggle/input/csiro-biomass-competition-comprehensive-eda/__results___files/__results___22_0.png\n",
      "/kaggle/input/csiro-biomass-competition-comprehensive-eda/__results___files/__results___28_0.png\n",
      "/kaggle/input/csiro-image-embeddings/train_siglip_embeddings.csv\n",
      "/kaggle/input/csiro-image-embeddings/train_dino_embeddings.csv\n",
      "/kaggle/input/dinov2/pytorch/giant/1/config.json\n",
      "/kaggle/input/dinov2/pytorch/giant/1/preprocessor_config.json\n",
      "/kaggle/input/dinov2/pytorch/giant/1/README.md\n",
      "/kaggle/input/dinov2/pytorch/giant/1/pytorch_model.bin\n",
      "/kaggle/input/dinov2/pytorch/giant/1/.gitattributes\n",
      "/kaggle/input/csiro-datasplit/__results__.html\n",
      "/kaggle/input/csiro-datasplit/csiro_data_split.csv\n",
      "/kaggle/input/csiro-datasplit/__notebook__.ipynb\n",
      "/kaggle/input/csiro-datasplit/__output__.json\n",
      "/kaggle/input/csiro-datasplit/custom.css\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_0/metrics.csv\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_0/swanlab_info.json\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_0/checkpoints/best_loss.pt\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_0/checkpoints/best_wr2.pt\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_0/checkpoints/last.pt\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_4/metrics.csv\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_4/swanlab_info.json\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_4/checkpoints/best_loss.pt\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_4/checkpoints/best_wr2.pt\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_4/checkpoints/last.pt\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_1/metrics.csv\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_1/swanlab_info.json\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_1/checkpoints/best_loss.pt\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_1/checkpoints/best_wr2.pt\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_1/checkpoints/last.pt\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_3/metrics.csv\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_3/swanlab_info.json\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_3/checkpoints/best_loss.pt\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_3/checkpoints/best_wr2.pt\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_3/checkpoints/last.pt\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_2/metrics.csv\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_2/swanlab_info.json\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_2/checkpoints/best_loss.pt\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_2/checkpoints/best_wr2.pt\n",
      "/kaggle/input/csiro/pytorch/default/12/fold_2/checkpoints/last.pt\n",
      "/kaggle/input/csiro-biomass/sample_submission.csv\n",
      "/kaggle/input/csiro-biomass/train.csv\n",
      "/kaggle/input/csiro-biomass/test.csv\n",
      "/kaggle/input/csiro-biomass/test/ID1001187975.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2099464826.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2037861084.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1211362607.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1853508321.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID193102215.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID698608346.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1859251563.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1880764911.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID853954911.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1403107574.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1781353117.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID384648061.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1563418511.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2125100696.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID482555369.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID638711343.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID779628955.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1876271942.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1692894460.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID746335827.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1136169672.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1471216911.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID846154859.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1294770420.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1183807388.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID423506847.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1889150649.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1140993511.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1413758094.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1545077474.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID95050718.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID528010569.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1645161155.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID786365141.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID896386823.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1025234388.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID663006174.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1509266870.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1496750796.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID471758347.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID740402124.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1624268863.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1098771283.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID710341728.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2086966681.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1573329652.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID54128926.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID50027657.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1559189397.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID290369222.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1590632667.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID552040066.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID488873801.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID363069566.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1839139621.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1131079710.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2010625680.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID152157478.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1357758282.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1498398599.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID679913293.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID697718693.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID4464212.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1275072698.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1579942839.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID799079114.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1415329644.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1510574031.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1078930021.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1456861072.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID930534670.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID13162390.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID567744300.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID344618040.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID566966892.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1437386574.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID667059550.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID72895391.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1193692654.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1386202352.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID871463897.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2096636211.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2003438517.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID21377800.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID230058600.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1753847361.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1512751450.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID12390962.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1746343319.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID978026131.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID383231615.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID146920896.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1036339023.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1168534540.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1859792585.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1251029854.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1113329413.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1874904894.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1671844336.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1831254380.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1103883611.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID797502182.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1784585001.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1058383417.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1488408526.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID429799190.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1291116815.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1516374298.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1618597318.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1345375788.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID686797154.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1139866256.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1149598723.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID212206250.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID112966473.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1540480250.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID544444725.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1513184765.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID668330410.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1444674500.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1962379474.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID605134229.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID914754166.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID354528442.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID950496197.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1395011773.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1357768767.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID210865340.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID936984905.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1976436386.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1215977190.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID803479541.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1244346858.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID158170916.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1208644039.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1314135397.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1012260530.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1053972079.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID656251220.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1084819986.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1337107565.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1268934251.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID617132135.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1472525822.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID668475812.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID681680726.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1476045099.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1570190541.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1403078396.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2030696575.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1782608354.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID194823383.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID196516535.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID212206832.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1638922597.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1457700382.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1989506559.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID789169173.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1634731537.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1428837636.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2006686196.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID885388135.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1789853061.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1655778545.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID697059386.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID121331988.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2099742797.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID342818398.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID317990700.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID706288721.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1159071020.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID755710743.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1254829053.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID475010202.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1693880739.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1894998379.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID48303557.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1385921939.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID147528735.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID407646960.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1035947949.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1119761112.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1988033238.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1857489997.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID742198710.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID588120964.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID431471530.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID353424190.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID380752847.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2069766023.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID600602588.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID560946727.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1011485656.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID808079729.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1217108125.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1623964968.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID980878870.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID793526563.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID397994621.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID975115267.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1237349078.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID684383343.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID866684633.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1665142816.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2048645043.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1953171547.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1451025862.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID71885430.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID307060225.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID969218269.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID980538882.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1028611175.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID670276799.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2002797732.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1374789439.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID473494649.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1993907137.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1962197151.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID828217731.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID972274220.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1954669045.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1354190372.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1458758610.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID40849327.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1952813879.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID572336285.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1473228876.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1963715583.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1463690813.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1899025384.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID386216505.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1789265307.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID315357834.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2089023774.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID520514019.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1970522802.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1139918758.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1051144034.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1370004842.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID761508093.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2052993274.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1277756619.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID6269659.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1574125908.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID135365668.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1182523622.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID554314721.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1049634115.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1127246618.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID900012207.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID574213894.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID415656958.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID61833032.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2053315094.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID550623196.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID657448172.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1675365449.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2014192906.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID162394992.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID968643034.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID684062938.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID802547515.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID294150104.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1618145129.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID956512130.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID142751858.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID325799913.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID443091455.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID661372352.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1062837331.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID498304885.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID187238869.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1450399782.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2056023629.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID576621307.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1199150612.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1411613934.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID105271783.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1703304524.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID875119737.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1176292407.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1729002155.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2091439402.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID576137678.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1946311744.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1982662138.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID983582017.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID661817669.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID753699705.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1789834546.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID529933668.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID490139972.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID743847993.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID7850481.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1088965591.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID629980789.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1119739385.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1477176296.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1113121340.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2131261930.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2145635095.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1414371018.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1148666289.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID839432753.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID157479394.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1761544403.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID846984946.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID751517087.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID577112774.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID353997899.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID748979397.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1070112260.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1108283583.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1868719645.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1980675327.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1163061745.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1148528732.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID534966093.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1717006117.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1953218650.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID633775166.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID808093827.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1997244125.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1920959057.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1948354837.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID364856705.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID249042826.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID332742639.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1680597197.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1421714468.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID905397692.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1782509721.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID141370843.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2056982009.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID94564238.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID8209776.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID908524512.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID610397481.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID750820644.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1515990019.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1547945326.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID587125778.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1620371305.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1474775613.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID545360459.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1783499590.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1249094008.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1525817840.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID227847873.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1052620238.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1888700589.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID2052442675.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID963903358.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1121692672.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1343327476.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID1667778338.jpg\n",
      "/kaggle/input/csiro-biomass/train/ID257822026.jpg\n",
      "/kaggle/input/google-siglip-so400m-patch14-384/transformers/default/1/config.json\n",
      "/kaggle/input/google-siglip-so400m-patch14-384/transformers/default/1/preprocessor_config.json\n",
      "/kaggle/input/google-siglip-so400m-patch14-384/transformers/default/1/spiece.model\n",
      "/kaggle/input/google-siglip-so400m-patch14-384/transformers/default/1/README.md\n",
      "/kaggle/input/google-siglip-so400m-patch14-384/transformers/default/1/tokenizer.json\n",
      "/kaggle/input/google-siglip-so400m-patch14-384/transformers/default/1/tokenizer_config.json\n",
      "/kaggle/input/google-siglip-so400m-patch14-384/transformers/default/1/gitattributes\n",
      "/kaggle/input/google-siglip-so400m-patch14-384/transformers/default/1/model.safetensors\n",
      "/kaggle/input/google-siglip-so400m-patch14-384/transformers/default/1/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9294709e",
   "metadata": {
    "_cell_guid": "4bbbc35a-1b46-41f0-ae38-a4cd5616aeaa",
    "_uuid": "900028d6-5e27-4e09-a86f-dcd3d7d5a2b5",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-01-11T14:13:13.169955Z",
     "iopub.status.busy": "2026-01-11T14:13:13.169596Z",
     "iopub.status.idle": "2026-01-11T14:13:38.747083Z",
     "shell.execute_reply": "2026-01-11T14:13:38.746218Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 25.583233,
     "end_time": "2026-01-11T14:13:38.748737",
     "exception": false,
     "start_time": "2026-01-11T14:13:13.165504",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CSIRO BIOMASS COMPETITION - PROFESSIONAL EDA\n",
      "================================================================================\n",
      "\n",
      "Loading data...\n",
      "Train shape: (1785, 9)\n",
      "Test shape: (5, 3)\n",
      "Pivoted train shape: (357, 6)\n",
      "\n",
      "================================================================================\n",
      "Creating Visualization 1: Target Distributions with Normal Curves\n",
      "================================================================================\n",
      "✓ Saved: professional_1_distributions.png\n",
      "\n",
      "================================================================================\n",
      "Creating Visualization 2: Complete Correlation Matrix\n",
      "================================================================================\n",
      "✓ Saved: professional_2_correlation_matrix.png\n",
      "\n",
      "================================================================================\n",
      "Creating Visualization 3: Detailed Box Plots\n",
      "================================================================================\n",
      "✓ Saved: professional_3_boxplots.png\n",
      "\n",
      "================================================================================\n",
      "Creating Visualization 4: Scatter Plot Matrix\n",
      "================================================================================\n",
      "✓ Saved: professional_4_scatter_matrix.png\n",
      "\n",
      "================================================================================\n",
      "Creating Visualization 5: Detailed Sample Image Analysis\n",
      "================================================================================\n",
      "✓ Saved: professional_5_sample_images.png\n",
      "\n",
      "================================================================================\n",
      "Creating Visualization 6: Hierarchical Relationships\n",
      "================================================================================\n",
      "✓ Saved: professional_6_hierarchical.png\n",
      "\n",
      "================================================================================\n",
      "EDA COMPLETE - ALL VISUALIZATIONS SAVED\n",
      "================================================================================\n",
      "\n",
      "Generated Professional Visualizations:\n",
      "  1. professional_1_distributions.png - Target distributions with bell curves\n",
      "  2. professional_2_correlation_matrix.png - Complete correlation analysis\n",
      "  3. professional_3_boxplots.png - Detailed box plots with statistics\n",
      "  4. professional_4_scatter_matrix.png - Scatter plot matrix with regression\n",
      "  5. professional_5_sample_images.png - Detailed image analysis\n",
      "  6. professional_6_hierarchical.png - Hierarchical relationship validation\n",
      "\n",
      "All visualizations use consistent professional color scheme\n",
      "Statistical details and annotations included on all plots\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# CSIRO BIOMASS COMPETITION - PROFESSIONAL EDA\n",
    "# ================================================================\n",
    "# Professional visualizations with consistent color scheme\n",
    "# Complete statistical analysis with bell curves and annotations\n",
    "# ================================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import skew, kurtosis, normaltest, pearsonr, spearmanr, norm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# ================================================================\n",
    "# PROFESSIONAL STYLING CONFIGURATION\n",
    "# ================================================================\n",
    "\n",
    "# Professional color palette (consistent across all plots)\n",
    "COLORS = {\n",
    "    'primary': '#2E4057',      # Dark blue-gray (main)\n",
    "    'secondary': '#048A81',     # Teal (accent)\n",
    "    'tertiary': '#54C6EB',      # Light blue\n",
    "    'highlight': '#D95D39',     # Coral red (emphasis)\n",
    "    'neutral': '#8B8B8B',       # Gray\n",
    "    'background': '#F5F5F5',    # Light gray background\n",
    "    'grid': '#E0E0E0',          # Grid color\n",
    "    'text': '#2C3E50'           # Dark text\n",
    "}\n",
    "\n",
    "# Set professional matplotlib style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams.update({\n",
    "    'figure.figsize': (12, 7),\n",
    "    'font.size': 11,\n",
    "    'axes.labelsize': 12,\n",
    "    'axes.titlesize': 14,\n",
    "    'xtick.labelsize': 10,\n",
    "    'ytick.labelsize': 10,\n",
    "    'legend.fontsize': 10,\n",
    "    'figure.titlesize': 16,\n",
    "    'axes.facecolor': COLORS['background'],\n",
    "    'figure.facecolor': 'white',\n",
    "    'axes.edgecolor': COLORS['neutral'],\n",
    "    'grid.color': COLORS['grid'],\n",
    "    'grid.alpha': 0.3,\n",
    "    'text.color': COLORS['text'],\n",
    "    'axes.labelcolor': COLORS['text'],\n",
    "    'xtick.color': COLORS['text'],\n",
    "    'ytick.color': COLORS['text']\n",
    "})\n",
    "\n",
    "# ================================================================\n",
    "# CONFIGURATION\n",
    "# ================================================================\n",
    "\n",
    "class Config:\n",
    "    BASE_PATH = Path(\"/kaggle/input/csiro-biomass/\")\n",
    "    TRAIN_PATH = BASE_PATH / \"train\"\n",
    "    TEST_PATH = BASE_PATH / \"test\"\n",
    "    OUTPUT_PATH = Path(\"/kaggle/working/\")\n",
    "    \n",
    "    TARGET_COLS = ['Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g', 'GDM_g', 'Dry_Total_g']\n",
    "    TARGET_NAMES = {\n",
    "        'Dry_Green_g': 'Green Biomass',\n",
    "        'Dry_Dead_g': 'Dead Biomass',\n",
    "        'Dry_Clover_g': 'Clover Biomass',\n",
    "        'GDM_g': 'Green Dry Matter',\n",
    "        'Dry_Total_g': 'Total Biomass'\n",
    "    }\n",
    "    \n",
    "    TARGET_WEIGHTS = {\n",
    "        'Dry_Green_g': 0.1,\n",
    "        'Dry_Dead_g': 0.1,\n",
    "        'Dry_Clover_g': 0.1,\n",
    "        'GDM_g': 0.2,\n",
    "        'Dry_Total_g': 0.5,\n",
    "    }\n",
    "\n",
    "cfg = Config()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CSIRO BIOMASS COMPETITION - PROFESSIONAL EDA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ================================================================\n",
    "# LOAD DATA\n",
    "# ================================================================\n",
    "\n",
    "print(\"\\nLoading data...\")\n",
    "train_df = pd.read_csv(cfg.BASE_PATH / \"train.csv\")\n",
    "test_df = pd.read_csv(cfg.BASE_PATH / \"test.csv\")\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")\n",
    "\n",
    "# Pivot to get one row per image\n",
    "train_pivot = train_df.pivot_table(\n",
    "    values='target',\n",
    "    index='image_path',\n",
    "    columns='target_name',\n",
    "    aggfunc='mean'\n",
    ").reset_index()\n",
    "\n",
    "print(f\"Pivoted train shape: {train_pivot.shape}\")\n",
    "\n",
    "# ================================================================\n",
    "# 1. COMPREHENSIVE TARGET DISTRIBUTION WITH BELL CURVES\n",
    "# ================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Creating Visualization 1: Target Distributions with Normal Curves\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fig = plt.figure(figsize=(20, 12))\n",
    "gs = fig.add_gridspec(3, 2, hspace=0.3, wspace=0.25)\n",
    "\n",
    "for idx, col in enumerate(cfg.TARGET_COLS):\n",
    "    row = idx // 2\n",
    "    col_idx = idx % 2\n",
    "    ax = fig.add_subplot(gs[row, col_idx])\n",
    "    \n",
    "    values = train_pivot[col].dropna().values\n",
    "    \n",
    "    # Histogram\n",
    "    n, bins, patches = ax.hist(values, bins=40, alpha=0.6, color=COLORS['primary'], \n",
    "                                edgecolor='white', linewidth=1.5, density=True,\n",
    "                                label='Observed Distribution')\n",
    "    \n",
    "    # Fit normal distribution\n",
    "    mu, sigma = values.mean(), values.std()\n",
    "    x = np.linspace(values.min(), values.max(), 100)\n",
    "    normal_curve = norm.pdf(x, mu, sigma)\n",
    "    \n",
    "    # Plot normal curve\n",
    "    ax.plot(x, normal_curve, color=COLORS['highlight'], linewidth=3, \n",
    "            label=f'Normal Fit (μ={mu:.1f}, σ={sigma:.1f})', linestyle='--')\n",
    "    \n",
    "    # Statistics lines\n",
    "    ax.axvline(mu, color=COLORS['secondary'], linestyle='-', linewidth=2.5, \n",
    "               label=f'Mean: {mu:.2f}g', alpha=0.8)\n",
    "    ax.axvline(np.median(values), color=COLORS['tertiary'], linestyle='-', linewidth=2.5,\n",
    "               label=f'Median: {np.median(values):.2f}g', alpha=0.8)\n",
    "    \n",
    "    # Annotations\n",
    "    ax.text(0.98, 0.97, f'n = {len(values):,}', transform=ax.transAxes,\n",
    "            verticalalignment='top', horizontalalignment='right',\n",
    "            bbox=dict(boxstyle='round', facecolor='white', alpha=0.9, edgecolor=COLORS['neutral']),\n",
    "            fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # Statistical properties\n",
    "    skewness = skew(values)\n",
    "    kurt = kurtosis(values)\n",
    "    stats_text = f'Skewness: {skewness:.2f}\\nKurtosis: {kurt:.2f}\\nMin: {values.min():.2f}g\\nMax: {values.max():.2f}g'\n",
    "    ax.text(0.02, 0.97, stats_text, transform=ax.transAxes,\n",
    "            verticalalignment='top', horizontalalignment='left',\n",
    "            bbox=dict(boxstyle='round', facecolor='white', alpha=0.9, edgecolor=COLORS['neutral']),\n",
    "            fontsize=9)\n",
    "    \n",
    "    # Labels and title\n",
    "    ax.set_xlabel(f'{cfg.TARGET_NAMES[col]} (grams)', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Probability Density', fontsize=12, fontweight='bold')\n",
    "    ax.set_title(f'{cfg.TARGET_NAMES[col]} Distribution', \n",
    "                 fontsize=14, fontweight='bold', pad=15, color=COLORS['text'])\n",
    "    ax.legend(loc='upper right', framealpha=0.95, edgecolor=COLORS['neutral'])\n",
    "    ax.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "# Add overall title\n",
    "fig.suptitle('Target Variable Distributions with Statistical Analysis', \n",
    "             fontsize=18, fontweight='bold', y=0.995, color=COLORS['text'])\n",
    "\n",
    "plt.savefig(cfg.OUTPUT_PATH / 'professional_1_distributions.png', dpi=300, bbox_inches='tight', \n",
    "            facecolor='white', edgecolor='none')\n",
    "print(\"✓ Saved: professional_1_distributions.png\")\n",
    "plt.close()\n",
    "\n",
    "# ================================================================\n",
    "# 2. COMPREHENSIVE CORRELATION MATRIX  \n",
    "# ================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Creating Visualization 2: Complete Correlation Matrix\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate correlations\n",
    "correlation_matrix = train_pivot[cfg.TARGET_COLS].corr()\n",
    "\n",
    "# Create simple figure\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "# Professional RED-WHITE-GREEN diverging colormap\n",
    "# Red (negative) → White/Yellow (zero) → Green (positive)\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "colors_cmap = [\n",
    "    '#D73027',  # Dark red (strong negative)\n",
    "    '#FC8D59',  # Medium red\n",
    "    '#FEE090',  # Light red/orange\n",
    "    '#FFFFBF',  # White/Yellow (zero)\n",
    "    '#E0F3DB',  # Very light green\n",
    "    '#A8DDB5',  # Light green\n",
    "    '#66C2A4',  # Medium light green\n",
    "    '#2CA25F',  # Medium green\n",
    "    '#006D2C'   # Dark green (strong positive)\n",
    "]\n",
    "n_bins = 100\n",
    "cmap_diverging = LinearSegmentedColormap.from_list('red_white_green', colors_cmap, N=n_bins)\n",
    "\n",
    "# Create heatmap with full range -1 to 1\n",
    "im = ax.imshow(correlation_matrix, cmap=cmap_diverging, aspect='auto', vmin=-1, vmax=1)\n",
    "\n",
    "# Set ticks\n",
    "ax.set_xticks(np.arange(len(cfg.TARGET_COLS)))\n",
    "ax.set_yticks(np.arange(len(cfg.TARGET_COLS)))\n",
    "ax.set_xticklabels([cfg.TARGET_NAMES[col] for col in cfg.TARGET_COLS], \n",
    "                    rotation=45, ha='right', fontsize=11, fontweight='bold')\n",
    "ax.set_yticklabels([cfg.TARGET_NAMES[col] for col in cfg.TARGET_COLS], \n",
    "                    fontsize=11, fontweight='bold')\n",
    "\n",
    "# Add correlation values and significance\n",
    "for i in range(len(cfg.TARGET_COLS)):\n",
    "    for j in range(len(cfg.TARGET_COLS)):\n",
    "        corr_val = correlation_matrix.iloc[i, j]\n",
    "        \n",
    "        # Calculate p-value\n",
    "        if i != j:\n",
    "            _, p_val = pearsonr(train_pivot[cfg.TARGET_COLS[i]], \n",
    "                               train_pivot[cfg.TARGET_COLS[j]])\n",
    "            sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else ''\n",
    "        else:\n",
    "            sig = ''\n",
    "        \n",
    "        # Choose text color based on correlation value\n",
    "        # Dark text for light colors (near zero), white text for dark colors (strong correlations)\n",
    "        if abs(corr_val) > 0.6:\n",
    "            text_color = 'white'\n",
    "        else:\n",
    "            text_color = 'black'\n",
    "        \n",
    "        # Main correlation value\n",
    "        ax.text(j, i, f'{corr_val:.3f}', ha='center', va='center',\n",
    "                color=text_color, fontsize=12, fontweight='bold')\n",
    "        \n",
    "        # Significance stars\n",
    "        if sig:\n",
    "            ax.text(j, i + 0.35, sig, ha='center', va='center',\n",
    "                    color=text_color, fontsize=10, fontweight='bold')\n",
    "\n",
    "# Colorbar with professional labels\n",
    "cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "cbar.set_label('Pearson Correlation Coefficient (r)', rotation=270, labelpad=25, \n",
    "               fontsize=12, fontweight='bold')\n",
    "cbar.ax.tick_params(labelsize=10)\n",
    "\n",
    "# Add interpretive labels on colorbar\n",
    "cbar.ax.text(3.5, -0.9, 'Strong Negative', transform=cbar.ax.transData,\n",
    "            fontsize=9, fontweight='bold', color='#D73027', rotation=0)\n",
    "cbar.ax.text(3.5, 0.0, 'No Correlation', transform=cbar.ax.transData,\n",
    "            fontsize=9, fontweight='bold', color='#666666', rotation=0)\n",
    "cbar.ax.text(3.5, 0.9, 'Strong Positive', transform=cbar.ax.transData,\n",
    "            fontsize=9, fontweight='bold', color='#006D2C', rotation=0)\n",
    "\n",
    "# Title with significance legend\n",
    "title_text = 'Complete Correlation Matrix: All Target Variables\\n'\n",
    "title_text += 'Red = Negative | Yellow = Zero | Green = Positive  |  '\n",
    "title_text += '*** p<0.001, ** p<0.01, * p<0.05'\n",
    "ax.set_title(title_text, fontsize=13, fontweight='bold', pad=20, color=COLORS['text'])\n",
    "\n",
    "# Grid\n",
    "ax.set_xticks(np.arange(len(cfg.TARGET_COLS)) - 0.5, minor=True)\n",
    "ax.set_yticks(np.arange(len(cfg.TARGET_COLS)) - 0.5, minor=True)\n",
    "ax.grid(which='minor', color='white', linestyle='-', linewidth=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(cfg.OUTPUT_PATH / 'professional_2_correlation_matrix.png', dpi=300, bbox_inches='tight',\n",
    "            facecolor='white', edgecolor='none')\n",
    "print(\"✓ Saved: professional_2_correlation_matrix.png\")\n",
    "plt.close()\n",
    "\n",
    "# ================================================================\n",
    "# 3. DETAILED BOX PLOTS WITH STATISTICS\n",
    "# ================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Creating Visualization 3: Detailed Box Plots\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "# Prepare data\n",
    "data_for_box = [train_pivot[col].dropna().values for col in cfg.TARGET_COLS]\n",
    "labels = [cfg.TARGET_NAMES[col] for col in cfg.TARGET_COLS]\n",
    "\n",
    "# Create box plot\n",
    "bp = ax.boxplot(data_for_box, labels=labels, patch_artist=True,\n",
    "                widths=0.6, showmeans=True,\n",
    "                meanprops=dict(marker='D', markerfacecolor=COLORS['highlight'], \n",
    "                              markeredgecolor='white', markersize=10),\n",
    "                medianprops=dict(color=COLORS['secondary'], linewidth=2.5),\n",
    "                boxprops=dict(facecolor=COLORS['primary'], alpha=0.7, \n",
    "                             edgecolor=COLORS['text'], linewidth=1.5),\n",
    "                whiskerprops=dict(color=COLORS['text'], linewidth=1.5),\n",
    "                capprops=dict(color=COLORS['text'], linewidth=1.5),\n",
    "                flierprops=dict(marker='o', markerfacecolor=COLORS['highlight'], \n",
    "                               markersize=5, alpha=0.5, markeredgecolor='none'))\n",
    "\n",
    "# Add detailed statistics\n",
    "for idx, (col, data) in enumerate(zip(cfg.TARGET_COLS, data_for_box)):\n",
    "    # Calculate statistics\n",
    "    q1 = np.percentile(data, 25)\n",
    "    median = np.median(data)\n",
    "    q3 = np.percentile(data, 75)\n",
    "    mean = np.mean(data)\n",
    "    iqr = q3 - q1\n",
    "    lower_whisker = q1 - 1.5 * iqr\n",
    "    upper_whisker = q3 + 1.5 * iqr\n",
    "    outliers = len(data[(data < lower_whisker) | (data > upper_whisker)])\n",
    "    \n",
    "    # Add text annotation\n",
    "    stats_text = f'Mean: {mean:.1f}g\\nMedian: {median:.1f}g\\nIQR: {iqr:.1f}g\\nOutliers: {outliers}'\n",
    "    ax.text(idx + 1, ax.get_ylim()[1] * 0.95, stats_text,\n",
    "            ha='center', va='top', fontsize=9,\n",
    "            bbox=dict(boxstyle='round', facecolor='white', alpha=0.9, \n",
    "                     edgecolor=COLORS['neutral']))\n",
    "\n",
    "# Labels and formatting\n",
    "ax.set_ylabel('Biomass (grams)', fontsize=13, fontweight='bold')\n",
    "ax.set_xlabel('Target Variables', fontsize=13, fontweight='bold')\n",
    "ax.set_title('Box Plot Analysis: Target Variables with Statistical Details', \n",
    "             fontsize=15, fontweight='bold', pad=20, color=COLORS['text'])\n",
    "\n",
    "# Legend\n",
    "legend_elements = [\n",
    "    plt.Line2D([0], [0], marker='D', color='w', label='Mean',\n",
    "              markerfacecolor=COLORS['highlight'], markersize=10),\n",
    "    plt.Line2D([0], [0], color=COLORS['secondary'], linewidth=2.5, label='Median'),\n",
    "    plt.Line2D([0], [0], marker='o', color='w', label='Outliers',\n",
    "              markerfacecolor=COLORS['highlight'], markersize=7, alpha=0.5)\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc='upper right', framealpha=0.95, \n",
    "         edgecolor=COLORS['neutral'], fontsize=11)\n",
    "\n",
    "ax.grid(True, alpha=0.3, axis='y', linestyle='--')\n",
    "ax.set_axisbelow(True)\n",
    "plt.xticks(rotation=15, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(cfg.OUTPUT_PATH / 'professional_3_boxplots.png', dpi=300, bbox_inches='tight',\n",
    "            facecolor='white', edgecolor='none')\n",
    "print(\"✓ Saved: professional_3_boxplots.png\")\n",
    "plt.close()\n",
    "\n",
    "# ================================================================\n",
    "# 4. SCATTER PLOT MATRIX WITH REGRESSION LINES\n",
    "# ================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Creating Visualization 4: Scatter Plot Matrix\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Select subset of targets for clarity\n",
    "main_targets = ['Dry_Green_g', 'Dry_Dead_g', 'GDM_g', 'Dry_Total_g']\n",
    "n_targets = len(main_targets)\n",
    "\n",
    "fig, axes = plt.subplots(n_targets, n_targets, figsize=(18, 18))\n",
    "\n",
    "for i, target1 in enumerate(main_targets):\n",
    "    for j, target2 in enumerate(main_targets):\n",
    "        ax = axes[i, j]\n",
    "        \n",
    "        if i == j:\n",
    "            # Diagonal: histogram with KDE\n",
    "            data = train_pivot[target1].dropna().values\n",
    "            ax.hist(data, bins=30, alpha=0.6, color=COLORS['primary'], \n",
    "                   edgecolor='white', density=True)\n",
    "            \n",
    "            # KDE\n",
    "            from scipy.stats import gaussian_kde\n",
    "            kde = gaussian_kde(data)\n",
    "            x_range = np.linspace(data.min(), data.max(), 100)\n",
    "            ax.plot(x_range, kde(x_range), color=COLORS['highlight'], \n",
    "                   linewidth=2.5, label='KDE')\n",
    "            \n",
    "            ax.set_ylabel('Density', fontsize=9)\n",
    "            if i == n_targets - 1:\n",
    "                ax.set_xlabel(cfg.TARGET_NAMES[target1], fontsize=10, fontweight='bold')\n",
    "        else:\n",
    "            # Off-diagonal: scatter plot\n",
    "            x_data = train_pivot[target2].dropna()\n",
    "            y_data = train_pivot[target1].dropna()\n",
    "            \n",
    "            # Align data\n",
    "            common_idx = x_data.index.intersection(y_data.index)\n",
    "            x_vals = x_data.loc[common_idx].values\n",
    "            y_vals = y_data.loc[common_idx].values\n",
    "            \n",
    "            # Scatter\n",
    "            ax.scatter(x_vals, y_vals, alpha=0.4, s=20, color=COLORS['primary'], \n",
    "                      edgecolors='none')\n",
    "            \n",
    "            # Regression line\n",
    "            if len(x_vals) > 1:\n",
    "                z = np.polyfit(x_vals, y_vals, 1)\n",
    "                p = np.poly1d(z)\n",
    "                x_line = np.linspace(x_vals.min(), x_vals.max(), 100)\n",
    "                ax.plot(x_line, p(x_line), color=COLORS['highlight'], \n",
    "                       linewidth=2.5, linestyle='--')\n",
    "                \n",
    "                # Correlation\n",
    "                r, p_val = pearsonr(x_vals, y_vals)\n",
    "                ax.text(0.05, 0.95, f'r={r:.3f}', transform=ax.transAxes,\n",
    "                       verticalalignment='top', fontsize=9, fontweight='bold',\n",
    "                       bbox=dict(boxstyle='round', facecolor='white', alpha=0.9))\n",
    "        \n",
    "        # Labels\n",
    "        if j == 0:\n",
    "            ax.set_ylabel(cfg.TARGET_NAMES[target1], fontsize=10, fontweight='bold')\n",
    "        if i == n_targets - 1:\n",
    "            ax.set_xlabel(cfg.TARGET_NAMES[target2], fontsize=10, fontweight='bold')\n",
    "        \n",
    "        ax.grid(True, alpha=0.2, linestyle='--')\n",
    "        ax.tick_params(labelsize=8)\n",
    "\n",
    "fig.suptitle('Scatter Plot Matrix: Pairwise Relationships Between Targets', \n",
    "             fontsize=16, fontweight='bold', y=0.995)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(cfg.OUTPUT_PATH / 'professional_4_scatter_matrix.png', dpi=300, bbox_inches='tight',\n",
    "            facecolor='white', edgecolor='none')\n",
    "print(\"✓ Saved: professional_4_scatter_matrix.png\")\n",
    "plt.close()\n",
    "\n",
    "# ================================================================\n",
    "# 5. DETAILED SAMPLE IMAGE ANALYSIS\n",
    "# ================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Creating Visualization 5: Detailed Sample Image Analysis\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Select 6 sample images\n",
    "sample_images = train_pivot['image_path'].sample(min(6, len(train_pivot)), random_state=42).tolist()\n",
    "\n",
    "fig = plt.figure(figsize=(20, 16))\n",
    "gs = fig.add_gridspec(6, 4, hspace=0.35, wspace=0.3)  # 6 rows, 4 columns\n",
    "\n",
    "for idx, img_path in enumerate(sample_images):\n",
    "    img_name = os.path.basename(img_path)\n",
    "    full_path = cfg.TRAIN_PATH / img_name\n",
    "    \n",
    "    img = cv2.imread(str(full_path))\n",
    "    if img is None:\n",
    "        continue\n",
    "    \n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img_hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    # Main image (left column)\n",
    "    ax_img = fig.add_subplot(gs[idx, 0])\n",
    "    ax_img.imshow(img_rgb)\n",
    "    ax_img.set_title(f'Sample {idx+1}', fontsize=12, fontweight='bold')\n",
    "    ax_img.axis('off')\n",
    "    \n",
    "    # Add image properties\n",
    "    h, w, _ = img_rgb.shape\n",
    "    green_ratio = img_rgb[:,:,1] / (img_rgb.sum(axis=2) + 1)\n",
    "    brightness = img_rgb.mean()\n",
    "    \n",
    "    props_text = f'Size: {w}×{h}\\nBrightness: {brightness:.1f}\\nGreen Ratio: {green_ratio.mean():.3f}'\n",
    "    ax_img.text(0.02, 0.98, props_text, transform=ax_img.transAxes,\n",
    "               verticalalignment='top', fontsize=9,\n",
    "               bbox=dict(boxstyle='round', facecolor='white', alpha=0.9))\n",
    "    \n",
    "    # RGB histogram\n",
    "    ax_hist = fig.add_subplot(gs[idx, 1])\n",
    "    colors_rgb = ['#E74C3C', '#27AE60', '#3498DB']  # Red, Green, Blue\n",
    "    labels_rgb = ['Red', 'Green', 'Blue']\n",
    "    \n",
    "    for c, color, label in zip(range(3), colors_rgb, labels_rgb):\n",
    "        hist = cv2.calcHist([img_rgb], [c], None, [256], [0, 256])\n",
    "        ax_hist.plot(hist, color=color, linewidth=2, label=label, alpha=0.7)\n",
    "    \n",
    "    ax_hist.set_xlabel('Pixel Value', fontsize=9)\n",
    "    ax_hist.set_ylabel('Frequency', fontsize=9)\n",
    "    ax_hist.set_title('RGB Distribution', fontsize=10, fontweight='bold')\n",
    "    ax_hist.legend(fontsize=8)\n",
    "    ax_hist.grid(True, alpha=0.3)\n",
    "    ax_hist.set_xlim([0, 256])\n",
    "    \n",
    "    # Green channel analysis\n",
    "    ax_green = fig.add_subplot(gs[idx, 2])\n",
    "    green_channel = img_rgb[:,:,1]\n",
    "    ax_green.imshow(green_channel, cmap='Greens')\n",
    "    ax_green.set_title('Green Channel', fontsize=10, fontweight='bold')\n",
    "    ax_green.axis('off')\n",
    "    \n",
    "    # Vegetation index\n",
    "    ax_veg = fig.add_subplot(gs[idx, 3])\n",
    "    ax_veg.imshow(green_ratio, cmap='RdYlGn', vmin=0, vmax=0.5)\n",
    "    ax_veg.set_title('Vegetation Index', fontsize=10, fontweight='bold')\n",
    "    ax_veg.axis('off')\n",
    "\n",
    "fig.suptitle('Detailed Sample Image Analysis: RGB, Green Channel, and Vegetation Index', \n",
    "             fontsize=16, fontweight='bold', y=0.995)\n",
    "\n",
    "plt.savefig(cfg.OUTPUT_PATH / 'professional_5_sample_images.png', dpi=300, bbox_inches='tight',\n",
    "            facecolor='white', edgecolor='none')\n",
    "print(\"✓ Saved: professional_5_sample_images.png\")\n",
    "plt.close()\n",
    "\n",
    "# ================================================================\n",
    "# 6. HIERARCHICAL RELATIONSHIP VALIDATION\n",
    "# ================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Creating Visualization 6: Hierarchical Relationships\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
    "\n",
    "# Green + Clover = GDM\n",
    "ax = axes[0]\n",
    "calculated = train_pivot['Dry_Green_g'] + train_pivot['Dry_Clover_g']\n",
    "actual = train_pivot['GDM_g']\n",
    "\n",
    "ax.scatter(calculated, actual, alpha=0.5, s=40, color=COLORS['primary'], \n",
    "          edgecolors='white', linewidth=0.5)\n",
    "\n",
    "# Perfect match line\n",
    "max_val = max(calculated.max(), actual.max())\n",
    "ax.plot([0, max_val], [0, max_val], color=COLORS['highlight'], \n",
    "       linewidth=3, linestyle='--', label='Perfect Match (y=x)')\n",
    "\n",
    "# Regression line\n",
    "z = np.polyfit(calculated, actual, 1)\n",
    "p = np.poly1d(z)\n",
    "x_line = np.linspace(calculated.min(), calculated.max(), 100)\n",
    "ax.plot(x_line, p(x_line), color=COLORS['secondary'], \n",
    "       linewidth=2.5, label=f'Regression: y={z[0]:.3f}x+{z[1]:.2f}')\n",
    "\n",
    "# Statistics\n",
    "r, p_val = pearsonr(calculated, actual)\n",
    "rmse = np.sqrt(((calculated - actual) ** 2).mean())\n",
    "\n",
    "stats_text = f'Pearson r: {r:.4f}\\np-value: {p_val:.2e}\\nRMSE: {rmse:.2f}g\\nn: {len(calculated):,}'\n",
    "ax.text(0.05, 0.95, stats_text, transform=ax.transAxes,\n",
    "       verticalalignment='top', fontsize=11, fontweight='bold',\n",
    "       bbox=dict(boxstyle='round', facecolor='white', alpha=0.95, \n",
    "                edgecolor=COLORS['neutral'], linewidth=2))\n",
    "\n",
    "ax.set_xlabel('Green + Clover (calculated, grams)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('GDM (actual, grams)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Hierarchical Validation: Green + Clover = GDM', \n",
    "            fontsize=13, fontweight='bold', pad=15)\n",
    "ax.legend(loc='lower right', fontsize=11, framealpha=0.95)\n",
    "ax.grid(True, alpha=0.3, linestyle='--')\n",
    "\n",
    "# GDM + Dead = Total\n",
    "ax = axes[1]\n",
    "calculated = train_pivot['GDM_g'] + train_pivot['Dry_Dead_g']\n",
    "actual = train_pivot['Dry_Total_g']\n",
    "\n",
    "ax.scatter(calculated, actual, alpha=0.5, s=40, color=COLORS['primary'], \n",
    "          edgecolors='white', linewidth=0.5)\n",
    "\n",
    "max_val = max(calculated.max(), actual.max())\n",
    "ax.plot([0, max_val], [0, max_val], color=COLORS['highlight'], \n",
    "       linewidth=3, linestyle='--', label='Perfect Match (y=x)')\n",
    "\n",
    "z = np.polyfit(calculated, actual, 1)\n",
    "p = np.poly1d(z)\n",
    "x_line = np.linspace(calculated.min(), calculated.max(), 100)\n",
    "ax.plot(x_line, p(x_line), color=COLORS['secondary'], \n",
    "       linewidth=2.5, label=f'Regression: y={z[0]:.3f}x+{z[1]:.2f}')\n",
    "\n",
    "r, p_val = pearsonr(calculated, actual)\n",
    "rmse = np.sqrt(((calculated - actual) ** 2).mean())\n",
    "\n",
    "stats_text = f'Pearson r: {r:.4f}\\np-value: {p_val:.2e}\\nRMSE: {rmse:.2f}g\\nn: {len(calculated):,}'\n",
    "ax.text(0.05, 0.95, stats_text, transform=ax.transAxes,\n",
    "       verticalalignment='top', fontsize=11, fontweight='bold',\n",
    "       bbox=dict(boxstyle='round', facecolor='white', alpha=0.95, \n",
    "                edgecolor=COLORS['neutral'], linewidth=2))\n",
    "\n",
    "ax.set_xlabel('GDM + Dead (calculated, grams)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Total Biomass (actual, grams)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Hierarchical Validation: GDM + Dead = Total', \n",
    "            fontsize=13, fontweight='bold', pad=15)\n",
    "ax.legend(loc='lower right', fontsize=11, framealpha=0.95)\n",
    "ax.grid(True, alpha=0.3, linestyle='--')\n",
    "\n",
    "fig.suptitle('Hierarchical Relationship Validation with Statistical Analysis', \n",
    "             fontsize=15, fontweight='bold', y=1.00)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(cfg.OUTPUT_PATH / 'professional_6_hierarchical.png', dpi=300, bbox_inches='tight',\n",
    "            facecolor='white', edgecolor='none')\n",
    "print(\"✓ Saved: professional_6_hierarchical.png\")\n",
    "plt.close()\n",
    "\n",
    "# ================================================================\n",
    "# SUMMARY\n",
    "# ================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EDA COMPLETE - ALL VISUALIZATIONS SAVED\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nGenerated Professional Visualizations:\")\n",
    "print(\"  1. professional_1_distributions.png - Target distributions with bell curves\")\n",
    "print(\"  2. professional_2_correlation_matrix.png - Complete correlation analysis\")\n",
    "print(\"  3. professional_3_boxplots.png - Detailed box plots with statistics\")\n",
    "print(\"  4. professional_4_scatter_matrix.png - Scatter plot matrix with regression\")\n",
    "print(\"  5. professional_5_sample_images.png - Detailed image analysis\")\n",
    "print(\"  6. professional_6_hierarchical.png - Hierarchical relationship validation\")\n",
    "print(\"\\nAll visualizations use consistent professional color scheme\")\n",
    "print(\"Statistical details and annotations included on all plots\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99dc06be",
   "metadata": {
    "_cell_guid": "a181f50e-d2db-476c-812c-be6a36866fdd",
    "_uuid": "ea88a7f0-413c-4af6-97d4-9c2453b145ab",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-01-11T14:13:38.756779Z",
     "iopub.status.busy": "2026-01-11T14:13:38.756543Z",
     "iopub.status.idle": "2026-01-11T14:16:20.832266Z",
     "shell.execute_reply": "2026-01-11T14:16:20.831239Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 162.081723,
     "end_time": "2026-01-11T14:16:20.833720",
     "exception": false,
     "start_time": "2026-01-11T14:13:38.751997",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-11 14:14:34.500886: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1768140874.685567      24 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1768140874.740662      24 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1768140875.202408      24 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768140875.202436      24 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768140875.202439      24 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768140875.202441      24 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CONFIGURATION SETTINGS\n",
      "================================================================================\n",
      "Data Path: /kaggle/input/csiro-biomass\n",
      "Device: cuda\n",
      "Timeout: 8.5 hours\n",
      "Patch Size: 520px (overlap: 16px)\n",
      "CV Folds: 5\n",
      "Ensemble Weights: LGB=0.6, CAT=0.4\n",
      "Use Semantic Features: True\n",
      "Use CatBoost: True\n",
      "================================================================================\n",
      "\n",
      "[TIME] Pipeline started | Elapsed: 0.00h | Remaining: 8.50h\n",
      "✓ Created fallback submission\n",
      "\n",
      "================================================================================\n",
      "LOADING DATA\n",
      "================================================================================\n",
      "✓ Loading pre-computed training embeddings\n",
      "[TIME] Data loaded | Elapsed: 0.00h | Remaining: 8.50h\n",
      "\n",
      "================================================================================\n",
      "EXTRACTING EMBEDDINGS\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c9c5a19800d4012a97a7ebf39a32972",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing embeddings:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TIME] Test embeddings computed | Elapsed: 0.01h | Remaining: 8.49h\n",
      "✓ Using pre-computed train embeddings\n",
      "\n",
      "================================================================================\n",
      "GENERATING SEMANTIC FEATURES\n",
      "================================================================================\n",
      "✓ Generated 10 features\n",
      "[TIME] Semantic features generated | Elapsed: 0.01h | Remaining: 8.49h\n",
      "\n",
      "================================================================================\n",
      "TRAINING ENSEMBLE\n",
      "================================================================================\n",
      "\n",
      "------------------------------------------------------------\n",
      "LIGHTGBM\n",
      "------------------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "Fold 1/5\n",
      "============================================================\n",
      "  Training Dry_Clover_g... ✓\n",
      "  Training Dry_Dead_g... ✓\n",
      "  Training Dry_Green_g... ✓\n",
      "  Training Dry_Total_g... ✓\n",
      "  Training GDM_g... ✓\n",
      "Fold 1 Score: 0.778470\n",
      "\n",
      "============================================================\n",
      "Fold 2/5\n",
      "============================================================\n",
      "  Training Dry_Clover_g... ✓\n",
      "  Training Dry_Dead_g... ✓\n",
      "  Training Dry_Green_g... ✓\n",
      "  Training Dry_Total_g... ✓\n",
      "  Training GDM_g... ✓\n",
      "Fold 2 Score: 0.773754\n",
      "\n",
      "============================================================\n",
      "Fold 3/5\n",
      "============================================================\n",
      "  Training Dry_Clover_g... ✓\n",
      "  Training Dry_Dead_g... ✓\n",
      "  Training Dry_Green_g... ✓\n",
      "  Training Dry_Total_g... ✓\n",
      "  Training GDM_g... ✓\n",
      "Fold 3 Score: 0.676823\n",
      "\n",
      "============================================================\n",
      "Fold 4/5\n",
      "============================================================\n",
      "  Training Dry_Clover_g... ✓\n",
      "  Training Dry_Dead_g... ✓\n",
      "  Training Dry_Green_g... ✓\n",
      "  Training Dry_Total_g... ✓\n",
      "  Training GDM_g... ✓\n",
      "Fold 4 Score: 0.787959\n",
      "\n",
      "============================================================\n",
      "Fold 5/5\n",
      "============================================================\n",
      "  Training Dry_Clover_g... ✓\n",
      "  Training Dry_Dead_g... ✓\n",
      "  Training Dry_Green_g... ✓\n",
      "  Training Dry_Total_g... ✓\n",
      "  Training GDM_g... ✓\n",
      "Fold 5 Score: 0.784284\n",
      "\n",
      "============================================================\n",
      "Full CV Score: 0.759586\n",
      "============================================================\n",
      "Raw CV Score: 0.759586\n",
      "Processed CV Score: 0.761950\n",
      "Improvement: 0.002364\n",
      "[TIME] LightGBM complete | Elapsed: 0.01h | Remaining: 8.49h\n",
      "\n",
      "------------------------------------------------------------\n",
      "CATBOOST\n",
      "------------------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "Fold 1/5\n",
      "============================================================\n",
      "  Training Dry_Clover_g... ✓\n",
      "  Training Dry_Dead_g... ✓\n",
      "  Training Dry_Green_g... ✓\n",
      "  Training Dry_Total_g... ✓\n",
      "  Training GDM_g... ✓\n",
      "Fold 1 Score: 0.775115\n",
      "\n",
      "============================================================\n",
      "Fold 2/5\n",
      "============================================================\n",
      "  Training Dry_Clover_g... ✓\n",
      "  Training Dry_Dead_g... ✓\n",
      "  Training Dry_Green_g... ✓\n",
      "  Training Dry_Total_g... ✓\n",
      "  Training GDM_g... ✓\n",
      "Fold 2 Score: 0.759951\n",
      "\n",
      "============================================================\n",
      "Fold 3/5\n",
      "============================================================\n",
      "  Training Dry_Clover_g... ✓\n",
      "  Training Dry_Dead_g... ✓\n",
      "  Training Dry_Green_g... ✓\n",
      "  Training Dry_Total_g... ✓\n",
      "  Training GDM_g... ✓\n",
      "Fold 3 Score: 0.668052\n",
      "\n",
      "============================================================\n",
      "Fold 4/5\n",
      "============================================================\n",
      "  Training Dry_Clover_g... ✓\n",
      "  Training Dry_Dead_g... ✓\n",
      "  Training Dry_Green_g... ✓\n",
      "  Training Dry_Total_g... ✓\n",
      "  Training GDM_g... ✓\n",
      "Fold 4 Score: 0.772934\n",
      "\n",
      "============================================================\n",
      "Fold 5/5\n",
      "============================================================\n",
      "  Training Dry_Clover_g... ✓\n",
      "  Training Dry_Dead_g... ✓\n",
      "  Training Dry_Green_g... ✓\n",
      "  Training Dry_Total_g... ✓\n",
      "  Training GDM_g... ✓\n",
      "Fold 5 Score: 0.769867\n",
      "\n",
      "============================================================\n",
      "Full CV Score: 0.748801\n",
      "============================================================\n",
      "Raw CV Score: 0.748801\n",
      "Processed CV Score: 0.749756\n",
      "Improvement: 0.000955\n",
      "[TIME] CatBoost complete | Elapsed: 0.03h | Remaining: 8.47h\n",
      "\n",
      "✓ Ensemble: 0.6*LGB + 0.4*CAT\n",
      "\n",
      "================================================================================\n",
      "HIERARCHICAL RECONCILIATION\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "CREATING SUBMISSION\n",
      "================================================================================\n",
      "✓ Submission: submission.csv\n",
      "\n",
      "Prediction Statistics:\n",
      "       Dry_Clover_g  Dry_Dead_g  Dry_Green_g  Dry_Total_g      GDM_g\n",
      "count      1.000000    1.000000     1.000000     1.000000   1.000000\n",
      "mean       3.436027   27.832131    25.134143    56.402302  28.570171\n",
      "std             NaN         NaN          NaN          NaN        NaN\n",
      "min        3.436027   27.832131    25.134143    56.402302  28.570171\n",
      "25%        3.436027   27.832131    25.134143    56.402302  28.570171\n",
      "50%        3.436027   27.832131    25.134143    56.402302  28.570171\n",
      "75%        3.436027   27.832131    25.134143    56.402302  28.570171\n",
      "max        3.436027   27.832131    25.134143    56.402302  28.570171\n",
      "[TIME] Complete! | Elapsed: 0.03h | Remaining: 8.47h\n",
      "\n",
      "================================================================================\n",
      "DONE\n",
      "================================================================================\n",
      "[TIME] Final | Elapsed: 0.03h | Remaining: 8.47h\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Jupyter/Colab-Friendly Version of Improved Biomass Prediction Pipeline\n",
    "\n",
    "This version removes argparse to avoid conflicts with Jupyter kernel arguments.\n",
    "Configure settings directly in the code below.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import math\n",
    "import json\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "from copy import deepcopy\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, List, Tuple, Dict\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import timm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import albumentations as A\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.model_selection import KFold, GroupKFold\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, BayesianRidge\n",
    "from sklearn.svm import SVR, LinearSVR\n",
    "from sklearn.ensemble import GradientBoostingRegressor, HistGradientBoostingRegressor, ExtraTreesRegressor\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "from transformers import AutoProcessor, AutoImageProcessor, AutoModel, AutoTokenizer\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION - MODIFY THESE SETTINGS\n",
    "# ============================================================================\n",
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Configuration settings - modify these for your environment\"\"\"\n",
    "    \n",
    "    # ===== PATHS (MODIFY FOR YOUR ENVIRONMENT) =====\n",
    "    DATA_PATH: Path = Path(\"/kaggle/input/csiro-biomass/\")  # Change for Colab: \"/content/drive/MyDrive/csiro-biomass\"\n",
    "    TRAIN_DATA_PATH: Path = DATA_PATH/'train'\n",
    "    TEST_DATA_PATH: Path = DATA_PATH/'test'\n",
    "    \n",
    "    # ===== RUNTIME SETTINGS =====\n",
    "    timeout_hours: float = 8.5  # Maximum execution time\n",
    "    seed: int = 42\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # ===== PATCH PROCESSING (from Section 5.1.1) =====\n",
    "    patch_size: int = 520  # Optimal patch size from paper\n",
    "    patch_overlap: int = 16  # 16px overlap prevents boundary artifacts\n",
    "    \n",
    "    # ===== CROSS-VALIDATION =====\n",
    "    n_folds: int = 5  # Number of CV folds (use 3 for faster iteration)\n",
    "    \n",
    "    # ===== ENSEMBLE WEIGHTS (from Section 5.1.3) =====\n",
    "    lgb_weight: float = 0.6  # LightGBM weight\n",
    "    cat_weight: float = 0.4  # CatBoost weight\n",
    "    \n",
    "    # ===== MODEL PATHS =====\n",
    "    siglip_path: str = \"/kaggle/input/google-siglip-so400m-patch14-384/transformers/default/1\"\n",
    "    # For Colab, download first: !git clone https://huggingface.co/google/siglip-so400m-patch14-384\n",
    "    \n",
    "    # ===== SPEED/QUALITY TRADE-OFFS =====\n",
    "    use_semantic_features: bool = True  # False for faster training\n",
    "    use_catboost: bool = True  # False to skip CatBoost (saves ~1 hour)\n",
    "    \n",
    "    # ===== OUTPUT =====\n",
    "    submission_file: str = \"submission.csv\"\n",
    "\n",
    "# Initialize config\n",
    "cfg = Config()\n",
    "\n",
    "# Print configuration\n",
    "print(\"=\"*80)\n",
    "print(\"CONFIGURATION SETTINGS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Data Path: {cfg.DATA_PATH}\")\n",
    "print(f\"Device: {cfg.device}\")\n",
    "print(f\"Timeout: {cfg.timeout_hours} hours\")\n",
    "print(f\"Patch Size: {cfg.patch_size}px (overlap: {cfg.patch_overlap}px)\")\n",
    "print(f\"CV Folds: {cfg.n_folds}\")\n",
    "print(f\"Ensemble Weights: LGB={cfg.lgb_weight}, CAT={cfg.cat_weight}\")\n",
    "print(f\"Use Semantic Features: {cfg.use_semantic_features}\")\n",
    "print(f\"Use CatBoost: {cfg.use_catboost}\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# UTILITY FUNCTIONS\n",
    "# ============================================================================\n",
    "class TimeoutManager:\n",
    "    \"\"\"Manages execution time to prevent timeout\"\"\"\n",
    "    def __init__(self, max_time_seconds):\n",
    "        self.start_time = time.time()\n",
    "        self.max_time = max_time_seconds\n",
    "        \n",
    "    def time_elapsed(self):\n",
    "        return time.time() - self.start_time\n",
    "        \n",
    "    def time_remaining(self):\n",
    "        return self.max_time - self.time_elapsed()\n",
    "    \n",
    "    def should_continue(self, buffer_seconds=300):\n",
    "        \"\"\"Check if we have enough time to continue (with 5 min buffer)\"\"\"\n",
    "        return self.time_remaining() > buffer_seconds\n",
    "    \n",
    "    def log_time(self, message=\"\"):\n",
    "        elapsed = self.time_elapsed()\n",
    "        remaining = self.time_remaining()\n",
    "        print(f\"[TIME] {message} | Elapsed: {elapsed/3600:.2f}h | Remaining: {remaining/3600:.2f}h\")\n",
    "\n",
    "def seeding(SEED):\n",
    "    np.random.seed(SEED)\n",
    "    random.seed(SEED)\n",
    "    os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available(): \n",
    "        torch.cuda.manual_seed(SEED)\n",
    "        torch.cuda.manual_seed_all(SEED)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def flush():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "seeding(cfg.seed)\n",
    "\n",
    "TARGET_NAMES = ['Dry_Clover_g', 'Dry_Dead_g', 'Dry_Green_g', 'Dry_Total_g', 'GDM_g']\n",
    "\n",
    "# Component weights reflecting agricultural importance (from Section 5.2.1)\n",
    "weights = {\n",
    "    'Dry_Green_g': 0.1,\n",
    "    'Dry_Dead_g': 0.1,\n",
    "    'Dry_Clover_g': 0.1,\n",
    "    'GDM_g': 0.2,\n",
    "    'Dry_Total_g': 0.5,  # Total biomass most important for grazing management\n",
    "}\n",
    "\n",
    "TARGET_MAX = {\n",
    "    \"Dry_Clover_g\": 71.7865,\n",
    "    \"Dry_Dead_g\": 83.8407,\n",
    "    \"Dry_Green_g\": 157.9836,\n",
    "    \"Dry_Total_g\": 185.70,\n",
    "    \"GDM_g\": 157.9836,\n",
    "}\n",
    "\n",
    "def competition_metric(y_true, y_pred) -> float:\n",
    "    \"\"\"\n",
    "    Weighted R² metric from Section 5.2.1\n",
    "    Penalizes errors on total biomass more heavily to align with grazing management needs\n",
    "    \"\"\"\n",
    "    y_weighted = 0\n",
    "    for l, label in enumerate(TARGET_NAMES):\n",
    "        y_weighted = y_weighted + y_true[:, l].mean() * weights[label]\n",
    "    ss_res = 0\n",
    "    ss_tot = 0\n",
    "    for l, label in enumerate(TARGET_NAMES):\n",
    "        ss_res = ss_res + ((y_true[:, l] - y_pred[:, l])**2).mean() * weights[label]\n",
    "        ss_tot = ss_tot + ((y_true[:, l] - y_weighted)**2).mean() * weights[label]\n",
    "    return 1 - ss_res / ss_tot\n",
    "\n",
    "def pivot_table(df: pd.DataFrame)->pd.DataFrame:\n",
    "    if 'target' in df.columns.tolist():\n",
    "        df_pt = pd.pivot_table(\n",
    "            df, \n",
    "            values='target', \n",
    "            index=['image_path', 'Sampling_Date', 'State', 'Species', 'Pre_GSHH_NDVI', 'Height_Ave_cm'], \n",
    "            columns='target_name', \n",
    "            aggfunc='mean'\n",
    "        ).reset_index()\n",
    "    else:\n",
    "        df['target'] = 0\n",
    "        df_pt = pd.pivot_table(\n",
    "            df, \n",
    "            values='target', \n",
    "            index='image_path', \n",
    "            columns='target_name', \n",
    "            aggfunc='mean'\n",
    "        ).reset_index()\n",
    "    return df_pt\n",
    "\n",
    "def melt_table(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    melted = df.melt(\n",
    "        id_vars='image_path',\n",
    "        value_vars=TARGET_NAMES,\n",
    "        var_name='target_name',\n",
    "        value_name='target'\n",
    "    )\n",
    "    melted['sample_id'] = (\n",
    "        melted['image_path']\n",
    "        .str.replace(r'^.*/', '', regex=True)\n",
    "        .str.replace('.jpg', '', regex=False)\n",
    "        + '__' + melted['target_name']\n",
    "    )\n",
    "    return melted[['sample_id', 'image_path', 'target_name', 'target']]\n",
    "\n",
    "def hierarchical_reconciliation(predictions: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Enforce biological constraints via constrained least-squares (Section 5.1.4)\n",
    "    \n",
    "    Constraints:\n",
    "    - Green + Clover = GDM\n",
    "    - Dead + GDM = Total\n",
    "    \n",
    "    Solves: minimize ||y_reconciled - y_raw||^2 subject to C @ y = 0\n",
    "    \n",
    "    This improves consistency by ~0.007 R² according to the paper.\n",
    "    \"\"\"\n",
    "    # Target order: [Dry_Clover_g, Dry_Dead_g, Dry_Green_g, Dry_Total_g, GDM_g]\n",
    "    # Reorder to: [Green, Clover, Dead, GDM, Total] for easier constraint definition\n",
    "    ordered_cols_idx = [2, 0, 1, 4, 3]  # Green, Clover, Dead, GDM, Total\n",
    "    \n",
    "    Y = predictions[:, ordered_cols_idx].T  # Shape: (5, N)\n",
    "    \n",
    "    # Define constraint matrix\n",
    "    C = np.array([\n",
    "        [1, 1, 0, -1,  0],  # Green + Clover - GDM = 0\n",
    "        [0, 0, 1,  1, -1]   # Dead + GDM - Total = 0\n",
    "    ])\n",
    "    \n",
    "    C_T = C.T\n",
    "    inv_CCt = np.linalg.inv(C @ C_T)\n",
    "    \n",
    "    # Projection matrix that enforces constraints\n",
    "    P = np.eye(5) - C_T @ inv_CCt @ C\n",
    "    \n",
    "    # Apply reconciliation\n",
    "    Y_reconciled = P @ Y\n",
    "    Y_reconciled = Y_reconciled.T  # Back to (N, 5)\n",
    "    \n",
    "    # Ensure non-negativity\n",
    "    Y_reconciled = np.maximum(Y_reconciled, 0)\n",
    "    \n",
    "    # Reorder back to original: [Clover, Dead, Green, Total, GDM]\n",
    "    reverse_idx = [1, 2, 0, 4, 3]\n",
    "    result = Y_reconciled[:, reverse_idx]\n",
    "    \n",
    "    return result\n",
    "\n",
    "def extract_overlapping_patches(image: np.ndarray, patch_size: int = 520, overlap: int = 16) -> Tuple[List[np.ndarray], List[Tuple]]:\n",
    "    \"\"\"\n",
    "    Extract overlapping patches from image (Section 5.1.1)\n",
    "    \n",
    "    Design rationale:\n",
    "    - 520×520 patch size captures grass clump structures (~30-50cm diameter)\n",
    "    - 16px overlap prevents boundary artifacts in embeddings\n",
    "    - Enables batch processing (4 patches per GPU forward pass on 16GB P100)\n",
    "    \"\"\"\n",
    "    h, w, c = image.shape\n",
    "    stride = patch_size - overlap\n",
    "    patches, coords = [], []\n",
    "    \n",
    "    for y in range(0, h, stride):\n",
    "        for x in range(0, w, stride):\n",
    "            y1, x1 = y, x\n",
    "            y2, x2 = min(y + patch_size, h), min(x + patch_size, w)\n",
    "            \n",
    "            patch = image[y1:y2, x1:x2, :]\n",
    "            \n",
    "            # Pad if necessary\n",
    "            if patch.shape[0] < patch_size or patch.shape[1] < patch_size:\n",
    "                pad_h = patch_size - patch.shape[0]\n",
    "                pad_w = patch_size - patch.shape[1]\n",
    "                patch = np.pad(patch, ((0, pad_h), (0, pad_w), (0, 0)), mode='reflect')\n",
    "            \n",
    "            patches.append(patch)\n",
    "            coords.append((y1, x1, y2, x2))\n",
    "    \n",
    "    return patches, coords\n",
    "\n",
    "def get_model(model_path: str, device: str = 'cpu'):\n",
    "    \"\"\"Load pre-trained vision transformer model\"\"\"\n",
    "    model = AutoModel.from_pretrained(model_path, local_files_only=True)\n",
    "    processor = AutoImageProcessor.from_pretrained(model_path)\n",
    "    return model.eval().to(device), processor\n",
    "\n",
    "def compute_embeddings(model_path: str, df: pd.DataFrame, patch_size: int = 520, overlap: int = 16) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract DINO embeddings via overlapping patch processing (Section 5.1.1)\n",
    "    \n",
    "    Implementation details:\n",
    "    - Processes images in overlapping patches to fit GPU memory constraints\n",
    "    - Mean pooling aggregates patch embeddings (outperforms max pooling by 0.018 R²)\n",
    "    - Batch processes 4-8 patches per forward pass for efficiency\n",
    "    \"\"\"\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model, processor = get_model(model_path=model_path, device=device)\n",
    "    \n",
    "    IMAGE_PATHS, EMBEDDINGS = [], []\n",
    "    \n",
    "    for i, row in tqdm(df.iterrows(), total=len(df), desc=f\"Computing embeddings\"):\n",
    "        img_path = row['image_path']\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "        \n",
    "        if img is None:\n",
    "            print(f\"Warning: Could not load {img_path}\")\n",
    "            continue\n",
    "            \n",
    "        # Extract overlapping patches\n",
    "        patches, coords = extract_overlapping_patches(img, patch_size=patch_size, overlap=overlap)\n",
    "        \n",
    "        # Convert patches to PIL Images\n",
    "        images = [Image.fromarray(p).convert(\"RGB\") for p in patches]\n",
    "        \n",
    "        # Process through model\n",
    "        inputs = processor(images=images, return_tensors=\"pt\").to(model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            if 'siglip' in model_path.lower():\n",
    "                features = model.get_image_features(**inputs)\n",
    "            elif 'dino' in model_path.lower():\n",
    "                features = model(**inputs).pooler_output\n",
    "            else:\n",
    "                outputs = model(**inputs)\n",
    "                if hasattr(outputs, 'pooler_output'):\n",
    "                    features = outputs.pooler_output\n",
    "                elif hasattr(outputs, 'last_hidden_state'):\n",
    "                    features = outputs.last_hidden_state[:, 0]\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown model output format\")\n",
    "        \n",
    "        # Mean pooling (biomass is cumulative)\n",
    "        embeds = features.mean(dim=0).detach().cpu().numpy()\n",
    "        \n",
    "        EMBEDDINGS.append(embeds)\n",
    "        IMAGE_PATHS.append(img_path)\n",
    "    \n",
    "    embeddings = np.stack(EMBEDDINGS, axis=0)\n",
    "    n_features = embeddings.shape[1]\n",
    "    emb_columns = [f\"emb{i+1}\" for i in range(n_features)]\n",
    "    emb_df = pd.DataFrame(embeddings, columns=emb_columns)\n",
    "    emb_df['image_path'] = IMAGE_PATHS\n",
    "    \n",
    "    df_final = df.merge(emb_df, on='image_path', how='left')\n",
    "    \n",
    "    flush()\n",
    "    return df_final\n",
    "\n",
    "def generate_semantic_features(image_embeddings: np.ndarray, model_path: str) -> Optional[np.ndarray]:\n",
    "    \"\"\"Generate semantic features via text-image similarity (Section 5.1.2)\"\"\"\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    try:\n",
    "        model = AutoModel.from_pretrained(model_path).to(device)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not load text encoder: {e}\")\n",
    "        return None\n",
    "    \n",
    "    concept_groups = {\n",
    "        \"bare\": [\"bare soil\", \"dirt ground\", \"sparse vegetation\"],\n",
    "        \"sparse\": [\"low density pasture\", \"thin grass\"],\n",
    "        \"medium\": [\"average pasture cover\", \"medium height grass\"],\n",
    "        \"dense\": [\"dense tall pasture\", \"thick grassy volume\"],\n",
    "        \"green\": [\"lush green vibrant pasture\", \"fresh growth\"],\n",
    "        \"dead\": [\"dry brown dead grass\", \"senesced material\"],\n",
    "        \"clover\": [\"white clover\", \"trifolium repens\"],\n",
    "        \"grass\": [\"ryegrass\", \"blade-like leaves\"],\n",
    "    }\n",
    "    \n",
    "    concept_vectors = {}\n",
    "    with torch.no_grad():\n",
    "        for name, prompts in concept_groups.items():\n",
    "            inputs = tokenizer(prompts, padding=\"max_length\", return_tensors=\"pt\").to(device)\n",
    "            emb = model.get_text_features(**inputs)\n",
    "            emb = emb / emb.norm(p=2, dim=-1, keepdim=True)\n",
    "            concept_vectors[name] = emb.mean(dim=0, keepdim=True)\n",
    "    \n",
    "    if isinstance(image_embeddings, np.ndarray):\n",
    "        img_tensor = torch.tensor(image_embeddings, dtype=torch.float32).to(device)\n",
    "    else:\n",
    "        img_tensor = image_embeddings.to(device)\n",
    "    \n",
    "    img_tensor = img_tensor / img_tensor.norm(p=2, dim=-1, keepdim=True)\n",
    "    \n",
    "    scores = {}\n",
    "    for name, vec in concept_vectors.items():\n",
    "        scores[name] = torch.matmul(img_tensor, vec.T).cpu().numpy().flatten()\n",
    "    \n",
    "    df_scores = pd.DataFrame(scores)\n",
    "    df_scores['ratio_greenness'] = df_scores['green'] / (df_scores['green'] + df_scores['dead'] + 1e-6)\n",
    "    df_scores['ratio_clover'] = df_scores['clover'] / (df_scores['clover'] + df_scores['grass'] + 1e-6)\n",
    "    \n",
    "    return df_scores.values\n",
    "\n",
    "class SupervisedEmbeddingEngine(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Multi-stage feature engineering pipeline (Section 5.1.2)\"\"\"\n",
    "    def __init__(self, n_pca=0.98, n_pls=8, n_gmm=5, random_state=42):\n",
    "        self.n_pca = n_pca\n",
    "        self.n_pls = n_pls\n",
    "        self.n_gmm = n_gmm\n",
    "        self.random_state = random_state\n",
    "        self.scaler = StandardScaler()\n",
    "        self.pca = PCA(n_components=n_pca, random_state=random_state)\n",
    "        self.pls = PLSRegression(n_components=n_pls, scale=False)\n",
    "        self.gmm = GaussianMixture(n_components=n_gmm, covariance_type='diag', random_state=random_state)\n",
    "        self.pls_fitted_ = False\n",
    "\n",
    "    def fit(self, X, y=None, X_semantic=None):\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        self.pca.fit(X_scaled)\n",
    "        self.gmm.fit(X_scaled)\n",
    "        \n",
    "        if y is not None:\n",
    "            y_clean = y.values if hasattr(y, 'values') else y\n",
    "            self.pls.fit(X_scaled, y_clean)\n",
    "            self.pls_fitted_ = True\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, X, X_semantic=None):\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        return self._generate_features(X_scaled, X_semantic)\n",
    "\n",
    "    def _generate_features(self, X_scaled, X_semantic=None):\n",
    "        features = []\n",
    "        features.append(self.pca.transform(X_scaled))\n",
    "        if self.pls_fitted_:\n",
    "            features.append(self.pls.transform(X_scaled))\n",
    "        features.append(self.gmm.predict_proba(X_scaled))\n",
    "        if X_semantic is not None:\n",
    "            sem_norm = (X_semantic - np.mean(X_semantic, axis=0)) / (np.std(X_semantic, axis=0) + 1e-6)\n",
    "            features.append(sem_norm)\n",
    "        return np.hstack(features)\n",
    "\n",
    "def get_optimized_lightgbm_params(use_early_stopping=False):\n",
    "    \"\"\"Optimized LightGBM hyperparameters from Section 5.1.3\"\"\"\n",
    "    params = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'learning_rate': 0.01,\n",
    "        'num_leaves': 31,\n",
    "        'max_depth': 6,\n",
    "        'min_child_samples': 20,\n",
    "        'feature_fraction': 0.8,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 5,\n",
    "        'n_estimators': 500,\n",
    "        'verbose': -1,\n",
    "        'random_state': 42\n",
    "    }\n",
    "    # Only add early stopping if we're using validation sets\n",
    "    if use_early_stopping:\n",
    "        params['early_stopping_rounds'] = 50\n",
    "    return params\n",
    "\n",
    "def get_optimized_catboost_params():\n",
    "    \"\"\"Optimized CatBoost hyperparameters from Section 5.1.3\"\"\"\n",
    "    return {\n",
    "        'iterations': 500,\n",
    "        'learning_rate': 0.01,\n",
    "        'depth': 6,\n",
    "        'l2_leaf_reg': 3,\n",
    "        'min_data_in_leaf': 20,\n",
    "        'random_strength': 0.5,\n",
    "        'bagging_temperature': 0.2,\n",
    "        'od_type': 'Iter',\n",
    "        'od_wait': 50,\n",
    "        'verbose': 0,\n",
    "        'random_state': 42\n",
    "    }\n",
    "\n",
    "def compare_results(oof: np.ndarray, train_data: pd.DataFrame) -> Tuple[float, float]:\n",
    "    \"\"\"Evaluate with and without reconciliation\"\"\"\n",
    "    y_oof_df = pd.DataFrame(oof, columns=TARGET_NAMES)\n",
    "    raw_score = competition_metric(train_data[TARGET_NAMES].values, y_oof_df.values)\n",
    "    print(f\"Raw CV Score: {raw_score:.6f}\")\n",
    "    \n",
    "    y_oof_proc = hierarchical_reconciliation(y_oof_df.values)\n",
    "    y_oof_proc_df = pd.DataFrame(y_oof_proc, columns=TARGET_NAMES)\n",
    "    proc_score = competition_metric(train_data[TARGET_NAMES].values, y_oof_proc_df.values)\n",
    "    print(f\"Processed CV Score: {proc_score:.6f}\")\n",
    "    print(f\"Improvement: {proc_score - raw_score:.6f}\")\n",
    "    \n",
    "    return raw_score, proc_score\n",
    "\n",
    "def cross_validate(\n",
    "    model, \n",
    "    train_data: pd.DataFrame, \n",
    "    test_data: pd.DataFrame, \n",
    "    feature_engine, \n",
    "    semantic_train: Optional[np.ndarray] = None, \n",
    "    semantic_test: Optional[np.ndarray] = None, \n",
    "    target_transform: str = 'max', \n",
    "    seed: int = 42,\n",
    "    n_splits: int = 5\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"5-fold GroupKFold cross-validation (Section 5.1.3)\"\"\"\n",
    "    \n",
    "    # GroupKFold to prevent leakage\n",
    "    if 'image_path' in train_data.columns:\n",
    "        unique_images = train_data['image_path'].unique()\n",
    "        image_to_group = {img: i for i, img in enumerate(unique_images)}\n",
    "        groups = train_data['image_path'].map(image_to_group).values\n",
    "        kfold = GroupKFold(n_splits=n_splits)\n",
    "        splits = kfold.split(train_data, groups=groups)\n",
    "    else:\n",
    "        kfold = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "        splits = kfold.split(train_data)\n",
    "    \n",
    "    target_max_arr = np.array([TARGET_MAX[t] for t in TARGET_NAMES], dtype=float)\n",
    "    y_true = train_data[TARGET_NAMES]\n",
    "    y_pred = pd.DataFrame(0.0, index=train_data.index, columns=TARGET_NAMES)\n",
    "    y_pred_test = np.zeros([len(test_data), len(TARGET_NAMES)], dtype=float)\n",
    "    \n",
    "    COLUMNS = [col for col in train_data.columns if col.startswith('emb')]\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(splits):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Fold {fold + 1}/{n_splits}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        seeding(seed * (seed // 2 + fold))\n",
    "        \n",
    "        X_train_raw = train_data.iloc[train_idx][COLUMNS].values\n",
    "        X_valid_raw = train_data.iloc[val_idx][COLUMNS].values\n",
    "        X_test_raw = test_data[COLUMNS].values\n",
    "        \n",
    "        sem_train_fold = semantic_train[train_idx] if semantic_train is not None else None\n",
    "        sem_valid_fold = semantic_train[val_idx] if semantic_train is not None else None\n",
    "        \n",
    "        y_train = train_data.iloc[train_idx][TARGET_NAMES].values\n",
    "        y_valid = train_data.iloc[val_idx][TARGET_NAMES].values\n",
    "        \n",
    "        # Target transformation\n",
    "        if target_transform == 'log':\n",
    "            y_train_proc = np.log1p(y_train)\n",
    "        elif target_transform == 'max':\n",
    "            y_train_proc = y_train / target_max_arr\n",
    "        else:\n",
    "            y_train_proc = y_train\n",
    "        \n",
    "        # Fit feature engine\n",
    "        engine = deepcopy(feature_engine)\n",
    "        engine.fit(X_train_raw, y=y_train_proc, X_semantic=sem_train_fold)\n",
    "        \n",
    "        x_train_eng = engine.transform(X_train_raw, X_semantic=sem_train_fold)\n",
    "        x_valid_eng = engine.transform(X_valid_raw, X_semantic=sem_valid_fold)\n",
    "        x_test_eng = engine.transform(X_test_raw, X_semantic=semantic_test)\n",
    "        \n",
    "        fold_valid_pred = np.zeros_like(y_valid)\n",
    "        fold_test_pred = np.zeros([len(test_data), len(TARGET_NAMES)])\n",
    "        \n",
    "        # Train per target\n",
    "        for k, target_name in enumerate(TARGET_NAMES):\n",
    "            print(f\"  Training {target_name}...\", end=\" \")\n",
    "            \n",
    "            regr = deepcopy(model)\n",
    "            \n",
    "            # Special handling for LightGBM and CatBoost with validation sets\n",
    "            if isinstance(regr, LGBMRegressor):\n",
    "                regr.fit(\n",
    "                    x_train_eng, y_train_proc[:, k],\n",
    "                    eval_set=[(x_valid_eng, y_valid[:, k])],\n",
    "                    callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=False)]\n",
    "                )\n",
    "            elif isinstance(regr, CatBoostRegressor):\n",
    "                regr.fit(\n",
    "                    x_train_eng, y_train_proc[:, k],\n",
    "                    eval_set=(x_valid_eng, y_valid[:, k]),\n",
    "                    verbose=False\n",
    "                )\n",
    "            else:\n",
    "                # Standard sklearn models\n",
    "                regr.fit(x_train_eng, y_train_proc[:, k])\n",
    "            \n",
    "            pred_valid_raw = regr.predict(x_valid_eng)\n",
    "            pred_test_raw = regr.predict(x_test_eng)\n",
    "            \n",
    "            # Inverse transform\n",
    "            if target_transform == 'log':\n",
    "                pred_valid_inv = np.expm1(pred_valid_raw)\n",
    "                pred_test_inv = np.expm1(pred_test_raw)\n",
    "            elif target_transform == 'max':\n",
    "                pred_valid_inv = pred_valid_raw * target_max_arr[k]\n",
    "                pred_test_inv = pred_test_raw * target_max_arr[k]\n",
    "            else:\n",
    "                pred_valid_inv = pred_valid_raw\n",
    "                pred_test_inv = pred_test_raw\n",
    "            \n",
    "            fold_valid_pred[:, k] = pred_valid_inv\n",
    "            fold_test_pred[:, k] = pred_test_inv\n",
    "            print(\"✓\")\n",
    "        \n",
    "        y_pred.iloc[val_idx] = fold_valid_pred\n",
    "        y_pred_test += fold_test_pred / n_splits\n",
    "        \n",
    "        fold_score = competition_metric(y_valid, fold_valid_pred)\n",
    "        print(f\"Fold {fold + 1} Score: {fold_score:.6f}\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    full_cv = competition_metric(y_true.values, y_pred.values)\n",
    "    print(f\"Full CV Score: {full_cv:.6f}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return y_pred.values, y_pred_test\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN PIPELINE\n",
    "# ============================================================================\n",
    "def run_pipeline():\n",
    "    \"\"\"Main training pipeline\"\"\"\n",
    "    timeout_mgr = TimeoutManager(max_time_seconds=cfg.timeout_hours * 3600)\n",
    "    seeding(cfg.seed)\n",
    "    timeout_mgr.log_time(\"Pipeline started\")\n",
    "    \n",
    "    # Create fallback\n",
    "    try:\n",
    "        sample_sub_path = cfg.DATA_PATH / 'sample_submission.csv'\n",
    "        if os.path.exists(sample_sub_path):\n",
    "            sample_sub = pd.read_csv(sample_sub_path)\n",
    "            sample_sub.to_csv(cfg.submission_file, index=False)\n",
    "            print(\"✓ Created fallback submission\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Warning: {e}\\n\")\n",
    "    \n",
    "    # LOAD DATA\n",
    "    print(\"=\"*80)\n",
    "    print(\"LOADING DATA\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    test_df = pd.read_csv(cfg.DATA_PATH/'test.csv')\n",
    "    test_df = pivot_table(df=test_df)\n",
    "    test_df['image_path'] = test_df['image_path'].apply(lambda p: str(cfg.DATA_PATH / p))\n",
    "    \n",
    "    train_embeddings_path = \"/kaggle/input/csiro-datasplit/csiro_data_split.csv\"\n",
    "    if os.path.exists(train_embeddings_path):\n",
    "        print(\"✓ Loading pre-computed training embeddings\")\n",
    "        train_df = pd.read_csv(train_embeddings_path)\n",
    "    else:\n",
    "        print(\"⚠ Computing embeddings from scratch\")\n",
    "        train_df = pd.read_csv(cfg.DATA_PATH/'train.csv')\n",
    "        train_df = pivot_table(df=train_df)\n",
    "        train_df['image_path'] = train_df['image_path'].apply(lambda p: str(cfg.DATA_PATH / p))\n",
    "    \n",
    "    timeout_mgr.log_time(\"Data loaded\")\n",
    "    \n",
    "    # EXTRACT EMBEDDINGS\n",
    "    if timeout_mgr.should_continue(buffer_seconds=5*3600):\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"EXTRACTING EMBEDDINGS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        if 'emb1' not in test_df.columns:\n",
    "            test_df = compute_embeddings(\n",
    "                model_path=cfg.siglip_path, \n",
    "                df=test_df, \n",
    "                patch_size=cfg.patch_size,\n",
    "                overlap=cfg.patch_overlap\n",
    "            )\n",
    "            timeout_mgr.log_time(\"Test embeddings computed\")\n",
    "        else:\n",
    "            print(\"✓ Using pre-computed test embeddings\")\n",
    "        \n",
    "        if 'emb1' not in train_df.columns:\n",
    "            train_df = compute_embeddings(\n",
    "                model_path=cfg.siglip_path, \n",
    "                df=train_df, \n",
    "                patch_size=cfg.patch_size,\n",
    "                overlap=cfg.patch_overlap\n",
    "            )\n",
    "            timeout_mgr.log_time(\"Train embeddings computed\")\n",
    "        else:\n",
    "            print(\"✓ Using pre-computed train embeddings\")\n",
    "        \n",
    "        flush()\n",
    "        \n",
    "        # SEMANTIC FEATURES\n",
    "        if cfg.use_semantic_features:\n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(\"GENERATING SEMANTIC FEATURES\")\n",
    "            print(\"=\"*80)\n",
    "            \n",
    "            try:\n",
    "                X_all_emb = np.vstack([\n",
    "                    train_df.filter(like=\"emb\").values, \n",
    "                    test_df.filter(like=\"emb\").values\n",
    "                ])\n",
    "                all_semantic = generate_semantic_features(X_all_emb, model_path=cfg.siglip_path)\n",
    "                \n",
    "                if all_semantic is not None:\n",
    "                    n_train = len(train_df)\n",
    "                    sem_train = all_semantic[:n_train]\n",
    "                    sem_test = all_semantic[n_train:]\n",
    "                    print(f\"✓ Generated {all_semantic.shape[1]} features\")\n",
    "                    timeout_mgr.log_time(\"Semantic features generated\")\n",
    "                else:\n",
    "                    sem_train = None\n",
    "                    sem_test = None\n",
    "            except Exception as e:\n",
    "                print(f\"⚠ Semantic features failed: {e}\")\n",
    "                sem_train = None\n",
    "                sem_test = None\n",
    "        else:\n",
    "            sem_train = None\n",
    "            sem_test = None\n",
    "        \n",
    "        # TRAIN ENSEMBLE\n",
    "        if timeout_mgr.should_continue(buffer_seconds=2*3600):\n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(\"TRAINING ENSEMBLE\")\n",
    "            print(\"=\"*80)\n",
    "            \n",
    "            feat_engine = SupervisedEmbeddingEngine(n_pca=0.80, n_pls=8, n_gmm=6)\n",
    "            \n",
    "            # LightGBM\n",
    "            print(\"\\n\" + \"-\"*60)\n",
    "            print(\"LIGHTGBM\")\n",
    "            print(\"-\"*60)\n",
    "            lgb_params = get_optimized_lightgbm_params(use_early_stopping=False)\n",
    "            oof_lgbm, pred_lgbm = cross_validate(\n",
    "                LGBMRegressor(**lgb_params),\n",
    "                train_df, test_df, \n",
    "                feature_engine=feat_engine, \n",
    "                semantic_train=sem_train, \n",
    "                semantic_test=sem_test, \n",
    "                target_transform='max',\n",
    "                n_splits=cfg.n_folds\n",
    "            )\n",
    "            raw_lgb, proc_lgb = compare_results(oof_lgbm, train_df)\n",
    "            timeout_mgr.log_time(\"LightGBM complete\")\n",
    "            \n",
    "            # CatBoost\n",
    "            if cfg.use_catboost and timeout_mgr.should_continue(buffer_seconds=1*3600):\n",
    "                print(\"\\n\" + \"-\"*60)\n",
    "                print(\"CATBOOST\")\n",
    "                print(\"-\"*60)\n",
    "                cat_params = get_optimized_catboost_params()\n",
    "                oof_cat, pred_cat = cross_validate(\n",
    "                    CatBoostRegressor(**cat_params),\n",
    "                    train_df, test_df, \n",
    "                    feature_engine=feat_engine, \n",
    "                    semantic_train=sem_train, \n",
    "                    semantic_test=sem_test,\n",
    "                    target_transform='max',\n",
    "                    n_splits=cfg.n_folds\n",
    "                )\n",
    "                raw_cat, proc_cat = compare_results(oof_cat, train_df)\n",
    "                timeout_mgr.log_time(\"CatBoost complete\")\n",
    "                \n",
    "                # Ensemble\n",
    "                pred_test = cfg.lgb_weight * pred_lgbm + cfg.cat_weight * pred_cat\n",
    "                print(f\"\\n✓ Ensemble: {cfg.lgb_weight}*LGB + {cfg.cat_weight}*CAT\")\n",
    "            else:\n",
    "                print(\"\\n⚠ Using LightGBM only\")\n",
    "                pred_test = pred_lgbm\n",
    "            \n",
    "            # RECONCILIATION\n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(\"HIERARCHICAL RECONCILIATION\")\n",
    "            print(\"=\"*80)\n",
    "            \n",
    "            test_df[TARGET_NAMES] = pred_test\n",
    "            test_df_rec = test_df.copy()\n",
    "            test_df_rec[TARGET_NAMES] = hierarchical_reconciliation(test_df[TARGET_NAMES].values)\n",
    "            \n",
    "            # SUBMISSION\n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(\"CREATING SUBMISSION\")\n",
    "            print(\"=\"*80)\n",
    "            \n",
    "            sub_df = melt_table(test_df_rec)\n",
    "            sub_df[['sample_id', 'target']].to_csv(cfg.submission_file, index=False)\n",
    "            print(f\"✓ Submission: {cfg.submission_file}\")\n",
    "            \n",
    "            print(\"\\nPrediction Statistics:\")\n",
    "            print(test_df_rec[TARGET_NAMES].describe())\n",
    "            \n",
    "            timeout_mgr.log_time(\"Complete!\")\n",
    "            \n",
    "        else:\n",
    "            print(\"\\n⚠ Insufficient time for training\")\n",
    "    else:\n",
    "        print(\"\\n⚠ Insufficient time for embeddings\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DONE\")\n",
    "    print(\"=\"*80)\n",
    "    timeout_mgr.log_time(\"Final\")\n",
    "\n",
    "# ============================================================================\n",
    "# RUN THE PIPELINE\n",
    "# ============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        run_pipeline()\n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ ERROR: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        # Emergency fallback\n",
    "        try:\n",
    "            sample_path = cfg.DATA_PATH / 'sample_submission.csv'\n",
    "            if os.path.exists(sample_path):\n",
    "                sample = pd.read_csv(sample_path)\n",
    "                sample.to_csv(cfg.submission_file, index=False)\n",
    "                print(\"✓ Emergency fallback submission\")\n",
    "        except:\n",
    "            print(\"❌ Could not create fallback\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 14254895,
     "sourceId": 112509,
     "sourceType": "competition"
    },
    {
     "datasetId": 8856212,
     "isSourceIdPinned": false,
     "sourceId": 13900620,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8929818,
     "isSourceIdPinned": false,
     "sourceId": 14018229,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 288467413,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 288761108,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 986,
     "modelInstanceId": 3329,
     "sourceId": 4537,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 251887,
     "modelInstanceId": 230141,
     "sourceId": 268942,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 487624,
     "modelInstanceId": 471723,
     "sourceId": 663314,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 194.138933,
   "end_time": "2026-01-11T14:16:23.705417",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-11T14:13:09.566484",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0ae602593d7f407db3010cbea649a190": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "15a41b88861949a290d16b2c4128e593": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "27a7169e75fd44cfac075003e0068c0f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_15a41b88861949a290d16b2c4128e593",
       "placeholder": "​",
       "style": "IPY_MODEL_ea0a9e9f92144ee590d8ca9855b94bd1",
       "tabbable": null,
       "tooltip": null,
       "value": " 1/1 [00:02&lt;00:00,  2.37s/it]"
      }
     },
     "4132a020658f4eb4a19a1a034f6a6d9e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_0ae602593d7f407db3010cbea649a190",
       "placeholder": "​",
       "style": "IPY_MODEL_9a6f594670f54a7c933d0d457db4b93d",
       "tabbable": null,
       "tooltip": null,
       "value": "Computing embeddings: 100%"
      }
     },
     "60acff972c084605b72ad2bb824640ff": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "830e527fabfe4b259edd56c9ae534d15": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8c9c5a19800d4012a97a7ebf39a32972": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_4132a020658f4eb4a19a1a034f6a6d9e",
        "IPY_MODEL_cd4abbb719944a55b55ad2b376262c9f",
        "IPY_MODEL_27a7169e75fd44cfac075003e0068c0f"
       ],
       "layout": "IPY_MODEL_60acff972c084605b72ad2bb824640ff",
       "tabbable": null,
       "tooltip": null
      }
     },
     "9a6f594670f54a7c933d0d457db4b93d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "b064f65440f04172b0baa7ee3c1c9cd8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "cd4abbb719944a55b55ad2b376262c9f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_830e527fabfe4b259edd56c9ae534d15",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_b064f65440f04172b0baa7ee3c1c9cd8",
       "tabbable": null,
       "tooltip": null,
       "value": 1.0
      }
     },
     "ea0a9e9f92144ee590d8ca9855b94bd1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
